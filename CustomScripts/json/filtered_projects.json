[
    {
        "id": 1040145,
        "date": "2024-05-27T14:28:57",
        "slug": "transvip",
        "title": "TransVIP",
        "link": "https://www.microsoft.com/en-us/research/project/transvip/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"transvip\">TransVIP</h1>\n\n\n\n<p>Speech to Speech Translation System with Voice and Isochrony Preservation</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>We introduce a novel model framework TransVIP that leverages diverse datasets in a cascade fashion yet facilitates end-to-end inference through joint probability. Furthermore, we propose two separated encoders to preserve the speaker\u2019s voice characteristics and isochrony from the source speech during the translation process, making it highly suitable for scenarios such as video dubbing.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"339\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/overview-2048-1024x339.png\" alt=\"Overview of our speech to speech translation framework, which consists of 1) Joint encoder-decoder model for translating speech into target text, and coarse-grained speech tokens, 2) Non-autoregressive acoustic model for acoustic details; 3) Codec model to convert discrete speech tokens back to waveform.\" class=\"wp-image-1040157\" style=\"width:1284px;height:auto\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/overview-2048-1024x339.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/overview-2048-300x99.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/overview-2048-768x254.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/overview-2048-1536x508.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/overview-2048-240x79.png 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/overview-2048.png 2047w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\">Overview of our speech to speech translation framework, which consists of 1) Joint encoder-decoder model for translating speech into target text, and coarse-grained speech tokens, <br>2) Non-autoregressive acoustic model for acoustic details; 3) Codec model to convert discrete speech tokens back to waveform.</figcaption></figure>\n\n\n\n<h5 class=\"wp-block-heading\" id=\"audio-clips\">Audio Clips</h5>\n\n\n\n<p></p>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-3 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-1 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>Source</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>CVSS-T (Groundtruth Text/Voice Cloning)</p>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-2 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>TransVIP</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>Groundtruth Text Label /<br>TransVIP Text Out</p>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<p>\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-6 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-4 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17309232.cut_.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17309232.cvss_.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-5 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17309232.vip_.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\">\n<p class=\"has-text-align-left\">Before contacting us, check out our frequently asked questions.</p>\n\n\n\n<p>Before contacting us, let&#8217;s look at the questions we have.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<p>\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-7 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17312325.cut_.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17312325.cvss_.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-8 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17312325.vip_.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\">\n<p class=\"has-text-align-left\">The Court of Auditors told you that.</p>\n\n\n\n<p>The Court of Auditors told you that.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<p>\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-12 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-10 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17312445.cut_.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17312445.cvss_.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-11 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17312445.vip_.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\">\n<p class=\"has-text-align-left\">The Government understood it.</p>\n\n\n\n<p>The Government understood it.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<p>\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-15 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-13 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17342187.cut_.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17342187.cvss_.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-14 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17342187.vip_.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\">\n<p class=\"has-text-align-left\">if they come or not.</p>\n\n\n\n<p>Whether he&#8217;s coming or not.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<p>\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-18 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-16 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17423913.cut_.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17423913.cvss_.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-17 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17423913.vip_.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\">\n<p class=\"has-text-align-left\">The Front de gauche member of parliaments don\u2019t understand you.</p>\n\n\n\n<p>The parliamentarians of the front-left do not understand you.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<p>\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-21 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-19 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17623671.cut_.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17623671.cvss_.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-20 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/common_voice_fr_17623671.vip_.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\">\n<p class=\"has-text-align-left\">It is true, and that\u2019s an absolute positive step forward.</p>\n\n\n\n<p>It&#8217;s accurate! And it&#8217;s a completely positive step.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p></p>\n</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Speech to Speech Translation System with Voice and Isochrony Preservation We introduce a novel model framework TransVIP that leverages diverse datasets in a cascade fashion yet facilitates end-to-end inference through joint probability. Furthermore, we propose two separated encoders to preserve the speaker\u2019s voice characteristics and isochrony from the source speech during the translation process, making [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1035114,
        "date": "2024-05-16T14:02:33",
        "slug": "devicescript",
        "title": "DeviceScript",
        "link": "https://www.microsoft.com/en-us/research/project/devicescript/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1536\" height=\"960\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/hero-6841bcefb7272a3b78c2650090ff2181.png\" class=\"attachment-full size-full\" alt=\"Screenshot of visual studio code with user interface of devicescript\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/hero-6841bcefb7272a3b78c2650090ff2181.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/hero-6841bcefb7272a3b78c2650090ff2181-300x188.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/hero-6841bcefb7272a3b78c2650090ff2181-1024x640.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/hero-6841bcefb7272a3b78c2650090ff2181-768x480.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/hero-6841bcefb7272a3b78c2650090ff2181-240x150.png 240w\" sizes=\"(max-width: 1536px) 100vw, 1536px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"typescript-for-tiny-iot-devices\">TypeScript </h1>\n\n\n\n<h1 class=\"wp-block-heading\" id=\"typescript-for-tiny-iot-devices\">for Tiny IoT Devices </h1>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p><strong>DeviceScript brings a TypeScript developer experience to low-resource microcontroller-based devices.</strong>&nbsp;DeviceScript is compiled to a custom VM bytecode, which can run in very constrained environments.</p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/devicescript\">GitHub repository<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://microsoft.github.io/devicescript/\">Documentation<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>DeviceScript brings a TypeScript developer experience to low-resource microcontroller-based devices.&nbsp;DeviceScript is compiled to a custom VM bytecode, which can run in very constrained environments. Opens in a new tab</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1035009,
        "date": "2024-05-16T11:41:56",
        "slug": "genaiscript-scripting-for-generative-ai",
        "title": "GenAIScript: Scripting for Generative AI",
        "link": "https://www.microsoft.com/en-us/research/project/genaiscript-scripting-for-generative-ai/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-grey card-background--inset-right\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"764\" height=\"764\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/GenAI-Script_logo.png\" class=\"attachment-full size-full\" alt=\"genAI design element\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/GenAI-Script_logo.png 764w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/GenAI-Script_logo-300x300.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/GenAI-Script_logo-150x150.png 150w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/GenAI-Script_logo-180x180.png 180w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/GenAI-Script_logo-360x360.png 360w\" sizes=\"(max-width: 764px) 100vw, 764px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h4 class=\"wp-block-heading\" id=\"genaiscript-scripting-for-generative-ai-1\">GenAIScript: Scripting for Generative AI</h4>\n\n\n\n<p>GenAIScript is open source</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<h2 class=\"wp-block-heading\" id=\"what-is-genaiscript\">What is GenAIScript?</h2>\n\n\n\n<p><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://microsoft.github.io/genaiscript/\" target=\"_blank\" rel=\"noreferrer noopener\">GenAIScript<span class=\"sr-only\"> (opens in new tab)</span></a> is scripting language where AI and foundation models are first-class entities, enabling a wide range of users to enhance their workflows with AI capabilities. GenAIScript provides support for authoring and debugging scripts that incorporate calls to foundation models and LLMs in their execution. With deep integration in a VS Code extension, users can author, debug, and deploy their GenAIScripts leveraging a state-of-the-art user experience. GenAIScript supports importing AI context from many different sources (.pdf, .docx, .csv, etc.) and generating multiple forms of output from LLM generations, such as files, edits, and structured data.</p>\n\n\n\n<p>In addition to enhancing productivity, GenAIScript is a research vehicle for building an AI-aware programming language runtime system and integrating it into a next-generation system stack that incorporates AI foundation models from first principles. For more thoughts about System Stack 2.0, read our SIGPLAN/SIGARCH blog article: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://blog.sigplan.org/2024/04/23/ai-software-should-be-more-like-plain-old-software/\" target=\"_blank\" rel=\"noreferrer noopener\">AI Software Should be More Like Plain Old Software | SIGPLAN Blog<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<p>You can start using it by downloading it from the VS Code Extensions or visiting: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://microsoft.github.io/genaiscript/\" target=\"_blank\" rel=\"noreferrer noopener\">Generative AI Scripting | GenAIScript (microsoft.github.io)<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://microsoft.github.io/genaiscript/\" target=\"_blank\" rel=\"noreferrer noopener\">Get the code</a></div>\n\n\n\n<div class=\"wp-block-button is-style-fill-github\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://github.com/microsoft/genaiscript\" target=\"_blank\" rel=\"noreferrer noopener\">Visit the GitHub repo</a></div>\n</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>GenAIScript is scripting language where AI and foundation models are first-class entities, enabling a wide range of users to enhance their workflows with AI capabilities.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1030176,
        "date": "2024-05-01T12:41:29",
        "slug": "speaking-the-world-into-existence",
        "title": "Speaking the World into Existence",
        "link": "https://www.microsoft.com/en-us/research/project/speaking-the-world-into-existence/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1032\" height=\"589\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/Picture1.jpg\" class=\"attachment-full size-full\" alt=\"Speaking the world into existence\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/Picture1.jpg 1032w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/Picture1-300x171.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/Picture1-1024x584.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/Picture1-768x438.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/Picture1-240x137.jpg 240w\" sizes=\"(max-width: 1032px) 100vw, 1032px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"real-time-creation-of-virtual-worlds\">Real-time creation of virtual worlds</h1>\n\n\n\n<p class=\"has-text-align-left\">AI co-creation has the potential to transform the way we interact with virtual worlds and bring about the early VR hopes of speaking shared environments into existence, allowing for novel modes of communicating ideas.</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>The&nbsp;<em>Speaking the world into existence</em><em>&nbsp;</em>research effort aims to apply the recent progress in large language models to prompt-based creation of interactive 3d scenes. Integrating LLMs with a game engine should enable not only faster development of 3d content in various domains, such as gaming, mixed reality applications or animated films, but also allow for spontaneous user-generated content in the course of an interactive experience. Our previous work has also demonstrated that giving a large AI model the ability to act in a simulated environment with feedback has the potential to improve the outputs of generative models by grounding them in the real world. One of the fundamental research directions of this project is to make large multimodal models more reliable in the domain of human-scale activity, by not only incorporating what has been said about the world, but also testing results in a simulation of the world.</p>\n\n\n\n<p>Beyond application in gaming and building compelling virtual worlds, easier creation of interactive 3d content and simulations by non-coding users also opens applications to education (e.g. allowing teachers to create immersive VR lessons in a short time), rapid creation of interactive training scenarios (imagine a group of first responders spinning up a simulation in the 30 minutes it takes them to arrive at the location of a disaster so that they can be prepared for the potential difficulties at the location) and many other applications in various domains such as real-time generation of virtual environments for therapy or creative applications.</p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>AI co-creation has the potential to transform the way we interact with virtual worlds and bring about the early VR hopes of speaking shared environments into existence, allowing for novel modes of communicating ideas. The&nbsp;Speaking the world into existence&nbsp;research effort aims to apply the recent progress in large language models to prompt-based creation of interactive [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1027041,
        "date": "2024-04-23T11:20:10",
        "slug": "graphrag",
        "title": "Project GraphRAG",
        "link": "https://www.microsoft.com/en-us/research/project/graphrag/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"2633\" height=\"1836\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/graphrag-banner.png\" class=\"attachment-full size-full\" alt=\"Image of an entity network with color-coded communities\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/graphrag-banner.png 2633w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/graphrag-banner-300x209.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/graphrag-banner-1024x714.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/graphrag-banner-768x536.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/graphrag-banner-1536x1071.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/graphrag-banner-2048x1428.png 2048w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/graphrag-banner-240x167.png 240w\" sizes=\"(max-width: 2633px) 100vw, 2633px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"project-graphrag\">Project GraphRAG</h1>\n\n\n\n<p>LLM-Derived Knowledge Graphs</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<div class=\"wp-block-media-text has-video  has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile\" data-bi-an=\"media-text\"><figure class=\"wp-block-media-text__media video-wrapper\"><iframe class=\"media-text__video\" src=\"https://www.youtube-nocookie.com/embed/jCjyaQL-7mA?enablejsapi=1&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></figure><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<h2 class=\"wp-block-heading\" id=\"graphrag\">GraphRAG</h2>\n\n\n\n<p>GraphRAG (Graphs + Retrieval Augmented Generation) is a technique for richly understanding text datasets by combining text extraction, network analysis, and LLM prompting and summarization into a single end-to-end system.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-fill\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/publication/from-local-to-global-a-graph-rag-approach-to-query-focused-summarization/\" data-bi-cn=\"GraphRAG\">Read the pre-print</a></div>\n\n\n\n<div class=\"wp-block-button is-style-fill\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/\" data-bi-cn=\"GraphRAG\">Read the blog</a></div>\n</div>\n</div></div>\n\n\n\n<p><strong>Open Source</strong></p>\n\n\n\n<p>A full open source GraphRAG implementation will be released soon on GitHub.</p>\n\n\n\n<p>In the meantime, reach out to us at <a href=\"mailto:teamgraphrag@microsoft.com\">teamgraphrag@microsoft.com</a> for feedback, questions, or if you have a use case where GraphRAG could be a good fit for you.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"mailto:teamgraphrag@microsoft.com\">Contact us</a></div>\n</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>LLM-Derived Knowledge Graphs GraphRAG (Graphs + Retrieval Augmented Generation) is a technique for richly understanding text datasets by combining text extraction, network analysis, and LLM prompting and summarization into a single end-to-end system. Open Source A full open source GraphRAG implementation will be released soon on GitHub. In the meantime, reach out to us at [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1025229,
        "date": "2024-04-16T08:13:08",
        "slug": "vasa-1",
        "title": "VASA-1",
        "link": "https://www.microsoft.com/en-us/research/project/vasa-1/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-plum card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 justify-content-center\">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 \">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading has-text-align-center\" id=\"vasa-1-lifelike-audio-driven-talking-faces-generated-in-real-time\">VASA-1: Lifelike Audio-Driven Talking Faces<br> Generated in Real Time</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n<iframe loading=\"lazy\" title=\"Inline Frame Example\" width=\"100%\" height=\"7300px\" frameBorder=\"0\" scrolling=\"auto\" src=\"https://vasavatar.github.io/VASA-1/\">\n</iframe>\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Opens in a new tab</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1017768,
        "date": "2024-04-02T09:04:26",
        "slug": "expand-opportunity-ai-for-good",
        "title": "Expand Opportunity &#8211; AI for Good",
        "link": "https://www.microsoft.com/en-us/research/project/expand-opportunity-ai-for-good/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Expand_Opportunity_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"Two persons working on a laptop\" style=\"object-position: 100% 51%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Expand_Opportunity_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Expand_Opportunity_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Expand_Opportunity_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Expand_Opportunity_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Expand_Opportunity_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Expand_Opportunity_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Expand_Opportunity_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/ai-for-good-research-lab/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"AI for Good Lab\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAI for Good Lab\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"expand-opportunity\">Expand opportunity</h1>\n\n\n\n<p></p>\n\n\n\n<div class=\"wp-block-button is-style-fill\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/corporate-responsibility/expand-opportunity\" target=\"_blank\" rel=\"noreferrer noopener\">Learn more</a></div>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<div class=\"wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile is-vertically-aligned-top\" style=\"grid-template-columns:40% auto\" data-bi-an=\"media-text\"><figure class=\"wp-block-media-text__media\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-2_AI-Driven-Solutions_1400x788-1024x576.jpg\" alt=\"AI for Good - a diverse group of six people gathered around a desk reviewing a project\" class=\"wp-image-1015023 size-full\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-2_AI-Driven-Solutions_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-2_AI-Driven-Solutions_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-2_AI-Driven-Solutions_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-2_AI-Driven-Solutions_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-2_AI-Driven-Solutions_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-2_AI-Driven-Solutions_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-2_AI-Driven-Solutions_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-2_AI-Driven-Solutions_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-2_AI-Driven-Solutions_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-2_AI-Driven-Solutions_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<h2 class=\"wp-block-heading\" id=\"increasing-opportunities-in-a-digital-world\">Increasing opportunities in a digital world</h2>\n\n\n\n<p>In today&#8217;s digital world, we&#8217;re committed to empowering individuals to thrive in an AI-enabled economy. Through initiatives aimed at building digital skills for employment and livelihoods, expanding access to computer science education, and addressing critical skills gaps, we strive to ensure that everyone can harness the opportunities presented by technology. By advocating for the open exchange, sharing, and collaboration of data, we believe in enabling better decision-making, enhancing efficiency, and collectively addressing some of society&#8217;s most pressing challenges.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.linkedin.com/pulse/power-open-data-github-fueling-ais-future-juan-m-lavista-ferres%3FtrackingId=GTPVD292lonJSmEjVQ6T3w%253D%253D/?trackingId=GTPVD292lonJSmEjVQ6T3w%3D%3D\" target=\"_blank\" rel=\"noreferrer noopener\" data-bi-cn=\"Increasing opportunities in a digital world\">The power of open data on GitHub: Fueling AI&#8217;s future</a></div>\n</div>\n</div></div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h2 class=\"wp-block-heading\" id=\"empower-communities-to-take-anticipatory-action-with-early-warnings\">Growing access to digital skills and economy</h2>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-22 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_AI-skills_1400x788-1024x576.jpg\" alt=\"AI4Good - Expand Opportunity | two people sitting at a desk and looking at the camera\" class=\"wp-image-1021476\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_AI-skills_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_AI-skills_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_AI-skills_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_AI-skills_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_AI-skills_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_AI-skills_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_AI-skills_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_AI-skills_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_AI-skills_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_AI-skills_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">AI skills</h4>\n\n\n\n<p>AI offers tremendous potential to empower workers around the world\u2014but only if everyone, everywhere has the skills to use it. Learn how we\u2019re helping people and communities harness the power of AI.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-5 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://microsoft.com/corporate-responsibility/ai-skills-resources\" target=\"_blank\" rel=\"noreferrer noopener\">Resources</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Cybersecurity-skill-gap_1400x788-1024x576.jpg\" alt=\"AI4Good - Expand Opportunity | woman looking at a computer screen\" class=\"wp-image-1021479\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Cybersecurity-skill-gap_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Cybersecurity-skill-gap_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Cybersecurity-skill-gap_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Cybersecurity-skill-gap_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Cybersecurity-skill-gap_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Cybersecurity-skill-gap_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Cybersecurity-skill-gap_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Cybersecurity-skill-gap_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Cybersecurity-skill-gap_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Cybersecurity-skill-gap_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Close the cybersecurity skills gap</h4>\n\n\n\n<p>Currently there aren\u2019t enough cybersecurity professionals to meet demand. Microsoft is helping to build an inclusive pipeline of cyber talent around the world.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-6 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/corporate-responsibility/cybersecurity-skills\" target=\"_blank\" rel=\"noreferrer noopener\">What&#8217;s next</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Sustainability-skills-gap_1400x788-1024x576.jpg\" alt=\"AI4Good - Expand Opportunity | woman taking notes in a greenhouse\" class=\"wp-image-1021473\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Sustainability-skills-gap_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Sustainability-skills-gap_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Sustainability-skills-gap_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Sustainability-skills-gap_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Sustainability-skills-gap_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Sustainability-skills-gap_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Sustainability-skills-gap_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Sustainability-skills-gap_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Sustainability-skills-gap_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Sustainability-skills-gap_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Close the sustainability skills gap</h4>\n\n\n\n<p>Over 3,500 companies globally have issued climate pledges but&nbsp;many lack a workforce with the skills to turn these pledges to progress. Our report shows the urgent need to develop a new level of sustainability skills and fluency \u2013 and how we aim to help support the transformation.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-7 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://aka.ms/SustainabilitySkillsReportBlog\" target=\"_blank\" rel=\"noreferrer noopener\">Findings</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/corporate-responsibility/sustainability-skills\" target=\"_blank\" rel=\"noreferrer noopener\">Resources</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Digital-literacy_1400x788-1024x576.jpg\" alt=\"AI4Good - Expand Opportunity | man looking at a tablet and a laptop\" class=\"wp-image-1021482\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Digital-literacy_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Digital-literacy_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Digital-literacy_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Digital-literacy_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Digital-literacy_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Digital-literacy_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Digital-literacy_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Digital-literacy_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Digital-literacy_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good_Digital-literacy_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"digital-literacy-fundamentals\">Digital literacy fundamentals</h4>\n\n\n\n<p>Digital literacy can play a powerful role in helping people create more promising futures. These resources can help individuals gain the basic skills needed to effectively explore the internet and navigate our digital world.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-8 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/digital-literacy\" target=\"_blank\" rel=\"noreferrer noopener\">Free courses</a></div>\n</div>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h2 class=\"wp-block-heading\" id=\"empower-communities-to-take-anticipatory-action-with-early-warnings\">Closing the data divide</h2>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-23 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-open-data-for-society-1024x576.jpg\" alt=\"AI4Good - Expand Opportunity | photo of a sunburst shining through a forest with an overlaid color swoosh\" class=\"wp-image-1017906\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-open-data-for-society-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-open-data-for-society-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-open-data-for-society-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-open-data-for-society-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-open-data-for-society-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-open-data-for-society-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-open-data-for-society-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-open-data-for-society-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-open-data-for-society-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-open-data-for-society.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Improving society with open data</h4>\n\n\n\n<p>We\u2019re working to make data that is relevant to important social problems as open as possible, including by contributing open data ourselves. The Data for Society resource center provides access to Microsoft\u2019s open datasets, resources, and tools to make data sharing, research, and collaboration easier.</p>\n\n\n\n<div class=\"wp-block-buttons is-content-justification-left is-content-justification-left is-layout-flex wp-container-core-buttons-is-layout-9 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/corporate-responsibility/open-data?activetab=pivot1:primaryr6\" target=\"_blank\" rel=\"noreferrer noopener\">Data for Society resources</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-10-Open-datasets-1024x576.jpg\" alt=\"AI4Good - Expand Opportunity | photo of two women in front of computer screens\" class=\"wp-image-1017807\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-10-Open-datasets-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-10-Open-datasets-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-10-Open-datasets-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-10-Open-datasets-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-10-Open-datasets-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-10-Open-datasets-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-10-Open-datasets-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-10-Open-datasets-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-10-Open-datasets-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-10-Open-datasets.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Machine readable documentation for open datasets</h4>\n\n\n\n<p>Introducing a no-code, machine-readable documentation framework for open datasets, emphasizing Responsible AI (RAI). Designed to enhance accessibility and usability, it simplifies dataset discovery, understanding, and evaluation for researchers and data scientists.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-10 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/publication/open-datasheets-machine-readable-documentation-for-open-datasets-and-responsible-ai-assessments/\" target=\"_blank\" rel=\"noreferrer noopener\">Paper</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://microsoft.github.io/opendatasheets/\" target=\"_blank\" rel=\"noreferrer noopener\">Open Datasheets</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-11-Code-repositories-1024x576.jpg\" alt=\"AI4Good - Expand Opportunity | abstract image of an AI chip embedded in a forest clearing\" class=\"wp-image-1017780\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-11-Code-repositories-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-11-Code-repositories-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-11-Code-repositories-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-11-Code-repositories-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-11-Code-repositories-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-11-Code-repositories-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-11-Code-repositories-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-11-Code-repositories-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-11-Code-repositories-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-11-Code-repositories.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">AI for Good code repositories</h4>\n\n\n\n<p>The AI for Good Lab is committed to the open data initiative. We&#8217;re dedicated to investing in essential assets that will make data sharing easier, including the necessary tools; frameworks; and templates. This is especially important for opening and collaborating around data to solve important societal issues.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-11 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/group/ai-for-good-research-lab/downloads/\" target=\"_blank\" rel=\"noreferrer noopener\">AI for Good datasets</a></div>\n</div>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Our commitment starts with ensuring everyone has the ability to thrive in a digital, AI-enabled economy, and extends to empowering other organizations to address society\u2019s biggest challenges.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1019952,
        "date": "2024-04-02T09:02:54",
        "slug": "earn-trust-ai-for-good",
        "title": "Earn Trust &#8211; AI for Good",
        "link": "https://www.microsoft.com/en-us/research/project/earn-trust-ai-for-good/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Earn_Trust_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"A person sitting at a table using a laptop\" style=\"object-position: 76% 53%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Earn_Trust_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Earn_Trust_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Earn_Trust_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Earn_Trust_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Earn_Trust_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Earn_Trust_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Earn_Trust_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/ai-for-good-research-lab/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"AI for Good Lab\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAI for Good Lab\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"earn-trust\">Earn trust</h1>\n\n\n\n<p></p>\n\n\n\n<div class=\"wp-block-button is-style-fill\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/corporate-responsibility/earn-trust\" target=\"_blank\" rel=\"noreferrer noopener\">Learn more</a></div>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<div class=\"wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile is-vertically-aligned-top\" style=\"grid-template-columns:40% auto\" data-bi-an=\"media-text\"><figure class=\"wp-block-media-text__media\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI-Driven-solutions-1024x576.jpg\" alt=\"AI4Good | Map pointing to a map of the countries of the world in a high tech setting\" class=\"wp-image-1020951 size-full\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI-Driven-solutions-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI-Driven-solutions-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI-Driven-solutions-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI-Driven-solutions-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI-Driven-solutions-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI-Driven-solutions-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI-Driven-solutions-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI-Driven-solutions-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI-Driven-solutions-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI-Driven-solutions.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<h2 class=\"wp-block-heading\" id=\"building-trust-with-ethical-ai\">Building trust with ethical AI</h2>\n\n\n\n<p>To create positive impact with technology, people must be able to trust the technologies they use and the companies behind them. That\u2019s why we\u2019re committed to the responsible use of AI, protecting privacy, and advancing digital safety and cybersecurity.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://blogs.microsoft.com/on-the-issues/2023/05/25/how-do-we-best-govern-ai/?culture=en-us&country=us\" target=\"_blank\" rel=\"noreferrer noopener\" data-bi-cn=\"Building trust with ethical AI\">How do we best govern AI</a></div>\n</div>\n</div></div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h2 class=\"wp-block-heading\" id=\"how-we-work\">How we work</h2>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-24 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Responsible_AI-1024x576.jpg\" alt=\"a person standing in front of a power plant\" class=\"wp-image-1020954\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Responsible_AI-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Responsible_AI-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Responsible_AI-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Responsible_AI-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Responsible_AI-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Responsible_AI-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Responsible_AI-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Responsible_AI-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Responsible_AI-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Responsible_AI.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Responsible AI</h4>\n\n\n\n<p>Microsoft is dedicated to advancing ethical AI principles, guiding us to construct safe, secure, and transparent AI systems aimed at benefiting society. As technological advancements accelerate, our efforts to govern AI responsibly must evolve accordingly.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-13 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/ai/responsible-ai\" target=\"_blank\" rel=\"noreferrer noopener\">Visit site</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Privacy-1024x576.jpg\" alt=\"Woman standing in front of a large screen monitor and surface hub\" class=\"wp-image-1020957\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Privacy-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Privacy-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Privacy-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Privacy-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Privacy-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Privacy-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Privacy-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Privacy-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Privacy-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Privacy.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Privacy</h4>\n\n\n\n<p>At Microsoft, we value, protect, and defend privacy. We believe in transparency, so that people and organizations can control their data and have meaningful choices in how it is used.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-14 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://privacy.microsoft.com/en-US/\">Visit site</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Digital-safety-1024x576.jpg\" alt=\"Three workers sitting in a row on their computers\" class=\"wp-image-1020960\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Digital-safety-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Digital-safety-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Digital-safety-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Digital-safety-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Digital-safety-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Digital-safety-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Digital-safety-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Digital-safety-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Digital-safety-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Digital-safety.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Digital safety</h4>\n\n\n\n<p>Microsoft works to preserve digital safety while respecting human rights like privacy, freedom of speech, and security. Safe online spaces help people create, connect, and share knowledge.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-15 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/DigitalSafety\" target=\"_blank\" rel=\"noreferrer noopener\">Visit site</a></div>\n</div>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h2 id=\"\" class=\"wp-block-heading\"></h2>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-25 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Private-synthetic-data-1024x576.png\" alt=\"Women on screen with AI overlay\" class=\"wp-image-1020963\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Private-synthetic-data-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Private-synthetic-data-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Private-synthetic-data-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Private-synthetic-data-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Private-synthetic-data-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Private-synthetic-data-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Private-synthetic-data-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Private-synthetic-data-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Private-synthetic-data-1280x720.png 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Private-synthetic-data.png 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Preserving privacy with synthetic data </h4>\n\n\n\n<p>Our research explores the effectiveness of synthetic data in critical fields like healthcare and humanitarian action, where data privacy is non-negotiable. We found models trained with synthetic data can match the performance of those trained on real data, while upholding privacy and fairness standards.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-16 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/publication/assessment-of-differentially-private-synthetic-data-for-utility-and-fairness-in-end-to-end-machine-learning-pipelines-for-tabular-data/\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Multifactor-authentication-1024x576.jpg\" alt=\"Man on computer using phone for multi-factor authentication\" class=\"wp-image-1020966\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Multifactor-authentication-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Multifactor-authentication-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Multifactor-authentication-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Multifactor-authentication-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Multifactor-authentication-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Multifactor-authentication-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Multifactor-authentication-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Multifactor-authentication-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Multifactor-authentication-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/Multifactor-authentication.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Multifactor authentication protection</h4>\n\n\n\n<p>Our study reveals that Multifactor Authentication (MFA) implementation offers outstanding protection, with over 99.99% of MFA-enabled accounts remaining secure during the investigation period. Moreover, MFA reduces the risk of compromise by 99.22% across the entire population.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-17 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://arxiv.org/abs/2305.00945\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>To create positive impact with technology, people must be able to trust the technologies they use and the companies behind them. That\u2019s why we\u2019re committed to the responsible use of AI, protecting privacy, and advancing digital safety and cybersecurity.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1014747,
        "date": "2024-04-02T08:58:55",
        "slug": "fundamental-rights-ai-for-good",
        "title": "Fundamental Rights &#8211; AI for Good",
        "link": "https://www.microsoft.com/en-us/research/project/fundamental-rights-ai-for-good/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Fundamental_Rights_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"A person carrying water\" style=\"object-position: 97% 66%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Fundamental_Rights_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Fundamental_Rights_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Fundamental_Rights_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Fundamental_Rights_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Fundamental_Rights_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Fundamental_Rights_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Fundamental_Rights_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/ai-for-good-research-lab/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"AI for Good Lab\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAI for Good Lab\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"fundamental-rights\">Fundamental rights</h1>\n\n\n\n<p></p>\n\n\n\n<div class=\"wp-block-button is-style-fill\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/corporate-responsibility/protect-fundamental-rights\" target=\"_blank\" rel=\"noreferrer noopener\">Learn more</a></div>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<div class=\"wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile is-vertically-aligned-top\" style=\"grid-template-columns:40% auto\" data-bi-an=\"media-text\"><figure class=\"wp-block-media-text__media\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-1024x576.jpg\" alt=\"Satellite map of a town\" class=\"wp-image-1017900 size-full\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<h2 class=\"wp-block-heading\" id=\"ai-driven-solutions\">AI-driven solutions</h2>\n\n\n\n<p>In today&#8217;s evolving landscape, safeguarding fundamental rights is crucial, especially in addressing the aftermath of natural disasters, which disproportionately impact vulnerable communities. Climate change puts millions in danger and causes significant economic losses. With help from AI, governments and emergency responders can better anticipate crises, and deploy targeted interventions that save lives and minimize damage. Simultaneously, advancing individuals&#8217; rights to accessible digital technology is vital. Without it, access to education, healthcare, jobs, and essential services is limited, hindering the ability to fully participate and thrive in society.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.linkedin.com/pulse/world-seen-full-better-juan-m-lavista-ferres-y96hc/?trackingId=b4O%2B2RjSrTEfsBW1XMR0VA%3D%3D\" data-bi-cn=\"AI-driven solutions\">A world seen in full is a better world</a></div>\n</div>\n</div></div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h2 class=\"wp-block-heading\" id=\"empower-communities-to-take-anticipatory-action-with-early-warnings\">Empower communities to take anticipatory action with early warnings</h2>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-26 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large is-style-default\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/I-3-At-risk-communities-1024x576.jpg\" alt=\"aerial view of Mathura, India\" class=\"wp-image-1020981\" style=\"object-fit:cover\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/I-3-At-risk-communities-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/I-3-At-risk-communities-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/I-3-At-risk-communities-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/I-3-At-risk-communities-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/I-3-At-risk-communities-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/I-3-At-risk-communities-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/I-3-At-risk-communities-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/I-3-At-risk-communities-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/I-3-At-risk-communities-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/I-3-At-risk-communities.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Identifying at-risk communities</h4>\n\n\n\n<p>Traditional population mapping often overlooks communities amidst unplanned growth and migration. Microsoft&#8217;s AI for Good Lab, partnering with <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.planet.com/pulse/ihme-microsoft-and-planet-collaborate-to-map-climate-vulnerable-populations-in-unprecedented-detail/\">Planet<span class=\"sr-only\"> (opens in new tab)</span></a> and the <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.healthdata.org/understanding-where-people-live-and-how-climate-change-may-impact-them-can-help-us-plan-and-respond?linkId=100000230364872\">Institute for Health Metrics and Evaluation (IHME)<span class=\"sr-only\"> (opens in new tab)</span></a>, employs satellite imagery and AI to produce high-resolution population maps for better climate-event resilience.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-19 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/video/the-prompt-with-trevor-noah-episode-1-ihme-population-mapping/\" target=\"_blank\" rel=\"noreferrer noopener\">Video</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4_Forecasting-hunger_1400x788-1024x576.jpg\" alt=\"AI for Good - a rural African woman carrying a large basket of leaves on her head as she walks between trees on a farm\" class=\"wp-image-1015029\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4_Forecasting-hunger_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4_Forecasting-hunger_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4_Forecasting-hunger_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4_Forecasting-hunger_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4_Forecasting-hunger_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4_Forecasting-hunger_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4_Forecasting-hunger_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4_Forecasting-hunger_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4_Forecasting-hunger_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4_Forecasting-hunger_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Advanced forecasting for hunger</h4>\n\n\n\n<p>With 690 million people facing hunger in 2020, worsened by climate disasters, urgent assistance is vital. A study by Microsoft AI for Good Lab and Catholic Relief Services used machine learning to predict food insecurity in southern Malawi with 83% accuracy up to 4 months ahead, aiding informed decisions for proactive interventions.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-20 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/publication/food-security-analysis-and-forecasting-a-machine-learning-case-study-in-southern-malawi/\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://medium.com/data-policy/forecasting-food-insecurity-levels-in-near-real-time-using-a-machine-learning-framework-24b553f70aca\" target=\"_blank\" rel=\"noreferrer noopener\">Blog</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2022/12/AI-for-Good_SEEDS-BeatTheHeat_1400x788-1024x576.jpg\" alt=\"AI for Good - SEEDS #BeatTheHeat program in India\" class=\"wp-image-908319\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2022/12/AI-for-Good_SEEDS-BeatTheHeat_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/AI-for-Good_SEEDS-BeatTheHeat_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/AI-for-Good_SEEDS-BeatTheHeat_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/AI-for-Good_SEEDS-BeatTheHeat_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/AI-for-Good_SEEDS-BeatTheHeat_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/AI-for-Good_SEEDS-BeatTheHeat_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/AI-for-Good_SEEDS-BeatTheHeat_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/AI-for-Good_SEEDS-BeatTheHeat_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/AI-for-Good_SEEDS-BeatTheHeat_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/AI-for-Good_SEEDS-BeatTheHeat_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/AI-for-Good_SEEDS-BeatTheHeat_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Targeted early warnings with risk maps</h4>\n\n\n\n<p>Extreme heat poses a significant global threat, especially in densely populated areas where soaring indoor temperatures endanger vulnerable populations. <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.seedsindia.org/portfolio/ai-for-humanitarian-action-with-microsoft/\">SEEDS<span class=\"sr-only\"> (opens in new tab)</span></a>, in collaboration with Microsoft AI for Good Lab, developed AI models that provide targeted early warnings, resource allocation, and actionable insights.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-21 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://arxiv.org/abs/2211.11636\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/video/the-prompt-with-trevor-noah-episode-2-how-is-ai-protecting-vulnerable-communities/\" target=\"_blank\" rel=\"noreferrer noopener\">Video</a></div>\n</div>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<blockquote class=\"wp-block-quote is-style-spectrum--blue-green is-layout-flow wp-block-quote-is-layout-flow\">\n<p>&#8220;A very important component to measuring health and being able to provide services is knowing where people live in the world.\u201d</p>\n<cite>\u2013 Dr. Emmanuela Gakidou, The Institute for Health Metrics and Evaluation</cite></blockquote>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h2 class=\"wp-block-heading\" id=\"empower-communities-to-take-anticipatory-action-with-early-warnings\">Protect communities with accurate and rapid response to climate events</h2>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-27 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6_Turkey-earthquake_1400x788-1024x576.jpg\" alt=\"AI for Good - photo of people and heavy equipment cleaning up the aftermath of a major earthquake in Turkey\" class=\"wp-image-1015032\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6_Turkey-earthquake_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6_Turkey-earthquake_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6_Turkey-earthquake_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6_Turkey-earthquake_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6_Turkey-earthquake_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6_Turkey-earthquake_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6_Turkey-earthquake_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6_Turkey-earthquake_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6_Turkey-earthquake_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6_Turkey-earthquake_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Reliable data is vital to relief efforts</h4>\n\n\n\n<p>After earthquakes in Afghanistan and Turkey-Syria region, aid organizations faced obstacles due to limited data. With over 56,400 lives lost and millions impacted, Microsoft AI for Good Lab collaborated with Planet Labs to deploy AI models for rapid building identification and damage assessment, highlighting the importance of technology in improving disaster relief efforts.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-22 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Turkey-Earthquake-Report-2_MS.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.linkedin.com/pulse/world-seen-full-better-juan-m-lavista-ferres-y96hc%3FtrackingId=tNIzbLqvyUqRS%252B%252BFSOlHCw%253D%253D/?trackingId=tNIzbLqvyUqRS%2B%2BFSOlHCw%3D%3D\" target=\"_blank\" rel=\"noreferrer noopener\">Blog</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7_Lahaina-fire_1400x788-1024x576.jpg\" alt=\"AI for Good - an aerial view of Lahaina Maui that shows which areas were devastated by fire in August 2023\" class=\"wp-image-1015035\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7_Lahaina-fire_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7_Lahaina-fire_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7_Lahaina-fire_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7_Lahaina-fire_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7_Lahaina-fire_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7_Lahaina-fire_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7_Lahaina-fire_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7_Lahaina-fire_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7_Lahaina-fire_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7_Lahaina-fire_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Rapid response means saving lives</h4>\n\n\n\n<p>Over 100 lives were lost in the Lahaina fire, and 2,810 buildings were extensively damaged, causing displacement and over $6 billion in damage. AI-powered damage assessments completed within four hours with 97% accuracy facilitated swift provision of actionable maps to the American Red Cross and emergency groups.<br></p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-23 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://ojs.aaai.org/index.php/AAAI/article/view/17797\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.linkedin.com/pulse/fighting-wildfire-ai-juan-m-lavista-ferres%3FtrackingId=xvpXAYWBTs62a3OXcYwosw%253D%253D/?trackingId=xvpXAYWBTs62a3OXcYwosw%3D%3D\" target=\"_blank\" rel=\"noreferrer noopener\">Blog</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.geekwire.com/2023/wildfires-and-microsoft-and-amazon/\" target=\"_blank\" rel=\"noreferrer noopener\">Article</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2022/01/Geospatial_BuildingDamage-predictions_01-2022_1400x788-1024x576.jpg\" alt=\"Geospatial building damage assessment predictions\" class=\"wp-image-815104\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2022/01/Geospatial_BuildingDamage-predictions_01-2022_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2022/01/Geospatial_BuildingDamage-predictions_01-2022_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2022/01/Geospatial_BuildingDamage-predictions_01-2022_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2022/01/Geospatial_BuildingDamage-predictions_01-2022_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2022/01/Geospatial_BuildingDamage-predictions_01-2022_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2022/01/Geospatial_BuildingDamage-predictions_01-2022_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2022/01/Geospatial_BuildingDamage-predictions_01-2022_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2022/01/Geospatial_BuildingDamage-predictions_01-2022_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2022/01/Geospatial_BuildingDamage-predictions_01-2022_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2022/01/Geospatial_BuildingDamage-predictions_01-2022_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2022/01/Geospatial_BuildingDamage-predictions_01-2022_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Geospatial fundamentals</h4>\n\n\n\n<p>Learn basic skills on AI and Disaster Management through a specialized course on Coursera with our partner <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.coursera.org/learn/ai-and-disaster-management?specialization=ai-for-good\">Deeplearning.ai<span class=\"sr-only\"> (opens in new tab)</span></a>, exploring mitigation, preparation, response, and recovery. Or solve a machine learning problem end to end in <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/deep-learning-indaba/indaba-pracs-2023/blob/geoai/practicals/geospatial_machine_learning.ipynb\">Let&#8217;s Map Africa<span class=\"sr-only\"> (opens in new tab)</span></a>. Our materials are here to support your learning journey.<br></p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-24 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://github.com/deep-learning-indaba/indaba-pracs-2023/blob/geoai/practicals/geospatial_machine_learning.ipynb\" target=\"_blank\" rel=\"noreferrer noopener\">Github Library</a></div>\n</div>\n</div>\n</div>\n\n\n\n<div style=\"padding-bottom:0;padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<blockquote class=\"wp-block-quote is-style-spectrum--blue-green is-layout-flow wp-block-quote-is-layout-flow\">\n<p>\u201cOur preliminary damage assessment process is days ahead of our typical timeframe, thanks to a donation of time, expertise and technology from Microsoft\u2019s AI for Good Research Lab that is using AI models and satellite imagery to analyze the extent of the damage\u201d</p>\n<cite>\u2013 Brad Kieserman, American Red Cross</cite></blockquote>\t\t</div>\n\t</div>\n\n\t</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h2 class=\"wp-block-heading\" id=\"empower-communities-to-take-anticipatory-action-with-early-warnings\">Increasing equity by expanding accessibility and connectivity</h2>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-28 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9_Airband_1400x788-1024x576.jpg\" alt=\"AI for Good - an expansive view of barns, a home, and other outbuildings on a rural farm\" class=\"wp-image-1015038\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9_Airband_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9_Airband_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9_Airband_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9_Airband_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9_Airband_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9_Airband_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9_Airband_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9_Airband_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9_Airband_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9_Airband_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Close the connectivity gap</h4>\n\n\n\n<p>Approximately 2.7 billion people globally lack internet access. Our commitment with partners aims to provide internet access to 250 million people in underserved communities worldwide through the Airband Initiative, enabling participation in education, work, telehealth, and beyond.</p>\n\n\n\n<div class=\"wp-block-buttons is-content-justification-left is-content-justification-left is-layout-flex wp-container-core-buttons-is-layout-25 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/corporate-responsibility/airband\" target=\"_blank\" rel=\"noreferrer noopener\">Airband Initiative</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10_Digital-equity_1400x788-1024x576.jpg\" alt=\"AI for Good - an overhead view of a marble tabletop with a tablet showing a state map of the USA\" class=\"wp-image-1015041\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10_Digital-equity_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10_Digital-equity_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10_Digital-equity_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10_Digital-equity_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10_Digital-equity_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10_Digital-equity_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10_Digital-equity_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10_Digital-equity_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10_Digital-equity_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10_Digital-equity_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Identify digital equity gaps</h4>\n\n\n\n<p>Accurate mapping is essential to disburse broadband funds efficiently and maximize resources and investments in communities of highest need. This tool was designed to empower state agencies, nonprofits, and policymakers to identify regions within their state with significant digital equity gaps.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-26 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://aka.ms/DigitalEquityMaps\" target=\"_blank\" rel=\"noreferrer noopener\">Visualization</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11_Accesibility_1400x788-1024x576.jpg\" alt=\"AI for Good - photo of a blind or low-vision woman working on a computer\" class=\"wp-image-1015044\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11_Accesibility_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11_Accesibility_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11_Accesibility_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11_Accesibility_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11_Accesibility_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11_Accesibility_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11_Accesibility_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11_Accesibility_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11_Accesibility_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11_Accesibility_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Promote accessibility and inclusion</h4>\n\n\n\n<p>We are committed to accessible technology and advocating for policies that recognize accessibility as a fundamental right. With over 285 million worldwide experiencing vision impairment, our aim is to enhance the Seeing AI Banknote recognition app for individuals with low vision.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-27 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/publication/banknote-net-open-dataset-for-assistive-universal-currency-recognition/\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://github.com/microsoft/banknote-net\" target=\"_blank\" rel=\"noreferrer noopener\">Github</a></div>\n</div>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p></p>\n\n\n\n<p></p>\n\n\n\n<p></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Microsoft is committed to strengthening communities and empowering the organizations that help them thrive. We have a responsibility to protect people\u2019s fundamental rights, and help all communities succeed.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1016418,
        "date": "2024-04-02T08:57:43",
        "slug": "advance-sustainability-ai-for-good",
        "title": "Advance Sustainability &#8211; AI for Good",
        "link": "https://www.microsoft.com/en-us/research/project/advance-sustainability-ai-for-good/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Advance_Sustainability_1920x720-2.jpg\" class=\"attachment-full size-full\" alt=\"A body of water with a mountain in the background\" style=\"object-position: 76% 53%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Advance_Sustainability_1920x720-2.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Advance_Sustainability_1920x720-2-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Advance_Sustainability_1920x720-2-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Advance_Sustainability_1920x720-2-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Advance_Sustainability_1920x720-2-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Advance_Sustainability_1920x720-2-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/AI4Good-Advance_Sustainability_1920x720-2-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/ai-for-good-research-lab/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"AI for Good Lab\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAI for Good Lab\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"advance-sustainability\">Advance sustainability</h1>\n\n\n\n<p></p>\n\n\n\n<div class=\"wp-block-button is-style-fill\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/corporate-responsibility/sustainability\" target=\"_blank\" rel=\"noreferrer noopener\">Learn more</a></div>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<div class=\"wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile is-vertically-aligned-top\" style=\"grid-template-columns:40% auto\" data-bi-an=\"media-text\"><figure class=\"wp-block-media-text__media\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-to-protect-Amazon-1024x576.jpg\" alt=\"River flowing through the Amazon\" class=\"wp-image-1017894 size-full\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-to-protect-Amazon-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-to-protect-Amazon-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-to-protect-Amazon-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-to-protect-Amazon-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-to-protect-Amazon-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-to-protect-Amazon-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-to-protect-Amazon-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-to-protect-Amazon-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-to-protect-Amazon-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-to-protect-Amazon.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<h2 class=\"wp-block-heading\" id=\"accelerating-sustainability-with-ai\">Accelerating sustainability with AI</h2>\n\n\n\n<p>Climate change is the defining issue of our time. Aligned with the UN Sustainable Development Goals, we are dedicated to creating a sustainable future. To navigate and mitigate its worst effects, reliable climate data is paramount for governments and decision-makers. That&#8217;s why one of our core initiatives focuses on mapping and measuring solar and wind installations, accelerating the clean energy transition. Moreover, with 25% of the planet&#8217;s species facing extinction, urgent action is imperative to protect biodiversity. Our commitment to transparency in sharing our learnings drives us to build a world better than the one we found.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.linkedin.com/pulse/using-ai-protect-amazon-juan-m-lavista-ferres/\" target=\"_blank\" rel=\"noreferrer noopener\" data-bi-cn=\"Accelerating sustainability with AI\">Using AI to protect the Amazon</a></div>\n</div>\n</div></div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h2 class=\"wp-block-heading\" id=\"empower-communities-to-take-anticipatory-action-with-early-warnings\">Innovations in measurements and action</h2>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-29 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-3-Mapping-energy_1400x788-1024x576.jpg\" alt=\"AI for Good - wind farm at sunrise\" class=\"wp-image-1016451\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-3-Mapping-energy_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-3-Mapping-energy_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-3-Mapping-energy_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-3-Mapping-energy_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-3-Mapping-energy_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-3-Mapping-energy_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-3-Mapping-energy_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-3-Mapping-energy_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-3-Mapping-energy_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-3-Mapping-energy_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Mapping the planet to clean energy</h4>\n\n\n\n<p>Global renewable energy transition requires accurate data on solar and wind installations. In collaboration with Partnerships: Planet and The Nature Conservancy, the <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.globalrenewableswatch.org/\">Global Renewables Watch<span class=\"sr-only\"> (opens in new tab)</span></a> is a living atlas using AI and satellite imagery to map utility-scale solar and wind installations worldwide.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-29 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/video/global-renewables-watch-ai-for-good-lab-geospatial/\" target=\"_blank\" rel=\"noreferrer noopener\">Video</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://github.com/microsoft/solar-farms-mapping\" target=\"_blank\" rel=\"noreferrer noopener\">Github</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4-Forecasting-solar-panels_1400x788-1024x576.jpg\" alt=\"AI for Good - solar panels on the side of a hill\" class=\"wp-image-1016454\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4-Forecasting-solar-panels_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4-Forecasting-solar-panels_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4-Forecasting-solar-panels_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4-Forecasting-solar-panels_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4-Forecasting-solar-panels_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4-Forecasting-solar-panels_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4-Forecasting-solar-panels_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4-Forecasting-solar-panels_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4-Forecasting-solar-panels_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-4-Forecasting-solar-panels_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Forecasting solar panel efficiency with AI</h4>\n\n\n\n<p>Solar energy is a vital tool for sustainability, with panels converting sunlight into stored energy. However, photovoltaic degradation happens over time challenging solar energy development. AI for Good Lab with partners at MIT developed AI tools to help forecast, advance, and maximize efficiency.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-30 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://chemrxiv.org/engage/chemrxiv/article-details/641fb58762fecd2a837019bc\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-5-Tracking-glacier-lakes_1400x788-1024x576.jpg\" alt=\"AI for Good - a view of a snow-covered mountain behind a glacial lake\" class=\"wp-image-1016460\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-5-Tracking-glacier-lakes_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-5-Tracking-glacier-lakes_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-5-Tracking-glacier-lakes_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-5-Tracking-glacier-lakes_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-5-Tracking-glacier-lakes_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-5-Tracking-glacier-lakes_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-5-Tracking-glacier-lakes_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-5-Tracking-glacier-lakes_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-5-Tracking-glacier-lakes_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-5-Tracking-glacier-lakes_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">AI maps and tracks glacier lakes </h4>\n\n\n\n<p>Understanding climate change in regions sensitive to environmental shifts is crucial. Glacial lakes, formed by glacier retreat, pose threats when excessive melting leads to floods downstream. The AI for Good Lab collaborated with ICIMOD to leverage mapping and automation to enhance flood risk.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-31 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/project/glacier-mapping/\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://twitter.com/MSFTIssues/status/1355206409487589377\" target=\"_blank\" rel=\"noreferrer noopener\">Video</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://news.microsoft.com/on-the-issues/2021/01/12/ai-open-data-glacial-melt-himalaya/\" target=\"_blank\" rel=\"noreferrer noopener\">Article</a></div>\n</div>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<blockquote class=\"wp-block-quote is-style-spectrum--blue-green is-layout-flow wp-block-quote-is-layout-flow\">\n<p>&#8220;You can\u2019t solve a problem if you can\u2019t measure it. Using AI tools, we can actually start measuring the problem.\u201d</p>\n<cite>\u2013 Juan M. Lavista Ferres, CVP and Chief Data Scientist, Microsoft</cite></blockquote>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h2 class=\"wp-block-heading\" id=\"empower-communities-to-take-anticipatory-action-with-early-warnings\">A collective responsibility to protecting our planet</h2>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-30 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6-Amazon_1400x788-1024x576.jpg\" alt=\"AI for Good - a green frog peering out from behind a dark green leaf\" class=\"wp-image-1016463\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6-Amazon_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6-Amazon_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6-Amazon_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6-Amazon_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6-Amazon_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6-Amazon_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6-Amazon_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6-Amazon_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6-Amazon_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-6-Amazon_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">AI-powered conservation for the Amazon</h4>\n\n\n\n<p>Deforestation threatens the health of our planet, with the Amazon losing nearly 2 million hectares in 2022 alone. Project Guacamaya, in partnership with Humboldt Institute, uses AI solutions: satellite analysis detects illegal deforestation, camera traps streamline wildlife monitoring, and bioacoustics aid species identification.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-32 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://news.microsoft.com/source/latam/features/ai/amazon-ai-rainforest-deforestation/?lang=en\" target=\"_blank\" rel=\"noreferrer noopener\">Article</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.linkedin.com/pulse/using-ai-protect-amazon-juan-m-lavista-ferres/\" target=\"_blank\" rel=\"noreferrer noopener\">Blog</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7-Conflict-resolution_1400x788-1024x576.jpg\" alt=\"AI for Good - a group of four male goat herders in rural Africa\" class=\"wp-image-1016466\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7-Conflict-resolution_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7-Conflict-resolution_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7-Conflict-resolution_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7-Conflict-resolution_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7-Conflict-resolution_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7-Conflict-resolution_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7-Conflict-resolution_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7-Conflict-resolution_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7-Conflict-resolution_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-7-Conflict-resolution_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Human needs & wildlife conservation</h4>\n\n\n\n<p>In Kenya\u2019s Maasai Mara, population expansion and resource scarcity threaten the delicate balance of biodiversity. Accurate mapping of conflict zones is crucial for targeted interventions. With AI and expertise from Smithsonian and Kenya Wildlife Trust, sustainable coexistence is achievable.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-33 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://arxiv.org/pdf/2403.02736.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-8-Eco-tourism_1400x788-1024x576.jpg\" alt=\"AI for Good - two snorkelers floating in a brilliant blue lagoon\" class=\"wp-image-1016469\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-8-Eco-tourism_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-8-Eco-tourism_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-8-Eco-tourism_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-8-Eco-tourism_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-8-Eco-tourism_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-8-Eco-tourism_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-8-Eco-tourism_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-8-Eco-tourism_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-8-Eco-tourism_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-8-Eco-tourism_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Sustainable tourism with AI</h4>\n\n\n\n<p>Amidst climate change, island nations turn to the blue economy for sustainable ocean resource management. With The Nature Institute our AI-driven approach in the Eastern Caribbean maps tourist activities, turning tourism into a sustainable economic driver while preserving natural resources.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-34 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.sciencedirect.com/science/article/pii/S030147972300484X?via%3Dihub\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://blog.nature.org/2023/04/06/the-value-of-words-pictures/\" target=\"_blank\" rel=\"noreferrer noopener\">Blog</a></div>\n</div>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<blockquote class=\"wp-block-quote is-style-spectrum--blue-green is-layout-flow wp-block-quote-is-layout-flow\">\n<p>&#8220;We need to be using technology and innovation to think outside of the box, we have powerful tools at hand to promote change in society.&#8221;</p>\n<cite>\u2013 Diego Ochoa, Alexander von Humboldt Institute</cite></blockquote>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h2 class=\"wp-block-heading\" id=\"empower-communities-to-take-anticipatory-action-with-early-warnings\">Preserving our natural heritage through wildlife conservation</h2>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-31 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-Giraffe-social-networks_1400x788-1024x576.jpg\" alt=\"AI for Good - two giraffes at sunset\" class=\"wp-image-1016472\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-Giraffe-social-networks_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-Giraffe-social-networks_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-Giraffe-social-networks_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-Giraffe-social-networks_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-Giraffe-social-networks_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-Giraffe-social-networks_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-Giraffe-social-networks_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-Giraffe-social-networks_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-Giraffe-social-networks_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-9-Giraffe-social-networks_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Giraffe social dynamics in the face of change</h4>\n\n\n\n<p>Environmental changes affect animal habitats and behaviors, impacting reproductive success. Our collaboration with Wild Nature Institute uncovers male giraffe dispersal patterns and social structures shaped by landscape features, crucial for assessing endangered Masai giraffe populations.</p>\n\n\n\n<div class=\"wp-block-buttons is-content-justification-left is-content-justification-left is-layout-flex wp-container-core-buttons-is-layout-35 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/publication/social-connectedness-and-movements-among-communities-of-giraffes-vary-by-sex-and-age-class/\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.psu.edu/news/research/story/male-giraffes-are-more-socially-connected-females/\" target=\"_blank\" rel=\"noreferrer noopener\">Article</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10-Whale-satellites_1400x788-1024x576.jpg\" alt=\"AI for Good - aerial view of a mother whale with a calf swimming side by side at the surface\" class=\"wp-image-1016475\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10-Whale-satellites_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10-Whale-satellites_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10-Whale-satellites_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10-Whale-satellites_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10-Whale-satellites_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10-Whale-satellites_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10-Whale-satellites_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10-Whale-satellites_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10-Whale-satellites_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-10-Whale-satellites_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Monitoring whale populations with satellites</h4>\n\n\n\n<p>Climate change and human impacts threaten marine biodiversity, with whales serving as vital indicators of ocean health. The Geospatial Artificial Intelligence for Animals (GAIA) initiative uses high-resolution satellite imagery and an active learning process to identify whale species efficiently.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-36 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.mdpi.com/2077-1312/11/3/595\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11-Pytorch_1400x788-1024x576.jpg\" alt=\"AI for Good - spotted cat against dark green vegetation background\" class=\"wp-image-1016445\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11-Pytorch_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11-Pytorch_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11-Pytorch_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11-Pytorch_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11-Pytorch_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11-Pytorch_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11-Pytorch_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11-Pytorch_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11-Pytorch_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI4Good_I-11-Pytorch_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Pytorch-Wildlife empowers collaboration</h4>\n\n\n\n<p>Pytorch-Wildlife serves as a versatile platform for developing and sharing AI conservation models, from camera traps and overhead images to bioacoustics. Achieving meaningful progress in conservation demands collective action and engagement.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-37 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://github.com/microsoft/CameraTraps\" target=\"_blank\" rel=\"noreferrer noopener\">Github</a></div>\n</div>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Climate change requires swift, collective action and technological innovation. We are committed to meeting our own goals while enabling others to do the same.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1018536,
        "date": "2024-04-01T11:16:56",
        "slug": "genai-for-industry",
        "title": "GenAI for Industry",
        "link": "https://www.microsoft.com/en-us/research/project/genai-for-industry/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GenAI-for-Industry_header_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"GenAI for Industry - AI-generated image compilation of industrial blueprints from renewable energy to transporation and manufacturing\" style=\"object-position: 81% 48%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GenAI-for-Industry_header_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GenAI-for-Industry_header_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GenAI-for-Industry_header_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GenAI-for-Industry_header_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GenAI-for-Industry_header_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GenAI-for-Industry_header_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GenAI-for-Industry_header_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/research-for-industry/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"Research for Industry\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tResearch for Industry\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"genai-for-industry\">GenAI for Industry</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<h2 class=\"wp-block-heading\" id=\"how-can-industry-benefit-from-generative-ai\">How can industry benefit from generative AI?</h2>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-32 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:60%\">\n<p>Large language models (LLMs) are powerful tools that can generate natural language texts and answer complex questions across various domains. Microsoft Research is exploring the use of LLMs for various industry, aiming to solve domain specific challenges. Our work, highlighted in recent projects and publications, showcases applications to demonstrate how LLMs can transform vertical industries, like agriculture.</p>\n\n\n\n<p>Our research is focused in these four areas of generative AI for industry:</p>\n\n\n\n<ul>\n<li>Customizing LLMs</li>\n\n\n\n<li>Small language models and edge</li>\n\n\n\n<li>Multi-modal GenAI</li>\n\n\n\n<li>Foundation models for industry</li>\n</ul>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:40%\">\n<figure class=\"wp-block-image size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions.jpg\" alt=\"Satellite map of a town\" class=\"wp-image-1017900\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/AI-Driven-Solutions-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></figure>\n</div>\n</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-33 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<h4 class=\"wp-block-heading\" id=\"customizing-llms-industry-ai-knowledge-plugins-and-safety\">Customizing LLMs</h4>\n\n\n\n<p><strong>Scenario</strong>: A company wants to use a customized LLM that was built using their own documents, custom data. This requires customer data processing pipelines for finetuning or Retrieval Augmented Generation (RAG). The company also wants to use plugins that can integrate the LLM with their existing system of record and provide interactive feedback to the internal or external customers. The generated answers from complies with the privacy and ethical standards of the industry and company policies.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<h4 class=\"wp-block-heading\" id=\"customizing-llms-industry-ai-knowledge-plugins-and-safety\">Small language models and edge</h4>\n\n\n\n<p><strong>Scenario</strong>: A company wants to use a small LLM to create immersive and dynamic narratives for their online versions. The LLM needs to run on the edge devices without relying on constant cloud servers or internet connection. The LLM also needs to adapt to the users&#8217; preferences and actions and generate realistic and diverse dialogues and scenarios. The company also wants to use a plugin that can optimize the LLM for low-latency and mobile operation, as well as ensure safety and fairness.</p>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-34 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<h4 class=\"wp-block-heading\" id=\"customizing-llms-industry-ai-knowledge-plugins-and-safety\">Multi-modal GenAI</h4>\n\n\n\n<p><strong>Scenario</strong>: A company wants to use a multi-modal LLM to generate captions and summaries for their video and audio content. The LLM needs to be able to process and understand different types of data, such as images, speech, music, and other non-text formats. The LLM also needs to be able to generate coherent and informative captions and summaries that capture the main points and emotions of the content. The company also wants to use a plugin that can enhance the quality and diversity of the LLM outputs, as well as provide feedback and editing options to the content creators.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<h4 class=\"wp-block-heading\" id=\"customizing-llms-industry-ai-knowledge-plugins-and-safety\">Foundation models for industry</h4>\n\n\n\n<p><strong>Scenario</strong>: A company wants to use simulations and custom computation to build a custom AI model that will answer target scenario questions. These are foundational models, custom trained for specific domains. These along with previous models can help holistically answer customers\u2019 queries.</p>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:32px; padding-top:32px\" class=\"wp-block-msr-immersive-section alignfull row has-background has-lighter-gray-background-color has-text-color has-black-color wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h3 class=\"wp-block-heading has-text-align-center\" id=\"key-industries\">Key industries</h3>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-35 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2022/10/FarmVibes-Nelson-trail-03_1400x788-1024x576.jpg\" alt=\"FarmVibes - man walking through a wheat field towards a distant barn (Photo by Dan DeLong for Microsoft)\" class=\"wp-image-883029\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2022/10/FarmVibes-Nelson-trail-03_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2022/10/FarmVibes-Nelson-trail-03_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2022/10/FarmVibes-Nelson-trail-03_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2022/10/FarmVibes-Nelson-trail-03_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2022/10/FarmVibes-Nelson-trail-03_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2022/10/FarmVibes-Nelson-trail-03_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2022/10/FarmVibes-Nelson-trail-03_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2022/10/FarmVibes-Nelson-trail-03_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2022/10/FarmVibes-Nelson-trail-03_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2022/10/FarmVibes-Nelson-trail-03_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2022/10/FarmVibes-Nelson-trail-03_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"agriculture\">Agriculture</h4>\n\n\n\n<p>Answers can help address the specific queries of different personas within the Agri-Food ecosystem, from farmers and policymakers, consumer, retailers to financial service providers and sustainability consultants.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/project/genai-for-industry/articles/generative-ai-in-agriculture\">GenAI in agriculture</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/GenAI-for-Industry_manufacturing_1400x788-1024x576.jpg\" alt=\"engineer check and control automation robot arms machine in intelligent factory industrial, welding robotics and digital manufacturing operation\" class=\"wp-image-1020474\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/GenAI-for-Industry_manufacturing_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/GenAI-for-Industry_manufacturing_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/GenAI-for-Industry_manufacturing_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/GenAI-for-Industry_manufacturing_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/GenAI-for-Industry_manufacturing_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/GenAI-for-Industry_manufacturing_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/GenAI-for-Industry_manufacturing_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/GenAI-for-Industry_manufacturing_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/GenAI-for-Industry_manufacturing_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/GenAI-for-Industry_manufacturing_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"agriculture\">Manufacturing</h4>\n\n\n\n<p>Streamlines operations, enhancing efficiency and innovation across the manufacturing sector, from production floor managers to supply chain analysts.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/04/Soundscape-woman-grocery-shopping-1024x576.jpg\" alt=\"Image of a lady shopping in a grocery store\" class=\"wp-image-650166\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/04/Soundscape-woman-grocery-shopping-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/04/Soundscape-woman-grocery-shopping-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/04/Soundscape-woman-grocery-shopping-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/04/Soundscape-woman-grocery-shopping-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2020/04/Soundscape-woman-grocery-shopping-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2020/04/Soundscape-woman-grocery-shopping-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2020/04/Soundscape-woman-grocery-shopping-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2020/04/Soundscape-woman-grocery-shopping-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2020/04/Soundscape-woman-grocery-shopping-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2020/04/Soundscape-woman-grocery-shopping.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"agriculture\">Consumer goods / Retail</h4>\n\n\n\n<p>Transforms the retail experience, offering personalized assistance to store managers, inventory specialists, and customer service representatives. Optimizes product lifecycle management and market analysis, benefiting brand managers and consumer insights analysts in the consumer goods industry.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/07/CSR_SustainabilityOverview_multi_2021-07_1400x788-1024x576.jpg\" alt=\"sustainability: photo of solar panels at sunrise\" class=\"wp-image-763684\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/07/CSR_SustainabilityOverview_multi_2021-07_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2021/07/CSR_SustainabilityOverview_multi_2021-07_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2021/07/CSR_SustainabilityOverview_multi_2021-07_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2021/07/CSR_SustainabilityOverview_multi_2021-07_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2021/07/CSR_SustainabilityOverview_multi_2021-07_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2021/07/CSR_SustainabilityOverview_multi_2021-07_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2021/07/CSR_SustainabilityOverview_multi_2021-07_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2021/07/CSR_SustainabilityOverview_multi_2021-07_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2021/07/CSR_SustainabilityOverview_multi_2021-07_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2021/07/CSR_SustainabilityOverview_multi_2021-07_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2021/07/CSR_SustainabilityOverview_multi_2021-07_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"agriculture\">Sustainability</h4>\n\n\n\n<p>Supports sustainability initiatives, providing actionable insights for environmental consultants and corporate sustainability officers to drive eco-friendly practices in their organizations.</p>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Microsoft Research is exploring the use of LLMs for various industry, aiming to solve domain specific challenges. Our work, highlighted in recent projects and publications, showcases applications to demonstrate how LLMs can transform vertical industries, like agriculture.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1018134,
        "date": "2024-03-28T14:28:52",
        "slug": "covomix",
        "title": "CoVoMix",
        "link": "https://www.microsoft.com/en-us/research/project/covomix/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"covomix\">CoVoMix</h1>\n\n\n\n<p>Advancing Zero-shot Speech Generation for Human-like Multi-talker Conversation</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n<div style=\"height:28px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<p>We introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation. In addition, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple speakers engaging in multiple rounds of conversation. These dialogues, generated within a single channel, are characterized by seamless speech transitions, including overlapping speech, and appropriate paralinguistic behaviors such as laughter and coughing.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://arxiv.org/abs/2404.06690\">Paper&#8217;s Link</a></div>\n</div>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"286\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/intro-1024x286.png\" alt=\"diagram\" class=\"wp-image-1023120\" style=\"width:1125px;height:auto\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/intro-1024x286.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/intro-300x84.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/intro-768x214.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/intro-1536x429.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/intro-2048x572.png 2048w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/intro-240x67.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:60px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<figure class=\"wp-block-embed is-provider-youtube wp-block-embed-youtube\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"CoVoMix Demo\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/OZPkBXhWT78?feature=oembed&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</div></figure>\n\n\n\n<div style=\"height:60px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<h2 class=\"wp-block-heading\" id=\"dialogue\">Dialogue</h2>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-38 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-36 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h6 class=\"wp-block-heading\" id=\"transcription\">Transcription</h6>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h6 class=\"wp-block-heading\" id=\"ground-truth-1\">Ground truth</h6>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-37 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\">\n<h6 class=\"wp-block-heading has-text-align-left\" id=\"prompt\">Prompt</h6>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\">\n<h6 class=\"wp-block-heading\" id=\"covosingle\">CoVoSingle</h6>\n\n\n\n<h6 class=\"wp-block-heading\" id=\"covomix-1\">CoVoMix</h6>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<div class=\"wp-block-columns are-vertically-aligned-center is-layout-flex wp-container-core-columns-is-layout-41 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-39 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>he was in jail for fourteen times and they finally deported him |&nbsp; fourteen times | yeah his family spent over two hundred thousand dollars keeping him here | wow | and then finally they said no that&#8217;s it he&#8217;s out can&#8217;t even come back to visit</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GT1-fe_03_09727_028.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-40 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/PT1-fe_03_09727_028.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/CS1-fe_03_09727_028.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/CM1-fe_03_09727_028.wav\"></audio></figure>\n</div>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<div class=\"wp-block-columns are-vertically-aligned-center is-layout-flex wp-container-core-columns-is-layout-44 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-42 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>which is uh very strange it&#8217;s not something i ever thought would happen | yeah that&#8217;s not good | no</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GT2-fe_03_09724_023.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-43 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/PT2-fe_03_09724_023.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/CS2-fe_03_09724_023.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/CM2-fe_03_09724_023.wav\"></audio></figure>\n</div>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-47 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-45 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>so i&#8217;m not i don&#8217;t know what it is i don&#8217;t know what the minimum wage is or | uh it&#8217;s five fifteen now it was like four seventy five something like that | my gosh</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GT3-fe_03_09720_004.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-46 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/PT3-fe_03_09720_004.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/CS3-fe_03_09720_004.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/CM3-fe_03_09720_004.wav\"></audio></figure>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-50 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-48 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>something around there | yeah that&#8217;s that&#8217;s good | i don&#8217;t think they&#8217;d ever get a divorce | no i know</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GT4-fe_03_09715_029.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-49 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/PT4-fe_03_09715_029.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/CS4-fe_03_09715_029.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/CM4-fe_03_09715_029.wav\"></audio></figure>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-center is-layout-flex wp-container-core-columns-is-layout-53 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-51 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>my life was like consumed with the television and it was it was just sad i it that was awful [laughter] | and i remember the i think the day after and like gas prices went up to like three bucks a gallon | oh i know | everybody was like filling up with gas in the town i live in panicking and mhm | really wow</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/GT5-fe_03_06386_033.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-52 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/PT5-fe_03_06386_033.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/CS5-fe_03_06386_033.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/CM5-fe_03_06386_033.wav\"></audio></figure>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-center is-layout-flex wp-container-core-columns-is-layout-56 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-54 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>hard choice to make especially when you get peer pressure and then once you start doing it hey look now i&#8217;m cool | mhm | but in reality if you actually had to do that to be cool you&#8217;re hanging with the wrong parents anyways [laughter] | right right i think my brother in law and his</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GT6-fe_03_06333_011-6605d31b42eb5.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns are-vertically-aligned-center is-layout-flex wp-container-core-columns-is-layout-55 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/PT6-fe_03_06333_011.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/CS6-fe_03_06333_011.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/CM6-fe_03_06333_011.wav\"></audio></figure>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n</div>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<h2 class=\"wp-block-heading\" id=\"dialogue\">Monologue</h2>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-59 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-57 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h6 class=\"wp-block-heading\" id=\"transcription\">Transcription</h6>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h6 class=\"wp-block-heading\" id=\"ground-truth-1\">Ground truth</h6>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-58 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\">\n<h6 class=\"wp-block-heading has-text-align-left\" id=\"prompt\">Prompt</h6>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\">\n<h6 class=\"wp-block-heading\" id=\"covosingle\">VoiceBox-Style</h6>\n\n\n\n<h6 class=\"wp-block-heading\" id=\"covosingle\">CoVoSingle</h6>\n\n\n\n<h6 class=\"wp-block-heading\" id=\"covomix-1\">CoVoMix</h6>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<div class=\"wp-block-columns are-vertically-aligned-center is-layout-flex wp-container-core-columns-is-layout-62 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-60 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>totally offended i and like they would be like oh that&#8217;s a yankee for you but like they like they would they would be like um</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_09764-B_076-MGT1.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-61 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_09764-B_076-MPT1.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_09764-B_076-MVB1.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_09764-B_076-MCS1.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_09764-B_076-MCM1.wav\"></audio></figure>\n</div>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<div class=\"wp-block-columns are-vertically-aligned-center is-layout-flex wp-container-core-columns-is-layout-65 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-63 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>um well a way to get more money at your job and that&#8217;s pretty much it you know</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_09758-B_139-MGT2.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-64 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_09758-B_139-MPT2.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_09758-B_139-MVB2.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_09758-B_139-MCS2.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_09758-B_139-MCM2.wav\"></audio></figure>\n</div>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-68 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-66 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>right i totally agree it gets in your clothes gets in your hair and for a nonsmoker they don&#8217;t realize how sensitive i think th um</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06379-A_039-MGT3.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-67 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06379-A_039-MPT3.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06379-A_039-MVB3.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06379-A_039-MCS3.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06379-A_039-MCM3.wav\"></audio></figure>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-71 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-69 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>yeah it&#8217;s really not a pleasant odor and it it&#8217;s it&#8217;s horrible my fiance smokes um on a r daily basis he smokes like a pack and a half a week or more i don&#8217;t know he doesn&#8217;t tell me</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06373-B_026-MGT4.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-70 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06373-B_026-MPT4.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06373-B_026-MVB4.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06373-B_026-MCS4.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06373-B_026-MCM4.wav\"></audio></figure>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-center is-layout-flex wp-container-core-columns-is-layout-74 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-72 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>and i i was i was saddened too because so many people do have a problem with tobacco and it is very addictive and i&#8217;m i have family members that</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06372-B_090-MGT5.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-73 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06372-B_090-MPT5.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06372-B_090-MVB5.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06372-B_090-MCS5.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06372-B_090-MCM5.wav\"></audio></figure>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-center is-layout-flex wp-container-core-columns-is-layout-77 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-75 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>right yeah and i i think that&#8217;s what you have to do personally to get to that point i have my my dad kinda got the same way you know it was too much of a hassle to go outside and if it was raining and you know just too much of an inconvenience</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06346-B_029-MGT7.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns are-vertically-aligned-center is-layout-flex wp-container-core-columns-is-layout-76 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06346-B_029-MPT7.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06346-B_029-MVB7.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06346-B_029-MCS7.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06346-B_029-MCM7.wav\"></audio></figure>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-center is-layout-flex wp-container-core-columns-is-layout-80 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-78 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>and the american culture is so big there that you know because most of the people i saw smoking</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio aligncenter\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06328-A_110-MGT8.wav\"></audio></figure>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-columns are-vertically-aligned-center is-layout-flex wp-container-core-columns-is-layout-79 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06328-A_110-MPT8.wav\"></audio></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-center is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06328-A_110-MVB8.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06328-A_110-MCS8.wav\"></audio></figure>\n\n\n\n<figure class=\"wp-block-audio\"><audio controls src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/fe_03_06328-A_110-MCM8.wav\"></audio></figure>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-</p>\n</div>\n\n\n\n\n\n<p></p>\n\n\n\n\n\n<p></p>\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Advancing Zero-shot Speech Generation for Human-like Multi-talker Conversation We introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation. In addition, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1017939,
        "date": "2024-03-22T17:14:57",
        "slug": "efficient-ai",
        "title": "Efficient AI",
        "link": "https://www.microsoft.com/en-us/research/project/efficient-ai/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RoboticsWorkshop_WebBanner_1920x720_Dark-Spectrum.jpg\" class=\"attachment-full size-full\" alt=\"robotics workshop header - server hallway with a color gradient overlay\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RoboticsWorkshop_WebBanner_1920x720_Dark-Spectrum.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RoboticsWorkshop_WebBanner_1920x720_Dark-Spectrum-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RoboticsWorkshop_WebBanner_1920x720_Dark-Spectrum-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RoboticsWorkshop_WebBanner_1920x720_Dark-Spectrum-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RoboticsWorkshop_WebBanner_1920x720_Dark-Spectrum-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RoboticsWorkshop_WebBanner_1920x720_Dark-Spectrum-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RoboticsWorkshop_WebBanner_1920x720_Dark-Spectrum-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"efficient-ai\">Efficient AI</h1>\n\n\n\n<p>Making Azure&#8217;s big bet possible</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Recent innovations in generative large language models (LLMs) have made their applications and use-cases ubiquitous. This has led to large-scale deployments of these models,<br>using complex, expensive, and power-hungry AI accelerators, most commonly GPUs. These developments make LLM training and inference efficiency an important challenge.</p>\n\n\n\n<p>In the <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://aka.ms/azrs\">Azure Research &#8211; Systems<span class=\"sr-only\"> (opens in new tab)</span></a> group we are working on improving the Azure infrastructure including hardware, power, and serving.</p>\n\n\n\n<p>Some of the work we have done so far:</p>\n\n\n\n<ul>\n<li>Splitwise: <a href=\"https://www.microsoft.com/en-us/research/publication/splitwise-efficient-generative-llm-inference-using-phase-splitting/\">Efficient generative LLM inference using phase splitting</a></li>\n\n\n\n<li>POLCA: <a href=\"https://www.microsoft.com/en-us/research/publication/polca-power-oversubscription-in-llm-cloud-providers/\">Power Oversubscription in LLM Cloud Providers</a></li>\n</ul>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Making Azure&#8217;s big bet possible Recent innovations in generative large language models (LLMs) have made their applications and use-cases ubiquitous. This has led to large-scale deployments of these models,using complex, expensive, and power-hungry AI accelerators, most commonly GPUs. These developments make LLM training and inference efficiency an important challenge. In the Azure Research &#8211; Systems [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 982563,
        "date": "2024-03-13T11:33:11",
        "slug": "garnet",
        "title": "Garnet",
        "link": "https://www.microsoft.com/en-us/research/project/garnet/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-auburn card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1921\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/03/garnet-bg-1920x720-1.png\" class=\"attachment-full size-full\" alt=\"Garnet background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/03/garnet-bg-1920x720-1.png 1921w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/garnet-bg-1920x720-1-300x112.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/garnet-bg-1920x720-1-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/garnet-bg-1920x720-1-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/garnet-bg-1920x720-1-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/garnet-bg-1920x720-1-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/garnet-bg-1920x720-1-240x90.png 240w\" sizes=\"(max-width: 1921px) 100vw, 1921px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"garnet\">Garnet</h1>\n\n\n\n<p>A high-performance, extensible, and low-latency remote cache-store from&nbsp;Microsoft Research</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<h2 class=\"wp-block-heading\" id=\"introducing-garnet\">Introducing Garnet</h2>\n\n\n\n<p>Garnet is a research project from Microsoft Research. It is a remote cache-store designed to offer high performance, extensibility, and low latency. Garnet is thread-scalable within a single node. It also supports sharded cluster execution, with replication, checkpointing, failover, and transactions. It can operate over main memory as well as tiered storage (such as SSD and&nbsp;Azure Storage). Garnet supports a rich API surface and a powerful extensibility model.</p>\n\n\n\n<p>Garnet uses Redis&#8217; <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://redis.io/docs/reference/protocol-spec/\">RESP<span class=\"sr-only\"> (opens in new tab)</span></a> as the starting point for its wire protocol. Thus, one can use Garnet with unmodified Redis clients&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://redis.io/resources/clients/\" target=\"_blank\" rel=\"noreferrer noopener\">available<span class=\"sr-only\"> (opens in new tab)</span></a>&nbsp;in most programming languages, for example, with&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/StackExchange/StackExchange.Redis\" target=\"_blank\" rel=\"noreferrer noopener\">StackExchange.Redis<span class=\"sr-only\"> (opens in new tab)</span></a>&nbsp;in C#. Compared to other open-source cache-stores, you get much better performance, latency, extensibility, and durability features.</p>\n\n\n\n<p>Version of Garnet have been deployed in several real-world use cases at Microsoft, such as those in the Windows & Web Experiences Platform, Azure Resource Manager, and Azure Resource Graph. Garnet is also now open-source at <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/garnet\">https://github.com/microsoft/garnet<span class=\"sr-only\"> (opens in new tab)</span></a>.</p>\n\n\n\n<div style=\"padding-bottom:32px; padding-top:32px\" class=\"wp-block-msr-immersive-section alignfull row has-background has-light-purple-20-background-color has-text-color has-black-color wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-81 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h3 class=\"wp-block-heading\" id=\"high-performance\">High Performance</h3>\n\n\n\n<p>Garnet uses a thread-scalable storage layer called Tsavorite, which provides cache-friendly shared-memory scalability with tiered storage support. Garnet supports cluster mode (sharding and replication). It has a fast pluggable network design to get high end-to-end performance (throughput and 99th percentile latency).</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h3 class=\"wp-block-heading\" id=\"rich-extensible\">Rich & Extensible</h3>\n\n\n\n<p>Garnet uses the popular RESP wire protocol, allowing it to be used with unmodified Redis clients in any language. Garnet supports a large fraction of the Redis API surface, including raw strings and complex data structures such as sorted sets, bitmaps, and HyperLogLog. Garnet also has scalable extensibility and transactional stored procedure capabilities.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h3 class=\"wp-block-heading\" id=\"modern-secure\">Modern & Secure</h3>\n\n\n\n<p>The Garnet server is written in modern .NET C#, and runs efficiently on almost any platform. It works equally well on Windows and Linux, and is designed to not incur garbage collection overheads. You can also extend Garnet&#8217;s capabilities using new .NET data structures to go beyond the core API. Finally, Garnet has efficient TLS support out of the box.</p>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:32px; padding-top:32px\" class=\"wp-block-msr-immersive-section alignfull row has-background has-lighter-gray-background-color has-text-color has-black-color wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h2 class=\"wp-block-heading\" id=\"key-advantages\">Key advantages</h2>\n\n\n\n<p>Garnet offers several unique benefits:</p>\n\n\n\n<ul>\n<li>Garnet adopts the popular RESP wire protocol as a starting point, which makes it possible to use Garnet from unmodified Redis clients available in most programming languages today.</li>\n\n\n\n<li>Garnet offers much better throughput and scalability with many client connections and small batches, relative to comparable open-source cache-stores, leading to cost savings for large apps and services.</li>\n\n\n\n<li>Garnet demonstrates extremely low client latencies (often less than 300 microseconds at the 99.9<sup>th</sup>&nbsp;percentile) using commodity cloud (Azure) VMs with accelerated TCP enabled, which is critical to real-world scenarios.</li>\n\n\n\n<li>Based on the latest .NET technology, Garnet is cross-platform, extensible, and modern. It is designed to be easy to develop for and evolve, without sacrificing performance in the common case. We leveraged the rich library ecosystem of .NET for API breadth, with open opportunities for optimization. Thanks to our careful use of .NET, Garnet achieves state-of-the-art performance on both Linux and Windows.</li>\n</ul>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"more-information\">More information</h2>\n\n\n\n<p>Learn more about Garnet from the following links:</p>\n\n\n\n<ul>\n<li>GitHub: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/garnet\">https://github.com/microsoft/garnet<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li>Website: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://microsoft.github.io/garnet\">https://microsoft.github.io/garnet<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n<p></p>\n\n\n\n<p><sub>Redis is a registered trademark of Redis Ltd. Any rights therein are reserved to Redis Ltd. Any use by Microsoft is for referential purposes only and does not indicate any sponsorship, endorsement or affiliation between Redis and Microsoft.</sub></p>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<p></p>\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>A high-performance, extensible, and low-latency remote cache-store from&nbsp;Microsoft Research Garnet is a research project from Microsoft Research. It is a remote cache-store designed to offer high performance, extensibility, and low latency. Garnet is thread-scalable within a single node. It also supports sharded cluster execution, with replication, checkpointing, failover, and transactions. It can operate over main [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 994095,
        "date": "2024-02-27T02:21:03",
        "slug": "ai-cognition-and-the-economy",
        "title": "AI, Cognition and the Economy",
        "link": "https://www.microsoft.com/en-us/research/project/ai-cognition-and-the-economy/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1600\" height=\"555\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/09/AICE-Banner-FINAL.png\" class=\"attachment-full size-full\" alt=\"AI, Cognition and the Economy\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/09/AICE-Banner-FINAL.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/AICE-Banner-FINAL-300x104.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/AICE-Banner-FINAL-1024x355.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/AICE-Banner-FINAL-768x266.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/AICE-Banner-FINAL-1536x533.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/AICE-Banner-FINAL-240x83.png 240w\" sizes=\"(max-width: 1600px) 100vw, 1600px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"ai-cognition-and-the-economy-aice\">AI, Cognition, and the Economy (AICE)</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<h2 class=\"wp-block-heading\" id=\"our-mission\">Our mission</h2>\n\n\n\n<p>AICE establishes a global research network dedicated to cultivating an interdisciplinary research community. This collective will delve into the profound impact of Generative AI (GAI) on human cognition, work dynamics, and economic growth.</p>\n\n\n\n<p>This pioneering initiative seeks to unravel the intricate relationship between GAI and human thinking, exploring its ramifications on work practices, organizational structures, and subsequent transformations in labor markets and the economy. By tracing this developmental arc, AICE endeavors to catalyze a fresh wave of research, providing a deeper understanding of the evolving landscape and offering a clear trajectory for navigating towards a better future.</p>\n\n\n\n<p>The research framework includes pilot studies, workshops, and extended collaborations. Through these endeavors, AICE aims not only to inform the development of new technologies and services but also to contribute to pioneering research in this burgeoning field.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"aice-accelerator-pilot-collaborations\">Our collaborators</h3>\n\n\n\n<p>These universities received unrestricted gifts from Microsoft to support their related research.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-82 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Aarhus-University-logo_300x200.jpg\" alt=\"Aarhus University logo\" class=\"wp-image-1001469\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Aarhus-University-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Aarhus-University-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Catholic-Univ-Argentina-placeholder-logo_300x200.jpg\" alt=\"Catholic University of Argentina logo placeholder\" class=\"wp-image-1001349\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Catholic-Univ-Argentina-placeholder-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Catholic-Univ-Argentina-placeholder-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Haas-UC-Berkeley-placeholder-logo_300x200.jpg\" alt=\"Haas School of Business, UC Berkeley logo placeholder\" class=\"wp-image-1001352\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Haas-UC-Berkeley-placeholder-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Haas-UC-Berkeley-placeholder-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/MIT-Sloan-placeholder-logo_300x200.jpg\" alt=\"MIT Sloan School of Management logo placeholder\" class=\"wp-image-1001355\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/MIT-Sloan-placeholder-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/MIT-Sloan-placeholder-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Northeastern-University_primaryA-logo_300x200.jpg\" alt=\"Northeastern University logo\" class=\"wp-image-998898\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Northeastern-University_primaryA-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Northeastern-University_primaryA-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-83 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Penn-State-logo_300x200.jpg\" alt=\"Penn State University logo\" class=\"wp-image-1001334\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Penn-State-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Penn-State-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UCL_logo_300x200.png\" alt=\"University College London logo\" class=\"wp-image-1000617\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UCL_logo_300x200.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UCL_logo_300x200-240x160.png 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UniversityofTexas-Austin-logo_300x200.jpg\" alt=\"University of Texas at Austin logo\" class=\"wp-image-1001499\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UniversityofTexas-Austin-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UniversityofTexas-Austin-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UniversityOfWaterloo_logo_horiz_300x200.png\" alt=\"University of Waterloo logo\" class=\"wp-image-1000359\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UniversityOfWaterloo_logo_horiz_300x200.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UniversityOfWaterloo_logo_horiz_300x200-240x160.png 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\"></div>\n</div>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<h2 class=\"wp-block-heading\" id=\"aice-accelerator-pilot-collaborations\">AICE Accelerator Pilot collaborations</h2>\n\n\n\n<p>AICE has funded a series of pilot collaborations to accelerate discovery of early insights into the quickly evolving influence Generative AI is having on how people think and work and what that might mean for jobs in the future.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"human-ai-interaction-and-user-experience\">Human-AI interaction and user experience</h3>\n\n\n\n\n\n<figure class=\"wp-block-image size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Northeastern-University_primaryA-logo_300x200.jpg\" alt=\"Northeastern University logo\" class=\"wp-image-998898\" style=\"width:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Northeastern-University_primaryA-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Northeastern-University_primaryA-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n\n\n\n<p><strong>Northeastern University</strong>: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://vedantswain.com/\" target=\"_blank\" rel=\"noreferrer noopener\">Vedant Swain</a> (PI)<br><strong>Microsoft</strong>: <a href=\"https://www.microsoft.com/en-us/research/people/javierh/\">Javier Hernandez</a>, <a href=\"https://www.microsoft.com/en-us/research/people/marycz/\">Mary Czerwinski</a><br><strong>Area(s) of impact</strong>: </p>\n\n\n\n<p>To make AI agents more empathetic towards worker\u2019s goals, the agent needs to (i) understand broader wellbeing goals beyond saving time, (ii) maintain latitudinal and longitudinal awareness of workers\u2019 context outside their task, and (iii) provide workers suggestions to meet those goals by preempting opportunities in their work context. In this project, we propose to prototype and study Pro-Pilot, an enhancement over the existing Copilot that introduces a new Human-AI interaction framework that builds empathy.                               <a href=\"https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/\">*Accelerate Foundation Models Research </a>collaboration                                                                                                                </p>\n\n\n\n\n\n<figure class=\"wp-block-image size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UniversityofTexas-Austin-logo_300x200.jpg\" alt=\"University of Texas at Austin logo\" class=\"wp-image-1001499\" style=\"width:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UniversityofTexas-Austin-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UniversityofTexas-Austin-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n\n\n\n<p><strong>University of Texas at Austin</strong>: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://cascoglab.psy.utexas.edu/desmond/\" target=\"_blank\" rel=\"noreferrer noopener\">Desmond Ong</a> (PI)<br><strong>Microsoft</strong>: <a href=\"https://www.microsoft.com/en-us/research/people/jinsuh/\">Jina Suh</a>, <a href=\"https://www.microsoft.com/en-us/research/people/marycz/\">Mary Czerwinski</a>, <a href=\"https://www.microsoft.com/en-us/research/people/javierh/\">Javier Hernandez</a><br><strong>Area(s) of impact</strong>: </p>\n\n\n\n<p>The Digital Empathy pilot aims to investigate emotional intelligence in Large Foundation Model (LFM) -driven systems and to develop and study a series of empathic AI agents to understand and augment human performance and wellbeing. Until now, there has been very little empirical evidence of how empathic LFM systems are or the psychological implications of these systems during human-AI interactions. The project will contribute to a comprehensive survey of the research opportunities and priorities concerning empathy in AI systems and a research platform for the systematic evaluation of empathic agents.</p>\n\n\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-84 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:15%\">\n<figure class=\"wp-block-image size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UCL_logo_300x200.png\" alt=\"University College London logo\" class=\"wp-image-1000617\" style=\"width:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UCL_logo_300x200.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UCL_logo_300x200-240x160.png 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:85%\">\n<figure class=\"wp-block-image size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UniversityOfWaterloo_logo_horiz_300x200.png\" alt=\"University of Waterloo logo\" class=\"wp-image-1000359\" style=\"width:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UniversityOfWaterloo_logo_horiz_300x200.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/UniversityOfWaterloo_logo_horiz_300x200-240x160.png 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n</div>\n</div>\n\n\n\n<p><strong>University College London</strong>, <strong>University of Waterloo</strong>: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://uwaterloo.ca/psychology/profiles/clara-colombatto\" target=\"_blank\" rel=\"noreferrer noopener\">Clara Colombatto</a> (PI)<br><strong>Microsoft</strong>: <a href=\"https://www.microsoft.com/en-us/research/people/t-levt/\">Lev Tankelevitch</a>, <a href=\"https://www.microsoft.com/en-us/research/people/serintel/\">Sean Rintel</a><br><strong>Area(s) of impact</strong>: </p>\n\n\n\n<p>The aim of this pilot is to investigate the metacognitive demands and opportunities involved in working with&nbsp;Generative AI&nbsp;(GenAI). Early usability studies of GenAI systems suggest that they require high metacognitive monitoring and control: awareness, understanding, and control of one&#8217;s own thought processes. For example, iterative prompting, output evaluation, and adjustment depend on users\u2019 calibrated self-confidence in their ability to complete these tasks. These demands are likely exacerbated by GenAI models\u2019 unique features, such as their flexibility and non-determinism. At the same time, GenAI offers opportunities to adaptively support users in their workflows via metacognitive scaffolding (e.g., in conversational interfaces).&nbsp;In this project, we propose to leverage insights from the psychology and neuroscience of metacognition and&nbsp;decision-making&nbsp;to study human-AI interactions and their potential for trustworthy collaboration. This past work has highlighted that successful collaboration&nbsp;hinges on sharing not just our cognitive states (e.g. what we believe), but also metacognitive estimates (e.g. our confidence in ourselves and one another). Humans routinely signal their metacognitive states explicitly (e.g., via verbal estimates) or implicitly (e.g., via speech prosody). &nbsp;Recent studies have found that humans tend to misattribute metacognitive states to AI (compared to other humans), highlighting the importance of developing robust and accurate methods for signaling metacognitive states in human-GenAI interactions. If signatures of metacognition are helpful in human-human interactions, we will explore whether they might they also facilitate collaboration and trust in human-GenAI interactions.</p>\n\n\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity is-style-dots\"/>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"ai-in-jobs-teamwork-and-collaboration\">AI in jobs, teamwork and collaboration</h3>\n\n\n\n\n\n<figure class=\"wp-block-image size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/MIT-Sloan-placeholder-logo_300x200.jpg\" alt=\"MIT Sloan School of Management placeholder logo\" class=\"wp-image-1001355\" style=\"width:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/MIT-Sloan-placeholder-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/MIT-Sloan-placeholder-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n\n\n\n<p><strong>MIT Sloan School of Management</strong>: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://danielle-li.github.io/\" target=\"_blank\" rel=\"noreferrer noopener\">Danielle Li</a> (PI)<br><strong>Microsoft</strong>: <a href=\"https://www.microsoft.com/en-us/research/people/sojaffe/\">Sonia Jaffe</a>, <a href=\"https://www.microsoft.com/en-us/research/people/eldillon/\">Eleanor Dillon</a><br><strong>Area(s) of impact</strong>: Labor markets; new capabilities and productivity gains</p>\n\n\n\n<p>Will generative AI reduce the barrier to entry into software development by making it easier for less skilled people to take on routine programming tasks, thereby expanding programming opportunities? Or will higher-skilled developers shortcut routine tasks with AI, thereby reducing the demand for programmers? How does AI affect team collaboration, for example, can people with complementary skills, such as managers and designers, do more programming work with the AI tools?</p>\n\n\n\n\n\n<figure class=\"wp-block-image size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Haas-UC-Berkeley-placeholder-logo_300x200.jpg\" alt=\"Haas School of Business, UC Berkeley logo placeholder\" class=\"wp-image-1001352\" style=\"width:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Haas-UC-Berkeley-placeholder-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Haas-UC-Berkeley-placeholder-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n\n\n\n<p><strong>Haas School of Business, UC Berkeley</strong>: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.daveholtz.net/\" target=\"_blank\" rel=\"noreferrer noopener\">David Holtz</a> (PI)<br><strong>Microsoft</strong>: <a href=\"https://www.microsoft.com/en-us/research/people/suri/\">Siddharth Suri</a><br><strong>Area(s) of impact</strong>: New industries and innovation; new capabilities and productivity gains</p>\n\n\n\n<p>As LLMs rise in popularity, prompting is going to take on a more important role in daily life for everyone. Studying how people learn to prompt engineer better is going to be a key research question for everyone using these models going forward. We will analyze how people learn to write prompts using generative AI models to complete various tasks. We want to understand the relationship between the importance of prompt engineering and the complexity of the task and the generative AI model used. We also want to understand whether insights and advice about prompt engineering can be effectively transferred between users to improve prompt engineering effectiveness.</p>\n\n\n\n\n\n<figure class=\"wp-block-image size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Aarhus-University-logo_300x200.jpg\" alt=\"Aarhus University logo\" class=\"wp-image-1001469\" style=\"width:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Aarhus-University-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Aarhus-University-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n\n\n\n<p><strong>Aarhus University</strong>: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.linkedin.com/in/jensemildk/\" target=\"_blank\" rel=\"noreferrer noopener\">Jens Emil Gr\u00f8nb\u00e6k</a> (PI), <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.linkedin.com/in/clemens-nylandsted-klokmose-6b204b2/\" target=\"_blank\" rel=\"noreferrer noopener\">Clemens Klokmose</a> (PI)<br><strong>Microsoft</strong>: <a href=\"https://www.microsoft.com/en-us/research/people/serintel/\">Sean Rintel</a>, <a href=\"https://www.microsoft.com/en-us/research/people/t-payodpanda/\">Payod Panda</a>, <a href=\"https://www.microsoft.com/en-us/research/people/t-levt/\">Lev Tankelevitch</a><br><strong>Area(s) of impact</strong>: </p>\n\n\n\n<p>The overall goal of this collaborative project is to understand how LLMs might increase productivity and dynamically generate tailor meeting interfaces to match goals customized to team needs. We aim to explore how the combination of LLMs and a rule-based declarative programming model support end-user adaptability of meeting software. From this we hope to establish the core principles for designing goal-adaptive meeting interfaces. Once that is established, we aim to study the cognitive and productivity impact/s of goal-adaptive meeting interfaces.</p>\n\n\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity is-style-dots\"/>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"societal-and-economic-impacts-of-ai\">Societal and economic impacts of AI</h3>\n\n\n\n\n\n<figure class=\"wp-block-image size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Catholic-Univ-Argentina-placeholder-logo_300x200.jpg\" alt=\"Catholic University of Argentina logo placeholder\" class=\"wp-image-1001349\" style=\"width:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Catholic-Univ-Argentina-placeholder-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Catholic-Univ-Argentina-placeholder-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n\n\n\n<p><strong>Catholic University of Argentina</strong>: Alicia Caballero (PI), Ma. Lourdes Puente Olivera (PI)<br><strong>Microsoft</strong>: <a href=\"https://www.microsoft.com/en-us/research/people/counts/\">Scott Counts</a><br><strong>Area(s) of impact</strong>: Labor markets; Equity</p>\n\n\n\n<p>The Observatorio de la Deuda Social Argentina at Catholic University of Argentina collects socioeconomic data and would like to extend its field work and other data collection infrastructure to collect AI related data, i.e., questions about AI usage and expectation for impact of AI. The debt survey is the largest country-wide survey in Argentina that is not administered by the government; the survey is akin to the American Community Survey census in scope. The gift will support data processing and analysis of the survey data.</p>\n\n\n\n\n\n<figure class=\"wp-block-image size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Penn-State-logo_300x200.jpg\" alt=\"Penn State University logo\" class=\"wp-image-1001334\" style=\"width:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Penn-State-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Penn-State-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n\n\n\n<p><strong>Penn State University</strong>: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://sites.google.com/site/nageeb/\" target=\"_blank\" rel=\"noreferrer noopener\">Nageeb Ali</a> (PI)<br><strong>Microsoft</strong>: <a href=\"https://www.microsoft.com/en-us/research/people/nicimm/\">Nicole Immorlica</a>, <a href=\"https://www.microsoft.com/en-us/research/people/brlucier/\">Brendan Lucier</a><br><strong>Area(s) of impact</strong>: </p>\n\n\n\n<p>Generative AI relies on large amounts of training data. This has always been a key input for AI, and traditionally is sourced by scraping the web for data byproducts and/or hiring data workers. &nbsp;Generative AI introduces a new twist though, namely the producers of the data now must compete with the downstream uses of their data. &nbsp;For example, a graphic designer might find that the AI trained on her own work is now replacing her. &nbsp;This creates a \u201chold-up problem\u201d: if actions are easily replicable and become one\u2019s future competition, then there is not going to be much incentive to put in effort to create high-quality content. Hence, for there to be high-quality generative AI, there needs to be a commitment to offering future returns to investment. &nbsp;Otherwise, it\u2019ll be low quality. &nbsp;The commitment problem is exacerbated by competition. Suppose that the original data / task could have been done by Ann, Bob, or Carol. If they could all commit together \u2014 as in a union \u2014 then there is hope. But if parties cannot commit, then this would make it even harder to create incentives for effort as now the firm needs any one of them to give their data, and they will compete. &nbsp;We&#8217;re interested in studying this data supply chain, the implicit norms around data use, when they ought to be formalized, mechanisms that can resolve hold-up problems, and the social consequences of various designs.</p>\n\n\n\n\n\n<figure class=\"wp-block-image size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"300\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/MIT-Sloan-placeholder-logo_300x200.jpg\" alt=\"MIT Sloan School of Management placeholder logo\" class=\"wp-image-1001355\" style=\"width:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/MIT-Sloan-placeholder-logo_300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/MIT-Sloan-placeholder-logo_300x200-240x160.jpg 240w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></figure>\n\n\n\n<p><strong>MIT Sloan School of Management</strong>: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://mitsloan.mit.edu/faculty/directory/mert-demirer\" target=\"_blank\" rel=\"noreferrer noopener\">Mert Demirer</a> (PI), <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://mitsloan.mit.edu/faculty/directory/john-j-horton\" target=\"_blank\" rel=\"noreferrer noopener\">John Horton</a> (PI)<br><strong>Microsoft</strong>: <a href=\"https://www.microsoft.com/en-us/research/people/brlucier/\">Brendan Lucier</a>, <a href=\"https://www.microsoft.com/en-us/research/people/nicimm/\">Nicole Immorlica</a><br><strong>Area(s) of impact</strong>: Labor markets; New industries and innovation</p>\n\n\n\n<p>As the adoption of generative AI tools becomes more widespread, it is crucial to anticipate the macroeconomic effects on labor and production. &nbsp;This requires both a whole-market view and a detailed accounting of the differences between jobs. &nbsp;We will approach this challenge by treating jobs as interconnected sequences of tasks that vary in how easily they can be automated and overseen. &nbsp;This results in some jobs being &#8220;more automatable&#8221; than others\u2014even accounting for the level of skill required to complete the job manually\u2014and suggests jobs where human-AI collaboration might be especially useful. &nbsp;We will use these models to study the general equilibrium impact of advances in AI automation across different job domains.</p>\n\n\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity is-style-end-mark\"/>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>AICE establishes a global research network dedicated to cultivating an interdisciplinary research community. This collective will delve into the profound impact of Generative AI (GAI) on human cognition, work dynamics, and economic growth. This pioneering initiative seeks to unravel the intricate relationship between GAI and human thinking, exploring its ramifications on work practices, organizational structures, [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1004841,
        "date": "2024-02-06T09:53:20",
        "slug": "elate",
        "title": "ELaTE",
        "link": "https://www.microsoft.com/en-us/research/project/elate/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"4400\" height=\"1857\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02/web-page-back-1.png\" class=\"attachment-full size-full\" alt=\"title back\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02/web-page-back-1.png 4400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/web-page-back-1-300x127.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/web-page-back-1-1024x432.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/web-page-back-1-768x324.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/web-page-back-1-1536x648.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/web-page-back-1-2048x864.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/web-page-back-1-665x280.png 665w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/web-page-back-1-240x101.png 240w\" sizes=\"(max-width: 4400px) 100vw, 4400px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"elate\">ELaTE  </h1>\n\n\n\n<p>Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n\n\n<p><strong>ELaTE</strong> is a zero-shot text-to-speech (TTS) system that can generate <strong>natural laughing speech from any speaker</strong> based on a speaker prompt to mimic the voice characteristic, a text prompt to indicate the contents of the generated speech, and an input to control the laughter expression.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://arxiv.org/abs/2402.07383\" target=\"_blank\" rel=\"noreferrer noopener\">Read the paper</a></div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-86 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>ELaTE has the following key features:</p>\n\n\n\n<ul>\n<li><strong>Precise control of laughter timing: </strong>A user can specify the timing for laughter, which critically affects the nuance of the generated speech. </li>\n\n\n\n<li><strong>Precise control of laughter expression: </strong>A user can guide the laughter expression using an example audio containing laughter. </li>\n\n\n\n<li><strong>Build upon a well-trained zero-shot TTS: </strong>ELaTE can generate natural speech without compromising audio quality and with a negligible increase in computational cost compared to the conventional zero-shot TTS model.</li>\n</ul>\n\n\n\n<p class=\"has-text-align-center\">Generated laughing speech samples by ELaTE</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-85 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_1_B.wav\"></audio>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_last_3_A.wav\"></audio>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_2_B.wav\"></audio></audio>\n</div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"526\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02/main-for-web-page-v2-65c5946676871-1024x526.png\" alt=\"Overview of ELaTE\" class=\"wp-image-1006047\" style=\"width:554px;height:auto\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02/main-for-web-page-v2-65c5946676871-1024x526.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/main-for-web-page-v2-65c5946676871-300x154.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/main-for-web-page-v2-65c5946676871-768x394.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/main-for-web-page-v2-65c5946676871-1536x788.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/main-for-web-page-v2-65c5946676871-2048x1051.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/main-for-web-page-v2-65c5946676871-240x123.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n</div>\n</div>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center is-style-l\" id=\"speech-to-speech-translation\">  Speech-to-speech translation</h2>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-88 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"200\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02/main-for-web-page-v3-1024x200.png\" alt=\"Application for Speech-to-Speech Translation\" class=\"wp-image-1006530\" style=\"width:565px;height:auto\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02/main-for-web-page-v3-1024x200.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/main-for-web-page-v3-300x59.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/main-for-web-page-v3-768x150.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/main-for-web-page-v3-240x47.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/main-for-web-page-v3.png 1404w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>ELaTE can be applied to speech-to-speech translation, <strong>transferring not only the voice characteristic but also the precise nuance of the source audio</strong> with&nbsp;unprecedented quality.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-87 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p class=\"has-text-align-left\">Original speech (Chinese)</p>\n\n\n\n<audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_source_1_C.wav\"></audio>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p class=\"has-text-align-left\">Generated speech (English)</p>\n\n\n\n<audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_1_C.wav\"></audio>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center is-style-l\" id=\"model-overview\">Model overview</h2>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-89 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>We develop ELaTE based on the foundation of conditional flow-matching-based zero-shot TTS, and fine-tune it with frame-level representation from a laughter detector as additional conditioning. With a simple scheme to mix small-scale laughter-conditioned data with large-scale pre-training data, we demonstrate that a pre-trained zero-shot TTS model can be readily fine-tuned to generate natural laughter with precise controllability, without losing any quality of the pre-trained zero-shot TTS model.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"476\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Overview-v9-1024x476.png\" alt=\"Model Overview\" class=\"wp-image-1007478\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Overview-v9-1024x476.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Overview-v9-300x140.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Overview-v9-768x357.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Overview-v9-1536x714.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Overview-v9-240x112.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Overview-v9.png 1920w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n</div>\n</div>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center is-style-l\" id=\"audio-samples\">Audio samples</h2>\n\n\n\n<p>Below, we included audio samples demonstrating how ELaTE performs with various laughter instructions. The speech samples were taken from LibriSpeech test-clean and DiariST-AliMeeting dataset. The speech samples below are provided for the sole purpose of illustrating ELaTE.</p>\n\n\n\n<div style=\"width: 100%;margin: 0 auto\">\n    <!-- Instruction by time -->\n    <div style=\"margin-bottom: 50px\">\n        <h3 style=\"text-align: left\">Instruction by time</h3>\n        <p style=\"text-align: left\">ELaTE synthesizes speech in the voice characteristic specified by a speaker prompt, adding laughter at the specified timing. </p>\n        <div style=\"border-bottom: 2px solid black;margin-bottom: 2px\"></div>\n        <div style=\"background-color: #E6E6FA;padding: 20px;border-radius: 5px;max-width: 80%;margin: 20px auto\">\n            <table style=\"width: 100%;border-collapse: collapse;border: none\">\n                <thead>\n                    <tr style=\"border-bottom: 2px solid black\">\n                        <th style=\"text-align: center;padding: 8px;width: 25%\">Text prompt</th>\n                        <th style=\"text-align: center;padding: 8px;width: 25%\">Speaker prompt</th>\n                        <th style=\"text-align: center;padding: 8px;width: 25%\">Laughter prompt</th>\n                        <th style=\"text-align: center;padding: 8px;width: 25%\">Generated speech</th>\n                    </tr>\n                </thead>\n                <tbody>\n            <tr>\n                <td style=\"text-align: left;padding: 8px;border-bottom: 1px solid #ccc\" rowspan=\"3\">That&#8217;s funny!</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\" rowspan=\"3\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_prompt_1_A.wav\"></audio>\n                </td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">0.0&#8211;1.4 sec (first half of speech)</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_first_1_A.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">1.4&#8211;2.8 sec (last half of speech)</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_last_thats-funny-last-1.4-2.8.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">No laughter</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_last_1_A.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: left;padding: 8px;border-bottom: 1px solid #ccc\" rowspan=\"3\">I didn&#8217;t see that one coming!</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\" rowspan=\"3\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_prompt_2_A.wav\"></audio>\n                </td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">0.0&#8211;2.0 sec (first half of speech)</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_first_2_A.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">2.0&#8211;4.0 sec (last half of speech)</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_last_2_A.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">No laughter</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_nolaugh_2_A.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: left;padding: 8px;border-bottom: 1px solid #ccc\" rowspan=\"3\">I&#8217;m not sure whether to laugh or cry!</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\" rowspan=\"3\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_prompt_3_A.wav\"></audio>\n                </td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">0.0&#8211;2.2 sec (first half of speech)</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_first_3_A.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">2.2&#8211;4.4 sec (last half of speech)</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_last_3_A.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">No laughter</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_nolaugh_3_A.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: left;padding: 8px;border-bottom: 1px solid #ccc\" rowspan=\"3\">You&#8217;ve got to be kidding me!</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\" rowspan=\"3\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_prompt_4_A.wav\"></audio>\n                </td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">0.0&#8211;1.8 sec (first half of speech)</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_first_4_A.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">1.8&#8211;3.6 sec (last half of speech)</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_last_4_A.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">No laughter</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_nolaugh_4_A.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: left;padding: 8px;border-bottom: 1px solid #ccc\" rowspan=\"3\">Who let the dogs out?</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\" rowspan=\"3\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_prompt_5_A.wav\"></audio>\n                </td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">0.0&#8211;1.6 sec (first half of speech)</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_first_5_A.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">1.6&#8211;3.2 sec (last half of speech)</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_last_5_A.wav\"></audio>\n                </td>\n            </tr>\n            <tr>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">No laughter</td>\n                <td style=\"text-align: center;padding: 8px;border-bottom: 1px solid #ccc\">\n                    <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_nolaugh_5_A.wav\"></audio>\n                </td>\n            </tr>\n                </tbody>\n            </table>\n        </div>\n    </div>\n    </div>\n    <!-- Instruction by example -->\n    <div style=\"margin-bottom: 50px\">\n    <h3 style=\"text-align: left\">Instruction by example</h3>\n    <p style=\"text-align: left\">ELaTE synthesizes speech in the voice characteristic specified by a speaker prompt and incorporates the laughter style specified by a laughter prompt. </p>\n        <div style=\"border-bottom: 2px solid black;margin-bottom: 2px\"></div>\n        <div style=\"background-color: #E6E6FA;padding: 20px;border-radius: 5px;max-width: 80%;margin: 20px auto\">\n            <table style=\"width: 100%;border-collapse: collapse;border: none\">\n                <thead>\n                    <tr style=\"border-bottom: 2px solid black\">\n                        <th style=\"text-align: center;padding: 8px;width: 25%\">Text prompt</th>\n                        <th style=\"text-align: center;padding: 8px;width: 25%\">Speaker prompt</th>\n                        <th style=\"text-align: center;padding: 8px;width: 25%\">Laughter prompt</th>\n                        <th style=\"text-align: center;padding: 8px;width: 25%\">Generated speech</th>\n                    </tr>\n                </thead>\n                <tbody>\n                <tr>\n                    <td style=\"text-align: left;padding: 8px\">That&#8217;s funny!</td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_speakerprompt_1_B.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laughterprompt_1_B.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_1_B.wav\"></audio>\n                    </td>\n                </tr>\n                <tr>\n                    <td style=\"text-align: left;padding: 8px\">That&#8217;s what she said!</td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_speakerprompt_2_B.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laughterprompt_2_B.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_2_B.wav\"></audio>\n                    </td>\n                </tr>\n                <tr>\n                    <td style=\"text-align: left;padding: 8px\">I&#8217;ve heard of air guitar, but this is ridiculous! </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_speakerprompt_3_B.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laughterprompt_3_B.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_3_B.wav\"></audio>\n                    </td>\n                </tr>\n                <tr>\n                    <td style=\"text-align: left;padding: 8px\">Well, that&#8217;s a plot twist!</td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_speakerprompt_4_B.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laughterprompt_4_B.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_4_B.wav\"></audio>\n                    </td>\n                </tr>\n                <tr>\n                    <td style=\"text-align: left;padding: 8px\">I guess that&#8217;s one way to do it!</td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_speakerprompt_5_B.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laughterprompt_5_B.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_5_B.wav\"></audio>\n                    </td>\n                </tr>\n                </tbody>\n            </table>\n        </div>\n    </div>\n    <!-- Application for speech-to-speech translation -->\n    <div style=\"margin-bottom: 50px\">\n       <h3 style=\"text-align: left\">Application for speech-to-speech translation</h3>\n    <p style=\"text-align: left\">ELaTE can be applied to speech-to-speech translation, transferring not only the voice characteristic but also the precise nuance of the source audio.</p>\n        <div style=\"border-bottom: 2px solid black;margin-bottom: 2px\"></div>\n        <div style=\"background-color: #E6E6FA;padding: 20px;border-radius: 5px;max-width: 80%;margin: 20px auto\">\n            <table style=\"width: 100%;border-collapse: collapse;border: none\">\n                <thead>\n                <tr style=\"border-bottom: 2px solid black\">\n                    <th style=\"text-align: center;padding: 8px\" rowspan=\"2\">Source audio (Chinese)</th>\n                    <th style=\"text-align: center;padding: 8px\" colspan=\"3\">Translated audio (English)</th>\n                </tr>\n                <tr style=\"border-bottom: 2px solid black\">\n                    <th style=\"text-align: center;padding: 8px\">Seamless Expressive</th>\n                    <th style=\"text-align: center;padding: 8px\">Our baseline TTS</th>\n                    <th style=\"text-align: center;padding: 8px\">ELaTE</th>\n                </tr>\n                </thead>\n                <tbody>\n                <tr>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_source_1_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_seamless_1_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_baseline_1_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_1_C.wav\"></audio>\n                    </td>\n                </tr>\n                <tr>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_source_4_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_seamless_4_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_baseline_4_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_4_C.wav\"></audio>\n                    </td>\n                </tr>\n                <tr>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_source_3_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_seamless_3_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_baseline_3_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_3_C.wav\"></audio>\n                    </td>\n                </tr>\n                <tr>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_source_5_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_seamless_5_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_baseline_5_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_5_C.wav\"></audio>\n                    </td>\n                </tr>\n                <tr>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_source_6_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_seamless_6_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_baseline_6_C.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02//elate_laugh_6_C.wav\"></audio>\n                    </td>\n                </tr>\n                </tbody>\n            </table>\n        </div>\n    </div>\n    <!-- End of Application for Speech-to-Speech Translation -->\n\n</div>\n\n\n\n<p><em><sup>(*) The list of DiariST-AliMeeting laughter utterances we used for our evaluation, along with their transcription and translation, can be downloaded from</sup></em> <sup><a href=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02/DiariST-AliMeeting-Laughter-Test-Set.txt\" target=\"_blank\" rel=\"noreferrer noopener\">https://www.microsoft.com/en-us/research/uploads/prod/2024/02/DiariST-AliMeeting-Laughter-Test-Set.txt</a></sup> <em><sup>under CC BY-SA 4.0 <span style=\"font-size: 13.0591px\">lic</span>ense.</sup></em><br><em><sup>(**) We used Seamless Expressive for a pure research purpose. Seamless Expressive was used based on the Seamless Licensing Agreement. Copyright \u00a9 Meta Platforms, Inc. All Rights Reserved.</sup></em></p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center is-style-l\" id=\"ethics-statement-1\">Ethics statement</h2>\n\n\n\n<p>ELaTE could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While ELaTE can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model.</p>\n\n\n</div>\n\n\n\n<p></p>\n\n\n\n<p></p>\n\n\n\n<p></p>\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like ELaTE is a zero-shot text-to-speech (TTS) system that can generate natural laughing speech from any speaker based on a speaker prompt to mimic the voice characteristic, a text prompt to indicate the contents of the generated speech, and an input to control the laughter expression. ELaTE has [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 1003089,
        "date": "2024-01-30T06:27:35",
        "slug": "ai-frontiers-explorations",
        "title": "AI Frontiers: Explorations",
        "link": "https://www.microsoft.com/en-us/research/project/ai-frontiers-explorations/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1921\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AI_Microsoft_Research_Header_1920x720.png\" class=\"attachment-full size-full\" alt=\"AI and Microsoft Research header - abstract neural network pattern on dark spectrum background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AI_Microsoft_Research_Header_1920x720.png 1921w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AI_Microsoft_Research_Header_1920x720-300x112.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AI_Microsoft_Research_Header_1920x720-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AI_Microsoft_Research_Header_1920x720-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AI_Microsoft_Research_Header_1920x720-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AI_Microsoft_Research_Header_1920x720-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AI_Microsoft_Research_Header_1920x720-240x90.png 240w\" sizes=\"(max-width: 1921px) 100vw, 1921px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/?p=992148&post_type=msr-research-lab&preview=1&_ppp=253bdc9b27\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"AI Frontiers\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAI Frontiers\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"explorations\">Explorations</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>We are actively exploring a range of projects that we believe hold significant potential not only for Microsoft but also for the broader society.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"large-action-behavioral-models\">Large Action & Behavioral Models</h2>\n\n\n\n<p class=\"h5\">Games, Robotics, Self-driving OS</p>\n\n\n\n<p>Large Action & Behavioral Models (LAM) are AI foundational models that can predict how to act on human environments and interfaces, like LLM are capable to interpret human language and generate coherent text. We develop LAMs that will be capable to perform complex tasks in a multitude of scenarios, like how to play a videogame, how a to move in the real world, and how to use an Operating System (OS).</p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\"/>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"transformer-enhancements\">Transformer Enhancements</h2>\n\n\n\n<p class=\"h5\">EMA (Exponential moving average), <a href=\"https://www.microsoft.com/en-us/research/publication/the-truth-is-in-there-improving-reasoning-in-language-models-with-layer-selective-rank-reduction/\">LASER</a>, Learning from Text feedback, feedback driven learning in a RAG</p>\n\n\n\n<p>We develop techniques that can dramatically improve the performance or reduce the costs of Transformer-based Large Language Models (LLMs) by applying simple interventions after training requiring minimal or zero parameters or data.</p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\"/>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"foundational-models-for-latent-state-and-latent-dynamics-in-llm\">Foundational Models for latent state and latent dynamics in LLM</h2>\n\n\n\n<p>A latent state or a latent dynamic can improve planning, exploration, and credit assignment of AI agents by keeping task-relevant information and discarding distractions. We develop techniques that autonomously discover such latent representations directly from real-world signals.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>We are actively exploring a range of projects that we believe hold significant potential not only for Microsoft but also for the broader society. Games, Robotics, Self-driving OS Large Action & Behavioral Models (LAM) are AI foundational models that can predict how to act on human environments and interfaces, like LLM are capable to interpret [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 994551,
        "date": "2024-01-12T08:32:02",
        "slug": "project-ex-vivo",
        "title": "Project Ex Vivo",
        "link": "https://www.microsoft.com/en-us/research/project/project-ex-vivo/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Project-ExVivo-pipette_header_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"Project Ex Vivo header - aerial view of lab tech working with a pipette\" style=\"object-position: 72% 41%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Project-ExVivo-pipette_header_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Project-ExVivo-pipette_header_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Project-ExVivo-pipette_header_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Project-ExVivo-pipette_header_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Project-ExVivo-pipette_header_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Project-ExVivo-pipette_header_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Project-ExVivo-pipette_header_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"project-ex-vivo\">Project <em>Ex Vivo</em></h1>\n\n\n\n<p>Modeling cancer outside the body specific to the patient</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Project <em>Ex Vivo</em> is a joint cancer research collaboration between Microsoft and the Broad Institute of MIT and Harvard. The project spans multiple research areas, including machine learning, statistics, bioengineering, automation, molecular biology, and clinical application.</p>\n\n\n\n<p>Cancer is understood primarily as a disease of DNA alterations and often treated using a reductionist approach, with each cancer distilled to a single metric such as histologic type or harboring a specific molecular abnormality. As a result, the current paradigm for precision oncology consists primarily of using genetic alterations to direct therapy (e.g., targeting tumor mutations). Strikingly, few patients receive prolonged benefit from this approach.</p>\n\n\n\n<p>In Project <em>Ex Vivo</em>, we see cancers as complex (eco)systems, beyond just mutational variation, that necessitate systems-level understanding and intervention. Our ultimate objective is to more effectively model cancer ex vivo \u2013 outside the body \u2013 in a patient-specific manner. Doing so will unlock the ability to more effectively stratify patients and identify therapies that target diverse aspects of human cancers.</p>\n\n\n\n<p>We use machine learning to develop and understand accurate representations of each tumor by integrating genetic markers, expression state, and tumor microenvironmental interactions. These representations help us precisely define and quantify the state and trajectory of each tumor in each patient. One of the benefits of this approach is that it gives us a clear measure of divergence between biological models (e.g., cell cultures, organoids) and the original tumor. We can directly target this divergence and minimize it using reinforcement learning and lab automation techniques to produce cellular models that are high-fidelity reproductions of the original tumor.</p>\n\n\n\n<p><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://pubmed.ncbi.nlm.nih.gov/34890551/\" target=\"_blank\" rel=\"noreferrer noopener\">Our initial work in pancreatic ductal adenocarcinoma<span class=\"sr-only\"> (opens in new tab)</span></a> (PDAC) uncovered limitations in current cancer modeling pipelines and revealed fundamental insights into the therapeutic significance of cell state and its microenvironmental drivers.</p>\n\n\n\n<p>Project <em>Ex Vivo</em> will codify new pipelines and workflows to establish non-genetic attributes (e.g., cell state) as targetable features in cancer, with an initial focus on pancreatic cancer and eventually extending to additional malignancies. We use machine learning to develop and understand accurate representations of each tumor and cellular model by integrating genetic markers, expression state, and tumor microenvironmental interactions. We will optimize model generation workflows for greater in vivo fidelity, nominate new candidates for therapeutic development, and establish a new paradigm integrating genetic and non-genetic attributes to guide therapy selection in precision oncology. We are developing rigorous experimental, computational, and automation pipelines that can be applied across a variety of cancer, non-cancer, and clinical contexts. By credentialing cell state and its relation to the local environment as a biomarker for both prognostication and therapy selection, the success of this project may improve patient stratification for diverse clinical trials (chemotherapy, immunotherapy, small molecules, etc.), thereby enhancing the likelihood of therapeutic efficacy, potentially lowering costs, and ultimately improving patient outcomes.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Modeling cancer outside the body specific to the patient Project Ex Vivo is a joint cancer research collaboration between Microsoft and the Broad Institute of MIT and Harvard. The project spans multiple research areas, including machine learning, statistics, bioengineering, automation, molecular biology, and clinical application. Cancer is understood primarily as a disease of DNA alterations [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 995583,
        "date": "2024-01-05T08:09:50",
        "slug": "afmr-domain-applications",
        "title": "AFMR: Domain applications",
        "link": "https://www.microsoft.com/en-us/research/project/afmr-domain-applications/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Domain-Applications-page-header_1920x720-6594368f154a5.png\" class=\"attachment-full size-full\" alt=\"white icon of a circle surrounded by 8 smaller circles connected by straight lines on a green gradient background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Domain-Applications-page-header_1920x720-6594368f154a5.png 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Domain-Applications-page-header_1920x720-6594368f154a5-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Domain-Applications-page-header_1920x720-6594368f154a5-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Domain-Applications-page-header_1920x720-6594368f154a5-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Domain-Applications-page-header_1920x720-6594368f154a5-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Domain-Applications-page-header_1920x720-6594368f154a5-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Domain-Applications-page-header_1920x720-6594368f154a5-240x90.png 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"Accelerating Foundation Models Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAccelerating Foundation Models Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"domain-applications\">Domain applications</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\">\n<p><strong><em>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models.</em></strong></p>\n<cite>Brad Smith, Vice Chair and President</cite></blockquote>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-90 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:50%\">\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"400\" height=\"400\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Discovery-Natural-Sciences_1.3.png\" alt=\"dark green icon of a lightbulb with a plant growing inside and a ring around the lightbulb\" class=\"wp-image-996369\" style=\"width:auto;height:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Discovery-Natural-Sciences_1.3.png 400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Discovery-Natural-Sciences_1.3-300x300.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Discovery-Natural-Sciences_1.3-150x150.png 150w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Discovery-Natural-Sciences_1.3-180x180.png 180w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Discovery-Natural-Sciences_1.3-360x360.png 360w\" sizes=\"(max-width: 400px) 100vw, 400px\" /></figure>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center h4\" id=\"afmr-goal-accelerate-scientific-discovery-in-natural-sciences\">AFMR Goal: Accelerate scientific discovery in natural sciences</h2>\n\n\n\n<p class=\"has-text-align-center\">via proactive knowledge discovery, hypothesis generation, and multiscale multimodal data generation</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p>This set of research projects explore how foundation models can be applied in a variety for domain applications in science and engineering, spanning agriculture, battery design, catalyst discovery, climate science, energy systems, health, Internet of Things (IoT), material science, and robotics. The breadth of methodologies explored include contextual understanding and representation, semantic parsing, interaction skills acquisition, dynamic adaptation and efficient retrieval. These efforts demonstrate how advanced AI can enable scientific discoveries to be realized through a range of applications that swiftly integrate foundation models with complementary technologies to drive innovation across many sectors.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<p><strong>National University of Singapore</strong>: Jingxian Wang (PI)</p>\n\n\n\n<p>Attempts to bridge the gap in foundation models that establish links across multiple types of IoT sensors in varied environments without the constraints of elaborate sensor calibration.</p>\n\n\n\n\n\n<p><strong>\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne</strong>: Josie Hughes (PI) </p>\n\n\n\n<p>The project aims to explore the role of foundation models in aiding the design of soft or compliant robots that interact with humans. It proposes the development of a framework for contextual human-robot interactions. The project design involves two work packages, focusing on design generation, contextual predictions for design, and qualitative feedback responses. Project leverages Microsoft Azure tools and AI models including GPT-4 series, Codex and Dall-E 2.</p>\n\n\n\n\n\n<p><strong>The University of Nottingham</strong>: Valerio Giuffrida (PI)</p>\n\n\n\n<p>The proposal aims to build foundation models that can be effectively applied to diverse vision-based plant phenotyping and agricultural data and tasks. The focus is on developing pre-training methods like self-supervised learning to leverage labeled and unlabeled crop datasets for agricultural use. This project would involve Azure data storage, computational power, Azure APIs along with open-sourced foundation models and datasets.</p>\n\n\n\n\n\n<p><strong>The University of Hong Kong</strong>: Yanchao Yang (PI)</p>\n\n\n\n<p>This research aims to use foundation models to train embodied agents and enable them to acquire diverse physical interaction skills and adapt efficiently to dynamic situations. The training process is autonomous, requiring little to no human annotation effort. Utilizing language and vision-language models, the proposed methodology endows embodied agents with the capabilities to comprehend language instructions, plan tasks, and derive actions for accomplishing the interaction goal. Additionally, the proposal aims to improve foundation models through embodied learning.</p>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Adam Brandt (PI)</p>\n\n\n\n<p>This project aims to deploy AI and large language models (LLMs) to extract data from energy datasets, with a focus on the oil and gas sector. The initiative is expected to help in the creation of a comprehensive database that highlights the broader energy industry and significantly contribute to formulating climate policy across the energy spectrum.</p>\n\n\n\n\n\n<p><strong>University of Michigan, Ann Arbor</strong>: Joyce Chai (PI)</p>\n\n\n\n<p>Explore the use of large language models (e.g., GPT-4) as communication facilitators for embodied AI agents in human-agent dialogue and multi-agent communication tasks. Our team will focus on grounding language to the agent\u2019s perception and action, improving semantic representation of the 3D environment through dialogue, and enhancing task planning capabilities using GPT-4 in a simplified grid world.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://chat-with-nerf.github.io/\" target=\"_blank\" rel=\"noreferrer noopener\">LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.07968\" target=\"_blank\" rel=\"noreferrer noopener\">Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of New South Wales</strong>: Imran Razzak (PI)</p>\n\n\n\n<p>This research leverages Foundation Models to generate structured knowledge from materials science literature. Goals include enhancement of pre-existing datasets, making data in material science literature more discoverable, interoperable, and reusable, and simplifying the data mining workflow in materials science. The approach includes dataset management and construction, information extraction and inference, and knowledge discovery.</p>\n\n\n\n\n\n<p><strong>University of Michigan, Ann Arbor</strong>: Venkat Viswanathan (PI)</p>\n\n\n\n<p>The proposal seeks to employ foundation models, specifically large language models, to accelerate materials-level innovation necessary for the design of next-gen batteries. The research focuses on extensive evaluation of these models on a customized dataset on battery electrolyte design, aiming to enhance property prediction accuracy through foundation models. The ultimate goal is integrating foundation models into an automated materials design workflow, AutoMat, to expedite electrolyte design for advanced batteries.</p>\n\n\n\n\n\n<p><strong>University of Texas at Arlington</strong>: William Beksi (PI)</p>\n\n\n\n<p>The ability to reason about cause and effect from observational data is crucial for robust generalization in robotic systems. However, the construction of a causal graph, a mechanism for representing causal relations, presents an immense challenge. Currently, a nuanced grasp of causal inference, coupled with an understanding of causal relationships, must be manually programmed into a causal graphical model. To address this difficulty, we propose an innovative augmented reality framework for creating causal graphical models via large language models during human-robot interaction. Concretely, our system will bootstrap the causal discovery process by utilizing large language models to assist humans in selecting variables, establishing relationships, performing interventions, generating counterfactual explanations, and evaluating the resulting causal graph at every step.</p>\n\n\n\n\n\n<p><strong>Emory University</strong>: Eugene Agichtein (PI)</p>\n\n\n\n<p>The proposal aims to improve access to healthcare information by utilizing Large Language Models (LLMs) to close the gap between user queries and specialized medical knowledge. The goal is to enhance query understanding and representation for health-oriented search across dialects and languages. Techniques such as data augmentation, prompt optimization, retrieval augmentation, and pseudo-relevance feedback will be used. The outcome will be a toolkit for robust, multi-lingual health-oriented search models.</p>\n\n\n\n\n\n<p><strong>University of Illinois Urbana-Champaign</strong>: Heng Ji (PI)</p>\n\n\n\n<p>The research proposes integration of Generative AI and Computational Chemistry to accelerate the development of biofuels-targeted catalyst discovery. The research team has developed a system, ChemReasoner, that uses large language models for heuristic search in chemical space. The proposal aims to extend this system by integrating density functional theory (DFT) simulations capability using Microsoft\u2019s Azure Quantum Elements. The system will bring together scientific literature-driven symbolic reasoning with atomistic-level structure guided reasoning.</p>\n\n\n\n\n\n<p><strong>University of California, Santa Barbara</strong>: Shiyu Chang (PI)</p>\n\n\n\n<p>This proposal introduces a novel framework for detecting and mitigating hallucinations in LLMs by constructing and propagating model beliefs on a &#8220;belief tree&#8221;. For any statement generated by an LLM, our first goal is to curate the model&#8217;s internal knowledge by constructing a tree where each node represents a statement logically related to the parent node, and each edge represents the inferential relationship. We can then assess the hallucination of the root node by propagating the LLMs&#8217; beliefs from the children to parent nodes. Our rationale for constructing the belief tree is straightforward: directly determining the accuracy of a complex statement is challenging, while converting statements into a tree of interconnected propositions creates a clear visual and structural representation of the LLMs&#8217; internal knowledge. The truthfulness of a target statement can then be determined by the LLM&#8217;s belief in its &#8220;surrounding&#8221; statements, their inferential relationship, and the consistency of beliefs among them. Specifically, we aim to answer: Q1) How to construct belief trees to curate the models internal knowledge? Q2) How to propagate the confidence of the LLM in the belief tree for reliable hallucination detection? Q3) How to use the proposed method to update the LLM and reduce hallucinations?</p>\n\n\n\n\n\n<p><strong>Carnegie Mellon University</strong>: Katerina Fragkiadaki (PI)</p>\n\n\n\n<p>Use Large Language Models (LLMs) as semantic parsers to map instructions and dialogues to programs over neural perceptual and control routines few-shot, via appropriate prompting. We will maintain and continually update a memory of prompt examples that will be retrieved and composed on-the-fly to prompt LLMs for semantic parsing of human\u2019s instructions, questions, clarifications, and human-agent dialogues. We will further maintain memories of events in natural language that our LLM will access to maintain coherence across long timespans of interactions with the users, but also, to personalize the agent\u2019s behaviour.</p>\n\n\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models. via proactive knowledge discovery, hypothesis generation, and multiscale multimodal data generation This set of research projects explore how foundation models can be applied in a [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 995577,
        "date": "2024-01-05T08:07:40",
        "slug": "afmr-scientific-discovery-and-innovation",
        "title": "AFMR: Scientific Discovery and Innovation",
        "link": "https://www.microsoft.com/en-us/research/project/afmr-scientific-discovery-and-innovation/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Scientific-Discovery-and-Innovation-page-header_1920x720.png\" class=\"attachment-full size-full\" alt=\"white icon of a beaker with particles inside and a circled checkmark to the side on a green gradient background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Scientific-Discovery-and-Innovation-page-header_1920x720.png 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Scientific-Discovery-and-Innovation-page-header_1920x720-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Scientific-Discovery-and-Innovation-page-header_1920x720-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Scientific-Discovery-and-Innovation-page-header_1920x720-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Scientific-Discovery-and-Innovation-page-header_1920x720-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Scientific-Discovery-and-Innovation-page-header_1920x720-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Scientific-Discovery-and-Innovation-page-header_1920x720-240x90.png 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"Accelerating Foundation Models Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAccelerating Foundation Models Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"scientific-discovery-and-innovation\">Scientific Discovery and Innovation</h1>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\">\n<p><strong><em>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models.</em></strong></p>\n<cite>Brad Smith, Vice Chair and President</cite></blockquote>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-91 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:50%\">\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"400\" height=\"400\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Discovery-Natural-Sciences_1.3.png\" alt=\"dark green icon of a lightbulb with a plant growing inside and a ring around the lightbulb\" class=\"wp-image-996369\" style=\"width:auto;height:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Discovery-Natural-Sciences_1.3.png 400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Discovery-Natural-Sciences_1.3-300x300.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Discovery-Natural-Sciences_1.3-150x150.png 150w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Discovery-Natural-Sciences_1.3-180x180.png 180w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Discovery-Natural-Sciences_1.3-360x360.png 360w\" sizes=\"(max-width: 400px) 100vw, 400px\" /></figure>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center h4\" id=\"afmr-goal-accelerate-scientific-discovery-in-natural-sciences\">AFMR Goal: Accelerate scientific discovery in natural sciences</h2>\n\n\n\n<p class=\"has-text-align-center\">via proactive knowledge discovery, hypothesis generation, and multiscale multimodal data generation</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p>These projects focus on using foundation models to enhance knowledge discovery and hypothesis generation across many different areas. They particularly leverage the ability of general models to make sense of the exponentially growing volume of scientific literature in astronomy, materials science, and neuroscience. These efforts include exploring domain-specific prompt engineering and specializing foundation models through fine-tuning using techniques such as Low-Rank Adaption (LoRA). A series of proposals are dedicated to biomedical and life sciences research and innovation, including specialized models for drug discovery, genomics, protein engineering, and rare diseases. These proposals underscore the potential of foundation models to accelerate scientific discovery and innovation across many fields and disciplines.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<p><strong>University of Texas at Arlington</strong>: Miao Yin (PI)</p>\n\n\n\n<p>Ion chromatography (IC) is a powerful analytical chemistry technique for selective, sensitive quantification of aqueous ions spanning applications from environmental monitoring to biopharma pipelines. However, intrinsic slow analysis times severely throttle sample throughput. This project intends to develop an artificial intelligence-based platform accelerating IC by leveraging immense datasets from vast historical runs coupled with large foundation models tailored to effectively encode complex interactive influences of system parameters spanning columns, eluents, and detectors on separation performance into predictive modeling engines on Microsoft Azure. Additionally, a special tuning algorithm with analytical chemistry specialists&#8217; feedback will be developed to ensure the correct prediction of the large foundation IC model. Broader anticipated impacts are poised to revolutionize ion chromatography practices with AI across academic, manufacturing, and innovation areas while providing students at MSI with interdisciplinary research opportunities incorporating computer science and analytical chemistry.</p>\n\n\n\n\n\n<p><strong>Georgia Institute of Technology</strong>: Yunan Luo (PI)</p>\n\n\n\n<p>This proposal aims to leverage foundation models, including large language models trained on natural language and protein sequences, to advance protein function prediction and optimization. Two key areas of focus are 1) protein function prediction &#8211; predicting the biological roles of natural proteins and 2) protein function optimization &#8211; predicting which sequence mutations are beneficial for enhancing the function of natural proteins.</p>\n\n\n\n\n\n<p><strong>University College London</strong>: Bradley Love (PI)</p>\n\n\n\n<p>The project intends to utilize large language models (LLMs) to aid in the accumulation and assimilation of vast scientific literatures, especially in the field of neuroscience. The proposal aims to create BrainGPT, an AI tool for navigating and understanding large pools of data. The model will generate data patterns based on the scientific literature, assist in identifying anomalous findings, and offer insights for novel study designs. Additionally, the team intends to open source the models and training data for scientific scrutiny and improvements, fostering participation from the scientific community.</p>\n\n\n\n\n\n<p><strong>New Mexico State University</strong>: Huiping Cao (PI)</p>\n\n\n\n<p>Data-driven machine learning (ML) models built on large amounts of data have gained great success in many applications. However, their success is less observed in scientific domains. Scientific discoveries and hypothesis generation largely depends on knowledge (commonsense knowledge and expert-domain knowledge). Most of such knowledge is scattered in different sources and such knowledge is rarely utilized in data-driven ML models. Developing ML models that can take both data and knowledge as input in the learning process is still in its infancy.</p>\n\n\n\n<p>Many scientific domains collect multi-modality data. However, there is no good benchmark multi-modal datasets to evaluate foundation models.</p>\n\n\n\n<p>This project will design and develop novel neural network models to extract domain knowledge, incorporate domain knowledge and account for multi-modality data in the learning framework to improve learning accuracy and efficiency. The proposed methods will be applied to one scientific domain, animal sciences, to validate their usefulness, and generate knowledge base and a multi-modality datasets as a benchmark dataset.</p>\n\n\n\n\n\n<p><strong>University of California, Los Angeles</strong>: Aditya Grover (PI)</p>\n\n\n\n<p>The project proposes to develop a few-shot machine learning model to learn and optimize multi-task deep learning surrogates across various scientific and engineering domains. The plan includes unsupervised pretraining on large unlabelled datasets, followed by fine-tuning and evaluation on multiple disciplines, including bioengineering, material science, and mechanical design.</p>\n\n\n\n\n\n<p><strong>Harvard University</strong>: Alyssa Goodman (PI)</p>\n\n\n\n<p>We aim to enhance human interaction with astronomy literature by utilizing the capabilities of the Large Language Models, particularly GPT-4. We employ in-context prompting techniques to expose the model to astronomy papers to build an astronomy-focused chat application to engage the broader community. On the research track, we want to explore the potential foundation models have to generate novel scientific hypotheses. Specifically, we use GPT-4 to construct an instruction set of scientific ideas to fine-tune smaller models on this astronomy-specific downstream task. To assess their output\u2019s accuracy, feasibility and creativity, we employ a hybrid evaluation strategy consisting of human experts and judge GPT-4 instances. Our research will illuminate a novel and unique way of applying LLMs in the scientific arena.</p>\n\n\n\n\n\n<p><strong>University of Toronto Scarborough</strong>: Oleksandr Voznyy (PI)</p>\n\n\n\n<p>The proposal aims to establish Large Language Model (LLM) agents for inorganic materials discovery by augmenting GPT-3.5 with external tools and databases. The team will develop new text representations for the 3D structures of inorganic materials in order to enable discovery of materials for applications like catalysts, batteries, and photovoltaics.</p>\n\n\n\n\n\n<p><strong>Imperial College London</strong>: Aaron Zhao (PI)</p>\n\n\n\n<p>The proposal aims to enhance the understanding of complex genomic data by developing a novel Machine Learning framework. Through the use of a new mathematical formulation termed &#8216;hybrid graphs&#8217;, it is suggested that gene expression prediction can be improved beyond the capabilities of current sequence-based approaches. The proposal is also set to construct new databases and theoretical frameworks geared towards genomic data, addressing a current gap in the field.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.06150\" target=\"_blank\" rel=\"noreferrer noopener\">Latent Diffusion Model for DNA Sequence Generation<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of Washington</strong>: Georg Seelig (PI)</p>\n\n\n\n<p>This proposal aims to develop a protein document dataset based on ontology and interaction annotations that will be used for developing protein language models (pLMs) capable of handling multi-protein inputs without linker strings. To take advantage of a structured protein training set, loss and training techniques inspired by natural language models such as RoBERTa will be used. To evaluate if the dataset and training approach generates more informative embeddings, we will evaluate using embeddings for tasks like functional prediction and low-N protein modeling.</p>\n\n\n\n\n\n<p><strong>Yale University</strong>: Arman Cohan (PI)</p>\n\n\n\n<p>The proposal focuses on making connections within scholarly documents using AI to accelerate scientific discovery. It aims to develop NLP systems that can generate reliable and trustworthy long-form summaries in response to user queries. The ultimate goal is to make it easier for users to comprehend vast amounts of scientific literature and foster faster scientific exploration</p>\n\n\n\n\n\n<p><strong>Carnegie Mellon University</strong>: Larry Pileggi (PI)</p>\n\n\n\n<p>The proposal presents a new approach to Situation Awareness based on a Physics-ML synergy approach for which both the physical and ML models are embedded throughout the process to augment each other. This synergy framework enables fast, accurate, and end-to-end situation awareness that integrates system identification, anomaly detection and root cause diagnosis capabilities. The approach incorporates state of the art ML into the operation pipeline of real systems toward advanced operational efficiency, security, and reliable automatic control decision-making.</p>\n\n\n\n\n\n<p><strong>University of California, San Francisco</strong>: Tanja Kortemme (PI) </p>\n\n\n\n<p>This proposal aims to train a foundation model, Frame2seq, for protein sequence design. Frame2seq is a structure-conditioned masked language model with state-of-the-art accuracy and speed.&nbsp;Frame2seq will accelerate design&nbsp;of new functional proteins&nbsp;by robustly sampling sequence space unexplored in nature. This research has broad applications in material science, biotechnology, synthetic&nbsp;biology, and medicine.</p>\n\n\n\n\n\n<p><strong>University of Illinois Urbana-Champaign</strong>: Haohan Wang (PI)</p>\n\n\n\n<p>This proposal aims to develop a Team of AI-made Scientists (TAIS) that could dissect complex research questions, pull knowledge from a vast array of academic literature and databases, and employ quantitative and qualitative analysis to uncover deeper insights. The project tackles two main points: statistical trustworthiness (developing mathematical principles and parameter-efficient learning frameworks for large models) and collaborative trustworthiness (formulating an interactive paradigm for large models to work together).</p>\n\n\n\n\n\n<p><strong>The Ohio State University</strong>: Yuan-Sen Ting (PI)</p>\n\n\n\n<p>The proposal aims to adapt Large Language Models (LLMs) to address complex research queries within the field of astronomy, where current general-purpose LLMs often fall short. The team proposes to develop Foundation models adapted for astronomical research, using over 300,000 LaTeX papers and employing GPT-4-generated instructions for precision fine-tuning. The resulting model will be used for conversational question-and-answering (QA) and hypothesis-generation tasks.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2401.01916\" target=\"_blank\" rel=\"noreferrer noopener\">AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of New South Wales</strong>: Bram Hoex (PI)</p>\n\n\n\n<p>The proposal emphasizes using unsupervised word embeddings for predicting functional materials through a comprehensive assessment of various embedding methodologies and foundational models. The research framework uses language models for scientific discovery and analysis of the latent knowledge in publications.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.cell.com/patterns/fulltext/S2666-3899(24)00054-0\" target=\"_blank\" rel=\"noreferrer noopener\">Creation of a structured solar cell material dataset and performance prediction using large language models<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>Universit\u00e9 de Montr\u00e9al</strong>: Glen Berseth (PI)</p>\n\n\n\n<p>The proposed research project aims to explore how large language models (LLMs) can assist in reducing the search space over molecular design. The researchers plan to formulate the molecular search problem as a sequence-generation problem, and develop an approach that leverages text-based RL to enhance molecular discovery efforts. Proposed methods include improving the objectives of research with more grounded metrics for evaluation and enhancing generalization by curating and fine-tuning datasets from related design problems.</p>\n\n\n\n\n\n<p><strong>University of Washington</strong>: Sheng Wang (PI)</p>\n\n\n\n<p>We propose to develop GPT-BLIAM, a model that utilizes GPT models to generate sentence descriptions for diseases, proteins, and their interactions, to enable the prediction of protein-disease associations. Our team will evaluate the model using existing protein-disease association databases and incorporate domain knowledge from Human Phenotype Ontology to learn prompts for rare diseases. Our goal is to improve the quality of the protein and disease embeddings and develop a machine learning model that can predict unknown protein-disease associations.</p>\n\n\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models. via proactive knowledge discovery, hypothesis generation, and multiscale multimodal data generation These projects focus on using foundation models to enhance knowledge discovery and hypothesis generation [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 995568,
        "date": "2024-01-05T08:06:54",
        "slug": "afmr-multicultural-analysis-and-empowerment",
        "title": "AFMR: Multicultural Analysis and Empowerment",
        "link": "https://www.microsoft.com/en-us/research/project/afmr-multicultural-analysis-and-empowerment/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multicultural-Analysis-and-Empowerment-page-header_1920x720.png\" class=\"attachment-full size-full\" alt=\"white icon of an avatar inside of a magnifying glass surrounded by nodes on a green gradient background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multicultural-Analysis-and-Empowerment-page-header_1920x720.png 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multicultural-Analysis-and-Empowerment-page-header_1920x720-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multicultural-Analysis-and-Empowerment-page-header_1920x720-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multicultural-Analysis-and-Empowerment-page-header_1920x720-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multicultural-Analysis-and-Empowerment-page-header_1920x720-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multicultural-Analysis-and-Empowerment-page-header_1920x720-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multicultural-Analysis-and-Empowerment-page-header_1920x720-240x90.png 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"Accelerating Foundation Models Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAccelerating Foundation Models Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"multicultural-analysis-and-empowerment\">Multicultural Analysis and Empowerment</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\">\n<p><strong><em>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models.</em></strong></p>\n<cite>Brad Smith, Vice Chair and President</cite></blockquote>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-92 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:50%\">\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"400\" height=\"400\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3.png\" alt=\"medium green icon of three people standing under an archway with a checkmark\" class=\"wp-image-996363\" style=\"width:auto;height:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3.png 400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3-300x300.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3-150x150.png 150w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3-180x180.png 180w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3-360x360.png 360w\" sizes=\"(max-width: 400px) 100vw, 400px\" /></figure>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center h4\" id=\"afmr-goal-improve-human-interactions-via-sociotechnical-research\">AFMR Goal: Improve human interactions via sociotechnical research</h2>\n\n\n\n<p class=\"has-text-align-center\">which increases trust, human ingenuity, creativity, and productivity, and decreases the digital divide while reducing the risks of developing AI which does not benefit individuals and society</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p>The research projects primarily focus on enhancing language models, emphasizing underrepresented languages and cultures. Projects aim to improve the accuracy of health-related responses and fine-tune models for specific languages like Vietnamese and various Indian languages. Cultural intelligence is a key goal, promoting linguistic inclusivity and understanding model behavior with knowledge graph tools. Additional efforts involve developing model-editing techniques for interpretability and robustness, particularly in underrepresented languages. The overarching aim is to enhance language models for improved accuracy, adaptability, and inclusivity across diverse languages and cultures.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<p><strong>University of Waterloo</strong>: Jimmy Lin (PI)</p>\n\n\n\n<p>The proposal aims to build robust foundation models for African languages to bridge the technology gap affecting these communities. The project objectives include design and optimization of model architecture, multilingual transfer learning, evaluation, and making the resources openly available.</p>\n\n\n\n\n\n<p><strong>University of British Columbia</strong>: Vered Shwartz (PI)</p>\n\n\n\n<p>The proposal aims at addressing the cultural bias in Large Language Models (LLMs), which currently hold a heavy Western, North American, or even US-centric lens. By constructing a new dataset consisting of narratives that evoke social norms, the proposal aims to test the values of English LLMs as they reflect in real-world scenarios and better align the responses of LLMs with the values of diverse cultures.</p>\n\n\n\n\n\n<p><strong>Kennesaw State University</strong>: Dylan Goldblatt (PI)</p>\n\n\n\n<p>This project aims to explore applications of AI to provide personalized and culturally-responsive support for second language learners at KSU. The objectives are to establish whether an AI learning support approach improves performance and engagement in language courses; if the approach helps narrow the achievement and engagement gap for underprepared students; and whether the support approach is successful across various languages.</p>\n\n\n\n\n\n<p><strong>New York University</strong>: Duygu Ataman (PI)</p>\n\n\n\n<p>Recent advances have brought Large Language Models (LLMs) to an important stage that will play a significant role in shaping the next generation of applications in essential social domains, such as education and the media. Despite the continuous exploration of its remarkable capabilities, the performance of state-of-the-art models in most languages typically falls short of matching their counterparts in English.&nbsp;This project aims to bridge this gap by developing an adaptation methodology to improve LLM compatibility with under-resourced languages. The study uses Turkic languages as a case study, whose grammatical features present a challenging yet ideal setting for assessing NLP models.</p>\n\n\n\n\n\n<p><strong>Georgia Institute of Technology</strong>: Srijan Kumar (PI)</p>\n\n\n\n<p>Investigate the capabilities of GPT-4 and its effectiveness in answering health-related queries in various languages. Our research will develop a comprehensive understanding of how broadly applicable the health-related reasoning abilities of foundational models are beyond the English language.</p>\n\n\n\n<p><strong>Related papers:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.13132\" target=\"_blank\" rel=\"noreferrer noopener\">Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2306.11065\" target=\"_blank\" rel=\"noreferrer noopener\">Cross-Modal Attribute Insertions for Assessing the Robustness of Vision-and-Language Learning<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>IIT Gandhinagar</strong>: Mayank Singh (PI)</p>\n\n\n\n<p>The proposal plans to develop model-editing techniques that can localize multilingual information and selectively update the parameters of Large Language Models (LLMs). The project involves experimenting with parameter-preserving and parameter-updating editing techniques. The goal of the project is to enhance LLMs in terms of interpretability, robustness, and factual accuracy for diverse communities.</p>\n\n\n\n\n\n<p><strong>IIT Bombay</strong>: Soumen Chakrabarti (PI)</p>\n\n\n\n<p>Our focus will be on the behavior of the latest generation of LLMs and their interaction with knowledge graph (KG) retrieval tools, in the context of Indian low-resource languages (LRLs), because it is easier to locate users of such languages locally, and code-switched texts.</p>\n\n\n\n\n\n<p><strong>University College London</strong>: Pontus Stenetorp (PI)</p>\n\n\n\n<p>The proposal aims to improve the performance of Large Language Models (LLMs) on African languages by augmenting low-resource African text data with synthetic data. It proposes a two-step method involving benchmarking tasks to understand the performance gap followed by generating training data for African languages.</p>\n\n\n\n\n\n<p><strong>IIT Kharagpur</strong>: Niloy Ganguly (PI)</p>\n\n\n\n<p>The proposal plans to analyze the performance of Large Language Models (LLMs) for Indian languages. Despite their proven utility, LLMs have not shown significant improvement for tasks in Indian languages compared to high-resource languages, possibly due to an underrepresented training corpus. The research aims to extensively benchmark LLMs&#8217; capabilities, strengths, and weaknesses for various tasks in Indian languages. The goal is to identify &#8216;good&#8217;, &#8216;bad&#8217;, and &#8216;ugly&#8217; performance cases and develop strategies for improvement, potentially addressing the underrepresentation of Indian languages in LLM performances.</p>\n\n\n\n\n\n<p><strong>KAIST</strong>: Alice Oh (PI)</p>\n\n\n\n<p>Develop a culturally-intelligent language model by creating a red-teaming dataset that evaluates actions in different cultures and testing the language model\u2019s responses in various language and cultural settings. The goal is to improve NLP models\u2019 awareness of cultural diversity and their ability to generate culturally intelligent responses.</p>\n\n\n\n\n\n<p><strong>Saarland University</strong>: Dietrich Klakow (PI)</p>\n\n\n\n<p>The project aims to advance research in multilingual foundation models, focusing on closing the gap in capabilities between English and non-English languages. The team plans to analyze the cross-lingual transfer abilities of foundation models, and seek to enhance these abilities with a focus on in-context learning.</p>\n\n\n\n\n\n<p><strong>IIT Bombay</strong>: Soumen Chakrabarti (PI)</p>\n\n\n\n<p>We are probing LLMs to detect presence or absence of knowledge in multi-lingual knowledge graphs, with a focus on low resource languages (LRLs). As we move from popular to even slightly obscure entities and relations, we are finding that the coverage and reliability of LLMs fall of perceptibly. Packaging a knowledge query in a prompt context, as well as fair evaluation of text output against structured gold knowledge, are proving challenging. We are also comparing LLMs against pure graph embedding techniques. We are finding that these two families of techniques make uncorrelated errors, suggesting a unified architecture leveraging the strengths of both.</p>\n\n\n\n\n\n<p><strong>Ho Chi Minh City University of Technology</strong>: Duc Nguyen (PI)</p>\n\n\n\n<p>This proposal aims to create a finetuned large language model (LLaMa-2) specifically for Vietnamese using the QLoRa technique. The researchers seek to bring about proficiency in Vietnamese that rivals human-level communication while maintaining the vast knowledge base of the original model. An evaluation against other commercial models is also planned.</p>\n\n\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models. which increases trust, human ingenuity, creativity, and productivity, and decreases the digital divide while reducing the risks of developing AI which does not benefit individuals [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 995562,
        "date": "2024-01-05T08:06:06",
        "slug": "afmr-creativity-and-design",
        "title": "AFMR: Creativity and Design",
        "link": "https://www.microsoft.com/en-us/research/project/afmr-creativity-and-design/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Creativity-and-Design-page-header_1920x720-659436165a9b0.png\" class=\"attachment-full size-full\" alt=\"white icon of a fountain pen tip inside of a lightbulb on a green gradient background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Creativity-and-Design-page-header_1920x720-659436165a9b0.png 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Creativity-and-Design-page-header_1920x720-659436165a9b0-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Creativity-and-Design-page-header_1920x720-659436165a9b0-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Creativity-and-Design-page-header_1920x720-659436165a9b0-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Creativity-and-Design-page-header_1920x720-659436165a9b0-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Creativity-and-Design-page-header_1920x720-659436165a9b0-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Creativity-and-Design-page-header_1920x720-659436165a9b0-240x90.png 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"Accelerating Foundation Models Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAccelerating Foundation Models Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"creativity-and-design\">Creativity and Design</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\">\n<p><strong><em>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models.</em></strong></p>\n<cite>Brad Smith, Vice Chair and President</cite></blockquote>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-93 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:50%\">\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"400\" height=\"400\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3.png\" alt=\"medium green icon of three people standing under an archway with a checkmark\" class=\"wp-image-996363\" style=\"width:auto;height:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3.png 400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3-300x300.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3-150x150.png 150w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3-180x180.png 180w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3-360x360.png 360w\" sizes=\"(max-width: 400px) 100vw, 400px\" /></figure>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center h4\" id=\"afmr-goal-improve-human-interactions-via-sociotechnical-research\">AFMR Goal: Improve human interactions via sociotechnical research</h2>\n\n\n\n<p class=\"has-text-align-center\">which increases trust, human ingenuity, creativity, and productivity, and decreases the digital divide while reducing the risks of developing AI which does not benefit individuals and society</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p>These projects examine how AI can be used across different disciplines to support human creativity, improve workflow efficiency, and enrich the user experience. Some of the notable projects include using these models to inspire human creativity, and applying AI-transformed applications for immersive learning, coaching, and language learning. Moreover, proposals aim to improve the efficiency of AI-powered agents with advanced language communication tools and explore how AI can enhance creativity in virtual reality settings. Another interesting venture is to look into how AI can make academic insights more accessible for practitioners, and how it can comprehend and utilize emotional intelligence. Methodologies range from host studio sessions, design workshops, experiments and computational frameworks, among others. We expect these initiatives to lead to innovations that can address the current challenges, provide tools that can improve user experience and facilitate learning and creativity, and new discoveries in the understanding of AI&#8217;s emotional intelligence ability.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<p><strong>Cornell University</strong>: Jeffrey Rzeszotarski (PI)</p>\n\n\n\n<p>This project outlines two experiments aimed at understanding how generative models can support human creativity. It explores the integration of DALL-E 2 into a creative drawing lab experiment and how GPT-4 can be integrated into an existing qualitative thematic coding tool to enhance user experience. By focusing on these successful AI models, it seeks to mitigate the potential risks of their application in creative tasks.</p>\n\n\n\n\n\n<p><strong>The University of Texas at Arlington</strong>: Cesar Torres (PI)</p>\n\n\n\n<p>Bricolage is a creative practice emphasizing the influence of a practitioner&#8217;s environment (physical, social, or otherwise) on their creative process. In contrast to an empty room or screen, bricolage posits that spaces filled with possibilities (e.g., via galleries of artifacts, varieties of tools, diverse peoples) encourage improvisation, innovation, and resourcefulness; however, such spaces often evolve over decades of practice. Large language models offer a unique opportunity to generate conceptual resources to enhance bricolage practice across a variety of disciplines. Leveraging our current work on harvesting video tutorial transcripts from 30 communities of practice, this work aims to tune foundational models into practice-tuned LLMs (e.g., ElectronicsGPT, 3DPrintingGPT, CeramicsGPT, PromptGPT). These models will be used to extract and organize information about techniques, tools, and materials into accessible formats, such as dynamic AI-generated image collages and flow charts. This approach will offer a collection of bricolage resources to enhance human creativity, promote interdisciplinary innovation, and serve as a bridge for more practitioners to leverage AI within their respective practices.</p>\n\n\n\n\n\n<p><strong>IIT Tirupati</strong>: Sridhar Chimalakonda (PI)</p>\n\n\n\n<p>Moving away from the predominantly common approach of hand-crafting source code representations [e.g. Abstract Syntax Tree (AST), Control Flow Graphs (CFG)] and AI pipelines for a given software engineering task (e.g. Code Summarization, Bug Localization, Code Clone Detection), the proposed project aims to find the appropriate mix of spruce code representation for a given SE task. Specifically, the proposed project builds on our mocktail approach and aims to create a&nbsp;a framework that can facilitate configuring and experimenting with different types and combinations of source code representations and ML models for various SE tasks. We see that this framework can help researchers and practitioners to explore, experiment and build an appropriate AI pipeline for a given SE task without manually creating each instance of the AI pipeline.</p>\n\n\n\n\n\n<p><strong>Prairie View A&M University</strong>: Malachi Crawford (PI)</p>\n\n\n\n<p>Briefly, this research project seeks to evaluate the use of large language models, such as GPT-4 and DALL-E 2, in the creation of a stage play from primary source data. Through a process of AI prompting and iteration, we will develop a rubric to evaluate the model&#8217;s performance across five indices: character development, character dialogue, plot structure, set design/visual elements, and stage direction. Ultimately, we anticipate AI diminishing the barriers, such as time spent writing dialogue, scene creation, outlining stage direction, and other highly-skilled artistic activity that might constrain students of history from bridging the historical profession with the creative and performative arts. In so doing, this proposal aligns with the Advance Beneficial Applications of AI research component.</p>\n\n\n\n\n\n<p><strong>William & Mary</strong>: Yixuan Zhang (PI)</p>\n\n\n\n<p>This proposal aims to investigate the capacity of LLMs to understand and harness emotional intelligence. The research expands emotional stimuli used in LLMs, assesses the depth and duration of emotional interaction with LLMs, and its influence on people\u2019s perceptions and trust.</p>\n\n\n\n\n\n<p><strong>University of Missouri-Kansas City</strong>: Shu-Ching Chen (PI)</p>\n\n\n\n<p>This research proposal aims to investigate the operational dynamics of Large Language Model (LLM) powered agents and their efficiency in task coordination. The primary focus areas of the study involve understanding the mechanisms that drive these agents, their specialized use of prompts and tools, knowledge sharing, and the role of a Knowledge Query Manipulation Language (KQML) inspired language in enhancing communication between these agents. The research aims to contribute additional knowledge to the field of LLM applications and explores the possibility of improving task efficiency and coordination. Moreover, the research intends to establish a foundation for future implementations in various applications requiring advanced task reasoning.</p>\n\n\n\n\n\n<p>University of Illinois, Chicago: Nikita Soni (PI)</p>\n\n\n\n<p>The long-term goal of this project is to design intuitive and natural collaborative child-AI visual storytelling interfaces for children&#8217;s creativity support. Researchers have made multiple efforts to enhance the efficiency of human-AI interactions in creativity support tools through algorithmic advancements. However, understanding how users of all ages, including children, interact with these creativity interfaces is limited. The output of this proposal will be a publicly available prototype and evidence-based design guidelines to inform the design of future AI-based visual storytelling interfaces for children.</p>\n\n\n\n\n\n<p><strong>Carnegie Mellon University</strong>: Sarah Fox (PI)</p>\n\n\n\n<p>This proposal focuses on how queer artists leverage generative AI, highlighting the importance of considering queer people as users of GMs rather than solely targets of harm. The study will work with artists over multiple weeks to host studio hours and facilitate design workshops to understand challenges encountered by queer artists while using GMs and identify potential solutions.</p>\n\n\n\n\n\n<p><strong>Simon Fraser University</strong>: Steve DiPaola (PI)</p>\n\n\n\n<p>The project aims to develop a next-generation AI conversational virtual human character and generative systems to enhance immersive training and coaching in various sectors. The research proposes to further AI visual agents and text-based conversational agents to facilitate language learning, job training simulations, health coaching, and real-time guidance.</p>\n\n\n\n\n\n<p><strong>University of Notre Dame</strong>: Diego Gomez-Zara (PI)</p>\n\n\n\n<p>This proposal explores how LLMs can support human-AI collaboration in creativity tasks within virtual reality (VR) environments. It aims to comprehend the social and psychological effects of large language models on team creativity in virtual environments. The goal is to develop a computational framework to implement LLMs for VR environments, replicating the physical sequences for AI agents in human-AI teams.</p>\n\n\n\n\n\n<p><strong>University of Washington</strong>: Gary Hsieh (PI)</p>\n\n\n\n<p>The proposal focuses on leveraging Generative AI to translate scientific insights from publications into more accessible forms for design practitioners, specifically through &#8216;design cards&#8217;. Using Microsoft Azure, they aim to further develop an initial version of a system that can convert academic papers into design cards, which has shown promise in preliminary studies. The research will investigate personalized translational resources, trust issues in AI-translated work, and real-world applications of AI-generated design implication cards.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://faculty.washington.edu/garyhs/docs/shin-CHI2024-paper2card.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">From Paper to Card: Transforming Design Implications with Generative AI<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models. which increases trust, human ingenuity, creativity, and productivity, and decreases the digital divide while reducing the risks of developing AI which does not benefit individuals [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 995556,
        "date": "2024-01-05T08:05:12",
        "slug": "afmr-cognition-and-societal-benefits",
        "title": "AFMR: Cognition and Societal Benefits",
        "link": "https://www.microsoft.com/en-us/research/project/afmr-cognition-and-societal-benefits/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Congition-and-societal-benefits-page-header_1920x720-659435f620979.png\" class=\"attachment-full size-full\" alt=\"white gear icon with a circle outline on a green gradient background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Congition-and-societal-benefits-page-header_1920x720-659435f620979.png 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Congition-and-societal-benefits-page-header_1920x720-659435f620979-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Congition-and-societal-benefits-page-header_1920x720-659435f620979-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Congition-and-societal-benefits-page-header_1920x720-659435f620979-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Congition-and-societal-benefits-page-header_1920x720-659435f620979-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Congition-and-societal-benefits-page-header_1920x720-659435f620979-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Congition-and-societal-benefits-page-header_1920x720-659435f620979-240x90.png 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"Accelerating Foundation Models Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAccelerating Foundation Models Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"cognition-and-societal-benefits\">Cognition and Societal Benefits</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\">\n<p><strong><em>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models.</em></strong></p>\n<cite>Brad Smith, Vice Chair and President</cite></blockquote>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-94 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:50%\">\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"400\" height=\"400\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3.png\" alt=\"medium green icon of three people standing under an archway with a checkmark\" class=\"wp-image-996363\" style=\"width:auto;height:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3.png 400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3-300x300.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3-150x150.png 150w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3-180x180.png 180w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Improve-Human-Interaction_1.3-360x360.png 360w\" sizes=\"(max-width: 400px) 100vw, 400px\" /></figure>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center h4\" id=\"afmr-goal-improve-human-interactions-via-sociotechnical-research\">AFMR Goal: Improve human interactions via sociotechnical research</h2>\n\n\n\n<p class=\"has-text-align-center\">which increases trust, human ingenuity, creativity, and productivity, and decreases the digital divide while reducing the risks of developing AI which does not benefit individuals and society</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n</div>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<p>The proposals mainly focus on significant advancements in the field of healthcare, education, and various social aspects. They highlight the use of Large Language Models (LLMs) to enhance several aspects, such as improved teaching in online education platforms, generating personalized education for cybersecurity, and advancing health outcomes research. There are also proposals focused on understanding the proficiency of LLMs in extracting and understanding clinical data, simulating student interactions in classrooms, and developing privacy-aware medical dialogue systems. Other studies investigate the utility and harms of LLMs for mental health support, their use in English as a foreign language (EFL) education, and their potential application within the legal field. In healthcare, LLMs aim to not only assist doctors in patient-trial matching and radiology report summarization but also to provide patients with more understandable health data. Additionally, there are efforts to align LLMs with the diversity of global user preferences, and establish standardized protocols for using Generative Artificial Intelligence (GAI) in behavioral research, among others.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<p><strong>George Mason University</strong>: Ziyu Yao (PI)</p>\n\n\n\n<p>The proposal is focused on using Large Language Models (LLMs) to simulate student agents discussing STEM concepts in a virtual classroom. The platform is intended to aid STEM concept learning in PreK-12 education, facilitating teacher professional development and immersive peer learning. The researchers aim to develop student agents with consistent stances in concept understanding, which would interact and debate with each other. A human teacher or student can also partake in the discussions, fostering deeper concept learning.</p>\n\n\n\n\n\n<p><strong>University of Illinois Urbana-Champaign</strong>: Volodymyr Kindratenko (PI)</p>\n\n\n\n<p>The proposal is about developing a versatile platform that allows the creation of course-specific chatbots for teaching and research purposes. The system uses the GPT-4 model via OpenAI API, and is capable executing codes, accessing databases, and assisting with computational research tasks.</p>\n\n\n\n\n\n<p><strong>MIT</strong>: Marzyeh Ghassemi (PI)</p>\n\n\n\n<p>Investigate the bias of de-identification systems on names in clinical notes via a large-scale empirical analysis. To achieve this, we created 16 name sets that vary along four demographic dimensions: gender, race, name popularity, and the decade of popularity. We insert these names into 100 manually curated clinical templates and evaluate the performance of nine public and private de-identification methods. We found that there are statistically significant performance gaps along a majority of the demographic dimensions in most methods.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2305.11348\" target=\"_blank\" rel=\"noreferrer noopener\">In the Name of Fairness: Assessing the Bias in Clinical Record De-identification<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>Morehouse School of Medicine</strong>: Muhammed Idris (PI)</p>\n\n\n\n<p>Breast and Cervical Cancers are among the most common cancers and leaders in cancer-related deaths among women worldwide. Due to low screening rates among disadvantaged groups, including minorities, late-stage diagnoses and mortality rates were higher when compared to their counterparts. The overarching goal of our project is to evaluate the capabilities of foundation models to help facilitate accessible and culturally congruent cancer-related health information with the aim of addressing community concerns (i.e., trust, myths, access) and promoting cancer screenings among underrepresented women of color. Specifically, we will evaluate and compare the breadth, quality, and accuracy of the health-related information around specific community concerns generated by GPT and Llama-2 model families and develop a prototype of an interactive tool, fine-tuned around specific community concerns related to breast and cervical cancer screening using GPT-3 and leverage Azure API and DALL-E 2 to address health literacy barriers.</p>\n\n\n\n\n\n<p><strong>University of Illinois Chicago</strong>: Mohan Zalake (PI)</p>\n\n\n\n<p>The research aims to evaluate the acceptability of using Digital Twins of Doctors (DTDs) in patient care. DTDs are AI-generated characters that share the facial and vocal identities of real doctors and can deliver health information to patients. Past research has explored the benefits (e.g., efficient delivery of repetitive information and personalizing patient care) and concerns (e.g., ethical and social concerns) for integrating DTDs in patient care from the perspective of doctors who share their identities with DTDs. Given there exist both potential benefits and limitations, research efforts are required to systematically study the implications of using DTDs in healthcare with all the stakeholders before widely adopting them. In this proposal, I aim to evaluate the acceptability of DTDs with the next important stakeholder: patients; by understanding a) how patients perceive and respond to DTDs that share the identities of their own doctors and b) how DTDs influence patients\u00e2\u20ac\u2122 trust, engagement, comprehension, and adherence to health information and advice. I will conduct a mixed-methods study involving patients interacting with DTDs, filling surveys, and follow-up interviews. The proposed research will contribute to the understanding of responsible integration of generative AI solutions like DTDs into healthcare.</p>\n\n\n\n\n\n<p><strong>Harvard University</strong>: Pranav Rajpurkar (PI)</p>\n\n\n\n<p>This proposal aims to develop an evaluation framework for assessing the performance of Large Language Models in medical AI applications. The framework simulates real-world doctor-patient conversations and uses AI agents to imitate doctor-patient interaction and assess conversational abilities. The project will initially focus on assessing the diagnosis of skin conditions.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://openreview.net/forum?id=Bk2nbTDtm8\" target=\"_blank\" rel=\"noreferrer noopener\">CRAFT-MD: A Conversational Evaluation Framework for Comprehensive Assessment of Clinical LLMs<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of Washington</strong>: Jennifer Mankoff (PI)</p>\n\n\n\n<p>The proposal aims to explore the capability of Generative AI (GAI) in the realm of text simplification, particularly to aid individuals with cognitive impairments. The team plans to develop a proof-of-concept text simplification system using Open AI\u2019s GPT4 for both generation and validation, addressing the potential risks and consequences for people with disabilities along the way. The research seeks to bridge the digital divide for those who benefit from simplified text and in the process generate important new insights about the value of GAI in accessibility.</p>\n\n\n\n\n\n<p><strong>University of California, Berkeley</strong>: Ahmed Alaa (PI)</p>\n\n\n\n<p>The proposal seeks to develop a foundation model that can aid in analyzing real-world observational data (RWD) and generate high-quality Real-World Evidence (RWE). The researchers aim to reduce the development time and expert input required to devise Statistical Analysis Plans (SAPs). The steps proposed include studying the zero-shot performance of LLMs at generating SAPs, and then refining the process to produce a model that can automate SAP production.</p>\n\n\n\n\n\n<p><strong>University of Illinois Urbana-Champaign</strong>: Karrie Karahalios (PI)</p>\n\n\n\n<p>The proposal aims to study the impact of AI errors on learners&#8217; engagement, learning outcomes, and perceived helpfulness of AI educational systems. The researchers intend to conduct an online controlled experiment involving adult learners in STEM where they are presented with various scenarios of imperfection in conversational Q&A systems. The outcomes of this study aim to provide the necessary knowledge to maximize learners&#8217; gains and do so fairly.</p>\n\n\n\n\n\n<p><strong>University of British Columbia</strong>: Xiaoxiao Li (PI)</p>\n\n\n\n<p>This project aims to leverage Large Language Models (LLM) for multimodal medical data analysis in community healthcare focusing on wound care. It proposes to build a trustworthy conversational AI tuned to provide evidence-based responses to clinical inquiries. Moreover, it seeks to overcome challenges related to multimodal data complexity, trustworthiness and fairness.</p>\n\n\n\n\n\n<p><strong>Northeastern University</strong>: Vedant Swain (PI)</p>\n\n\n\n<p>*<a href=\"https://www.microsoft.com/en-us/research/collaboration/ai-cognition-and-the-economy-aice/overview/\" target=\"_blank\" rel=\"noreferrer noopener\"><em>AICE Accelerator collaboration</em></a></p>\n\n\n\n<p>To make AI agents more empathetic towards worker\u2019s goals, the agent needs to (i) understand broader wellbeing goals beyond saving time, (ii) maintain latitudinal and longitudinal awareness of workers\u2019 context outside their task, and (iii) provide workers suggestions to meet those goals by preempting opportunities in their work context. In this project, we propose to prototype and study Pro-Pilot, an enhancement over the existing Copilot that introduces a new Human-AI interaction framework that builds empathy.</p>\n\n\n\n\n\n<p><strong>University of Toronto</strong>: Alistair Johnson (PI)</p>\n\n\n\n<p>Create a highly accurate and efficient deidentification system that can be applied to various medical data sources, ultimately facilitating secure data sharing and collaboration in the healthcare industry.</p>\n\n\n\n\n\n<p><strong>The University of Texas at Arlington</strong>: Junzhou Huang (PI)</p>\n\n\n\n<p>This project proposes to address the critical challenge in personalized healthcare of accurately predicting survival outcomes using digital pathology techniques. It identifies two key challenges: the complexity of microenvironment of tissues in histopathological images, and the integration of the images with corresponding biomedical text data. To tackle these, we propose two aims: The first is to develop an advanced cell segmentation foundation model that enhances feature extraction and analysis in histopathological images. The second aim focuses on developing a multimodal foundation model that effectively combines pathological image features with biomedical captions for improved survival predictions. These proposed methods promise to significantly influence both machine learning and histopathological imaging by introducing novel foundation models for integrated image-caption data analytics and have the potential to impact other related fields in a similar capacity.</p>\n\n\n\n\n\n<p><strong>Georgia Institute of Technology</strong>: May Dongmei Wang (PI)</p>\n\n\n\n<p>Expedite AI research and improve healthcare by developing a privacy-aware medical dialogue system that 1) leverages human interaction and prompting in dialogue systems for unstructured clinical data analysis, and 2) adapts large language models (LLMs) to clinical use cases.</p>\n\n\n\n<p><strong>Related papers:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2401.07128\" target=\"_blank\" rel=\"noreferrer noopener\">EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://dl.acm.org/doi/10.1145/3584371.3612956\" target=\"_blank\" rel=\"noreferrer noopener\">Retrieval-Augmented Large Language Models for Adolescent Idiopathic Scoliosis Patients in Shared Decision-Making<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>Waseda University</strong>: Daisuke Kawahara (PI)</p>\n\n\n\n<p>Proposal for a system to assist visually impaired individuals in outdoor navigation by using vision and language foundation models to extract visual information from images/videos captured by mobile cameras. These insights are communicated through relevant dialogues. Includes use of Azure and OpenAI technologies, and the creation of two specific datasets for model development.</p>\n\n\n\n\n\n<p><strong>University of Michigan, Ann Arbor</strong>: Qiaozhu Mei (PI)</p>\n\n\n\n<p>The proposal focuses on a novel approach, distributional alignment, that aligns Large Language Models (LLMs) to the broad spectrum of human preferences stemming from varied contexts. This involves the collection of diverse human contexts and preferences across expansive domains and their integration into LLM training and refinement. Unique metrics to assess the diversity of LLM outputs will be introduced. The investigators request access to GPT-3, GPT-4, and other modeling resources to aid their research.</p>\n\n\n\n\n\n<p><strong>Harvard University</strong>: Emily Alsentzer (PI)</p>\n\n\n\n<p>Clinicians face challenges in summarizing a patient\u2019s medical history upon hospital admission due to information overload in electronic health records. We aim to develop LLM-based methods for generating factual summaries by leveraging retrieval-based approaches and to design evaluation approaches for assessing the quality of the generated summaries, comparing them to existing metrics and clinician evaluations.</p>\n\n\n\n\n\n<p><strong>University of New South Wales</strong>: Raina MacIntyre (PI)</p>\n\n\n\n<p>The project aims to further develop EPIWATCH, an epidemic detection and surveillance system, using large language models (LLMs). The developed LLMs will be used to automoate certain functions of EPIWATCH and will include low-resource languages important for Australian communities. The models will be fine-tuned and retrained for tasks including classification of public health threats and extraction of key information from large data sets. The project aims to fill the gap in AI usage for public health, especially for underrepresented languages.</p>\n\n\n\n\n\n<p><strong>KAIST</strong>: Sangchul Park (PI)</p>\n\n\n\n<p>Study prompting or fine-tuning strategies for improving GPT\u2019s capabilities for contract generation. A set of prompts can be produced as an output of this research. I find GPT particularly good at generating contract terms if proper prompts are fed into it. I will study prompting&nbsp;or&nbsp;fine-tuning&nbsp;strategies&nbsp;for improving GPT\u2019s&nbsp;capabilities for&nbsp;contract generation. A set of prompts can be produced as an output of this research. I will also try to prepare an evaluation set for measuring the performance of contract generation and compare GPT with other language models. If there are difficulties in designing an evaluation set, I can consider, as an alternative, conducting a \u201csnowball sampling\u201d to solicit multiple law professors for human evaluation, or using my law school class to engage law students for evaluation.</p>\n\n\n\n\n\n<p><strong>University of California, San Francisco</strong>: Vivek Rudrapatna</p>\n\n\n\n<p>The project aims to evaluate the accuracy of GPT-4 in extracting patient symptoms and medications from clinical notes in the electronic health record. It intends to compare GPT-4&#8217;s performance against comparator models and explore hybrid approaches to improve the accuracy of clinical information extraction.</p>\n\n\n\n\n\n<p><strong>San Diego State University</strong>: Hajar Homayouni</p>\n\n\n\n<p>AI&#8217;s potential in healthcare is limited by the scarcity of representative and balanced Electronic Health Records (EHRs). This proposal aims to address issues stemming from inaccessible, incomplete, and biased EHRs crucial for critical data analysis and decision-making. The research approach involves utilizing limited available data to generate balanced, correlated EHRs for precise and equitable training and validation of data-driven models. The proposed solution is a Federated Privacy-preserving Multimodal Generative (FPMG) framework, designed to generate unbiased EHR data and facilitate secure collaborative learning. Primarily, it targets the generation of balanced and correlated multimodal EHR data types, utilizing a deep generative adversarial model. By capturing cross-modal correlations and associations, the framework aims to enhance decision-making systems. Additionally, the project seeks to explore decentralized cross-silo federated learning to safeguard patients data privacy and enhance the robustness and generalization of models in healthcare applications.</p>\n\n\n\n\n\n<p><strong>University of California, Berkeley</strong>: David Bamman (PI)</p>\n\n\n\n<p>Foundation models such as ChatGPT, GPT-4 and Llama 2 are poised to transform research at the intersection of natural language processing and computational social science/cultural analytics, not simply in providing more accurate measuring instruments for existing tasks (Ziems et al. 2023) but also in opening up the ability to ask fundamentally more difficult questions that require world knowledge, long document context, and sophisticated inference. This research project probes the ability of foundation models to accelerate responsible computational research in the social sciences and humanities; the goal is to generate new knowledge about culture and society and provide a roadmap for other researchers to do so themselves.</p>\n\n\n\n\n\n<p><strong>Rice University</strong>: Xia Hu (PI)</p>\n\n\n\n<p>The proposal seeks to leverage the abilities of Large Language Models (LLMs) to address the challenge of accurately and reliably matching patients with appropriate clinical trials. It aims to achieve precise and reliable patient-trial matching by resolving the incompatibility between Electronic Health Records (EHRs) and clinical trial descriptions and providing comprehensive explanations for the matches.</p>\n\n\n\n\n\n<p><strong>University of North Carolina at Charlotte</strong>: Razvan Bunescu (PI)</p>\n\n\n\n<p>With the aim of improving teaching and learning of coding, we propose to develop Socratic conversational agents by fine-tuning large foundation models on a dataset of dialogues where an instructor helps students debug code. The Socratic conversational agents are intended to augment human instruction, assisting novice programmers to fix their code and thus enhancing their learning outcomes.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.03210\" target=\"_blank\" rel=\"noreferrer noopener\">Can Language Models Employ the Socratic Method? Experiments with Code Debugging<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of California, Berkeley</strong>: Juliana Schroeder (PI)</p>\n\n\n\n<p>This proposal tackles the urgent need for standardized protocols when integrating Generative Artificial Intelligence (GAI) into behavioral research. The research goals include understanding the current state of GAI use in behavioral science, exploring the potential benefits and risks, and developing guidelines for its effective and responsible use. The proposal includes the conduction of a large field study and consultation with a panel of experts.</p>\n\n\n\n\n\n<p><strong>Florida International University</strong>: Mohammadhadi Amini (PI)</p>\n\n\n\n<p>Natural disasters introduce major challenges for critical infrastructure and human lives. In such scenarios, effective communication among first responders, agencies, and residents, is critical to ensure timely recovery and survival. However, existing notification systems are not benefiting from the state-of-the-art AI-based solutions to handle the real-time evolving situations that arise during disasters. Hence, this project proposes to develop and evaluate a conversational agent, using Microsoft Azure OpenAI Services, that can facilitate the coordination of stakeholders in disaster situations using natural language. The conversational agents that are created using Microsoft Azure OpenAI service will be based on large language models (LLMs) that are fine-tuned using historical disaster datasets. The PI and his team have prior experience in using pre-trained models for computer vision, critical infrastructure resilience, and healthcare applications. They used MS Azure services including Azure Machine Learning.</p>\n\n\n\n<p>The performance of the conversational agent will be validated using synthetic scenarios that simulate realistic and challenging use cases and interactions in disasters, by tracking performance metrics such as loss, accuracy, and perplexity. The project will contribute to identifying and exploring new use cases for conversational AI in critical infrastructure resilience, and the outcomes will be disseminated via research publications.</p>\n\n\n\n\n\n<p><strong>Northeastern University</strong>: Samuel Scarpino (PI)</p>\n\n\n\n<p>This proposal sets out the goal of developing AI tools for pandemic risk assessment. This is to be achieved through a partnership between the CAPTRS and the IEAI at Northeastern University. By using data from ProMED and the WHO Disease Outbreak Network, the team aims to fine-tune two distinct models &#8211; OpenAI\u2019s GPT-3 text-davinci-003 and the open-source model Llama-2. The final outcome of this project would be models capable of generating early-stage outbreak alerts along with an associated risk score.</p>\n\n\n\n\n\n<p><strong>University of Cambridge</strong>: Mihaela van Der Schaar (PI)</p>\n\n\n\n<p>Develop and evaluate a hallucination detection system for medical text generation. By detecting and mitigating hallucinations in AI generated text, we aim to enhance patient safety and improve the quality of medical care. Developing a hallucination detection system can improve the safety and quality of AI generated medical text. Furthermore, the insights gained from this research will contribute to the broader understanding of responsible AI deployment in healthcare and help develop best practices for the ethical use of AI in medicine.</p>\n\n\n\n\n\n<p><strong>Emory University</strong>: Carl Yang (PI)</p>\n\n\n\n<p>The proposed research aims to explore the application of foundation models, specifically GPTs, to advance health outcomes research. This is achieved by focusing on their alignment with patient values, dealing with social determinants of health, and their capabilities in advancing health sciences and healthcare.</p>\n\n\n\n\n\n<p><strong>University of Berlin</strong>: Matthias Groeschel (PI)</p>\n\n\n\n<p>The research project aims at evaluating the feasibility of GPT-4\u2019s ability to compare physicians\u2019 diagnostic and therapeutic choices to national guidelines. The goal is to provide in hospital access to local and national guidelines for two chronic pulmonary diseases, and assessing and defining the prompt strategy for comparison between national guidelines and patient treatment.</p>\n\n\n\n\n\n<p><strong>Carnegie Mellon University</strong>: Tom M. Mitchell (PI)</p>\n\n\n\n<p>We propose research applying GPT models to improve the quality of teaching in online education platforms, and request Azure access to GPT to support this research. As a case study of this problem, we will partner with the widely used K-12 education platform freely available at the non-profit www.ck12.org website, which has served over 100 million unique student visitors worldwide.</p>\n\n\n\n<p><strong>Related papers:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://gaied.org/neurips2023/files/38/38_paper.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring Systems<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.cs.cmu.edu/~tom/pubs/Learning_to_Compare_Hints_2024.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">Learning to Compare Hints: Combining Insights from Student Logs and Large Language Models<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of Illinois Urbana-Champaign</strong>: Hari Sundaram (PI)</p>\n\n\n\n<p>The proposal is aimed at developing a framework using LLMs to simplify the understanding of online Terms of Service and privacy notices, making them accessible to people with little or no legal understanding. The research will focus on making these contracts understandable in fifth-grade English and analyse alignment with user values on four privacy-related dimensions.</p>\n\n\n\n\n\n<p><strong>KAIST</strong>: Alice Oh (PI)</p>\n\n\n\n<p>The integration of ChatGPT in the field of education has garnered significant interest, offering an opportunity to examine its effectiveness in English as a foreign language (EFL) education. A novel learning platform, RECIPE, collects students\u2019 interaction data with ChatGPT by guiding students and ChatGPT prompting. The goal is to investigate students\u2019 usage and perception of generative AI and explore how students can effectively utilize generative AI in EFL writing education.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://gaied.org/neurips2023/files/19/19_paper.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">Exploring Student-ChatGPT Dialogue in EFL Writing Education<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>San Diego State University</strong>: Hossein Shirazi (PI)</p>\n\n\n\n<p>In the U.S. knowledge-based sector, 10% of new hires (around 2 million employees) face recurrent challenges, partly from a flawed mentorship system where less than 40% of employees receive mentorship. Overwhelmed mentors hinder productivity, resulting in inefficiencies and wasted resources. To address this, employees turn to Large Language Model (LLM)-based chatbots, like ChatGPT, for career advice, though their reliability and personalization remain concerns. This research explores how LLMs, accessible through Azure and OpenAI Services, can enhance employee experiences, focusing on AI-driven professional mentorship. We investigate benefits, challenges, and strategies to optimize mentorship using advanced technologies. Our methodology comprises three phases: data collection through interviews and platform analysis, LLM utilization for refining AI responses, and developing a Retrieval-Augmented Generation (RAG) chatbot to assess AI mentor effects across industries. Empirical data collection involves interviews with employees, managers, and mentors, supplemented by platform data, creating a robust dataset. We employ LLMs to answer common queries, iteratively refining responses for accuracy and emotional dynamics.</p>\n\n\n\n<p>This project&#8217;s potential impact on Employee Experience Management and Human Resource Management is substantial, promising to enrich mentorship experiences and improve HR processes through AI innovation.</p>\n\n\n\n\n\n<p><strong>Georgia Institute of Technology</strong>: Munmun De Choudhury (PI)</p>\n\n\n\n<p>In collaboration with mental health clinicians from Northwell Health, we explore the question of where LLM-based chatbots may be useful for mental health contexts, and where they may be harmful. We conduct audits of how LLM-based chatbots (accessed via the Azure OpenAI Service) respond to pregenerated queries seeking support, and how responses from chatbots compare to how peer supporters might answer the query. We identify where chatbots provide credible mental health information and support, and where they may provide poor advice or propagate misinformation. This work contributes a beginning framework around the harms of generative AI for mental health, including methods for studying generative AI in mental health and ethical considerations.</p>\n\n\n\n\n\n<p><strong>Emory University</strong>: Craig Jabaley (PI)</p>\n\n\n\n<p>The research proposal aims to evaluate the proficiency of LLMs in extracting and understanding clinical concepts from routine clinical documentation in adult critical care. The process involves comparing LLM outputs against human-annotated clinical notes. The study seeks to understand the capabilities, strengths, and limitations of LLMs in the realm of adult critical care.</p>\n\n\n\n\n\n<p><strong>Emory University</strong>: Pedram Rooshenas (PI)</p>\n\n\n\n<p>This project aims to develop an AI-based teaching assistant by leveraging large language models (LLMs). Through precise fine-tuning and strategic prompting, this system will be capable of offering constructive feedback to students and responding to their course-specific queries. Moreover, by incorporating feedback from human educators, we steer the LLMs to produce responses that exemplify the thought process essential for mastering each concept in the course. We are going to pilot our system for Database Systems, a core course in the Computer Science program. \nOur proposed system has the potential to enhance the learning experience in public universities, particularly in light of the significant rise in enrollments for computer science and data science programs.</p>\n\n\n\n\n\n<p><strong>University of Texas at Austin</strong>: Desmond Ong (PI)</p>\n\n\n\n<p>*<a href=\"https://www.microsoft.com/en-us/research/collaboration/ai-cognition-and-the-economy-aice/overview/\" target=\"_blank\" rel=\"noreferrer noopener\"><em>AICE Accelerator collaboration</em></a></p>\n\n\n\n<p>The Digital Empathy pilot aims to investigate emotional intelligence in Large Foundation Model (LFM) -driven systems and to develop and study a series of empathic AI agents to understand and augment human performance and wellbeing. Until now, there has been very little empirical evidence of how empathic LFM systems are or the psychological implications of these systems during human-AI interactions. The project will contribute to a comprehensive survey of the research opportunities and priorities concerning empathy in AI systems and a research platform for the systematic evaluation of empathic agents.</p>\n\n\n\n<p><strong>Related papers:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2403.18148\" target=\"_blank\" rel=\"noreferrer noopener\">Large Language Models Produce Responses Perceived to be Empathic<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2404.01288\" target=\"_blank\" rel=\"noreferrer noopener\">Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>Carnegie Mellon University</strong>: Zachary Lipton (PI)</p>\n\n\n\n<p>The proposal aims to study the statistical and algorithmic foundation of training and applying LLMs in a human-centered manner. It consists of two research projects:</p>\n\n\n\n\n\n<p><strong>University of Toronto</strong>: Anastasia Kuzminykh (PI)</p>\n\n\n\n<p>We investigate the use of randomized A/B experiments to provide a more scientific basis for prompt engineering. We experimentally compare LLM prompts and user interfaces for LLMs to emulate teaching assistant behaviours: Providing feedback vs asking questions for students to explain to themselves; guiding students to effectively learn from an LLM; motivating students to engage in reflection vs passive learning. Field data from thousands of students will be used for qualitative and quantitative analysis of impact. In addition, we use reinforcement learning for statistically informed adaptive experiments, which automatically enhance, personalize, and contextualize prompts to different users, or the same user at different points in time. The techniques for adaptive experimentation in prompt engineering will be evaluated in domains besides education, from mental well-being chat to field experiments that integrate human-computer interaction, ML/AI, statistics, and psychology.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.13712\" target=\"_blank\" rel=\"noreferrer noopener\">Impact of Guidance and Interaction Strategies for LLM Use on Learner Performance and Perception<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Akshay Chaudhari (PI)</p>\n\n\n\n<p>We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization and other generative text tasks in healthcare. Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, or clinical text) and via discrete prompting or parameter-efficient fine-tuning. Our findings highlight the importance of domain adaptation for applying LLMs to healthcare and provide valuable insights toward developing effective natural language processing solutions for clinical tasks.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.nature.com/articles/s41591-024-02855-5\" target=\"_blank\" rel=\"noreferrer noopener\">Adapted large language models can outperform medical experts in clinical text summarization<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of Washington</strong>: Linda Shapiro (PI)</p>\n\n\n\n<p>This project serves three synergistic projects aimed at advancing medical AI: Quilt-Medical for curating a new multimodal multi-domain dataset of medical concepts; Quilt-Instruct, which uses the curated data to develop a medical chatbot for histopathology education; and MediEval, which evaluates the performance and diagnosis capability of these AI models, highlighting potential unexplored gaps in medical AI. Therefore, we aim to&nbsp;advance&nbsp;AI research in the healthcare sector by enhancing multi-modal chatbot training, data curation, and model evaluation.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://quilt-llava.github.io/\" target=\"_blank\" rel=\"noreferrer noopener\">Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of Southern California</strong>: Mahdi Soltanolkotabi (PI)</p>\n\n\n\n<p>The proposal aims to leverage Large Language Models (LLMs) in improving the interpretation and reporting of MRI scans. The approach involves creating a multi-modal dataset consisting of images paired with captions and question-answer pairs and fine-tuning architectures for accuracy and increasing their reliability using conformal prediction. The expected outcome is an improved system of providing reliable interpretations of medical images and assisting in medical report generation and decision making. This system could also potentially provide a &#8216;second opinion&#8217; for radiologists, reducing the risk of misdiagnosis and decreasing healthcare expenses.</p>\n\n\n\n\n\n<p><strong>Florida International University</strong>: Christian Poellabauer (PI)</p>\n\n\n\n<p>Medication errors most commonly occur at the ordering or prescribing stage, potentially leading to medical complications and poor health outcomes. While it is possible to catch these errors using different techniques; the focus of our work is on textual and contextual analysis of prescription information to detect and prevent potential medication errors.</p>\n\n\n\n<p>In previous work, we demonstrated how to use contextual language models (based on BERT) to detect anomalies in written or spoken text based on a data set extracted from real-world medical data of thousands of patient records. The resulting models are able to learn patterns of text dependency and predict erroneous output based on contextual information such as patient data, with an experimental accuracy of about 96% for text input and 79% for speech input. In the proposed project, we will extend this work to replace the contextual language model with a large language model to investigate how such a change will impact accuracy, especially for speech input, which will be increasingly important with the growing use of speech input in medical software and systems.</p>\n\n\n\n\n\n<p><strong>University College London</strong>: Clara Colombatto (PI)</p>\n\n\n\n<p>*<a href=\"https://www.microsoft.com/en-us/research/collaboration/ai-cognition-and-the-economy-aice/overview/\" target=\"_blank\" rel=\"noreferrer noopener\"><em>AICE Accelerator collaboration</em></a></p>\n\n\n\n<p>In this project, we propose to leverage insights from the psychology and neuroscience of metacognition and decision-making to study human-AI interactions and their potential for trustworthy collaboration. This past work has highlighted that successful collaboration hinges on sharing not just our cognitive states (e.g. what we believe), but also metacognitive estimates (e.g. our confidence in ourselves and one another). Humans routinely signal their metacognitive states explicitly (e.g., via verbal estimates) or implicitly (e.g., via speech prosody).</p>\n\n\n\n\n\n<p><strong>Emory University</strong>: Judy Gichoya (PI)</p>\n\n\n\n<p>Leverage access to the various versions of GPT to evaluate the accuracy and utility of LLMs for radiology report summarization. Our ultimate goal is to develop video and image integration (capable in GPT-4) to evaluate synthetically generated video and text combinations for radiology reports summarization.</p>\n\n\n\n\n\n<p><strong>New Jersey Institute of Technology</strong>: Salam Daher (PI)</p>\n\n\n\n<p>Before treating real patients, healthcare trainees use patient simulations to practice safely. Simulated patients range from physical (e.g. mannequin) to virtual (e.g. computer graphics). In simulation, patient responses can be controlled via a human in the loop or can be automated. Having a human in the loop is costly and sometimes suffers from scheduling limitations. The automation of patients\u00e2\u20ac\u2122 responses is the future of training; it allows healthcare students to practice certain skills that involve communication with the patient anytime anywhere. Digital Assistants (e.g. Microsoft\u00e2\u20ac\u2122s Cortana, Amazon Alexa, Apple Siri) have been used in various fields but their use in healthcare has been limited thus far. Large Language Models such as ChatGPT can provide text responses to any query, but those responses are not personalized to simulate healthcare scenarios such as a conversation with a patient. We propose to develop and test Automated Digital Assistants for Patient Tele-Simulation (ADAPTS) to simulate patients at any time anywhere. ADAPTS combines personalized responses (e.g., a specific healthcare scenario) with the versatility of digital assistants and OpenAi\u00e2\u20ac\u2122s Large Language Models to simulate a patient\u00e2\u20ac\u2122s responses, and to provide feedback for healthcare students after they interact with the simulated patient.</p>\n\n\n\n\n\n<p><strong>Carnegie Mellon University</strong>: Cleotilde Gonzalez (PI)</p>\n\n\n\n<p>The proposal describes a method to integrate foundation models with cognitive models to generate personalized education that can train users to identify and manage risks related to online scams or phishing activities. This will be done by running experiments with human participants using LLM-generated example emails, as well as natural language feedback of participant responses. The intention is to develop an understanding of human behavior and learning styles and to use this data to provide individualized educational feedback.</p>\n\n\n\n\n\n<p><strong>Harvard University</strong>: Junwei Lu (PI)</p>\n\n\n\n<p>Adapting large language models to the medical domain by aligning electronic health records (EHR) with NLP prompts and utilizing EHR data to fine-tune the models. The aim is to improve diagnosis reporting, handle disparities and biases in the data, and explore the potential of EHR data for generating deeper medical insights.</p>\n\n\n\n\n\n<p><strong>KAIST</strong>: Sangchul Park (PI)</p>\n\n\n\n<p>This study focuses on the application of a large language model (LLM) within the legal domain of trademarks. An essential aspect of all trademark procedures is assessing the likelihood of consumer confusion, which serves as a pivotal touchstone. However, the evaluation of trademark similarity, while being a decisive factor, has historically been characterized by its elusive and subjective nature. To introduce greater structure and consistency to this process, the proposed project aims to vectorize the judgments rendered by judges at the U.S. Trademark Trial and Appeal Board (TTAB) and subsequently develop a judgment prediction model. To achieve this, GPT will be employed not only for the fine-tuning of a model based on TTAB decisions but also to capture pertinent features such as semantic similarity between pairs of marks and the acquired distinctiveness (commonly known as secondary meaning) of generic marks. The amalgamation of GPT and the analysis of judge decisions seeks to enhance the precision and reliability of trademark evaluations in legal proceedings.</p>\n\n\n\n\n\n<p><strong>University of California, Berkeley</strong>: David Holtz (PI)</p>\n\n\n\n<p>This research study explores the micro-level interactions between human users and generative AI, aiming to provide rigorous evidence on &#8216;prompt engineering&#8217;. The goal is to understand the optimal use of foundation models in a variety of domains using robust quantitative research. The researchers will conduct large scale online experiments to explore, among other things, individual aptitude for prompt engineering, learning dynamics, and generative AI&#8217;s integration into information work.</p>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Curtis Langlotz (PI)</p>\n\n\n\n<p>RadGPT is a system that uses artificial intelligence to generate simple and personalized explanations of radiology reports for patients. It extracts key concepts from the reports and uses GPT-4 to create prompts and responses that describe them in plain language. The system also allows patients to ask follow-up questions and provides hyperlinks to more information. The system will be tested by radiologists and radiology trainees, and will be integrated with Stanford Health Care\u2019s myHealth App. The goal of RadGPT is to empower patients with a better understanding of their health data, and to improve patient engagement and health outcomes.</p>\n\n\n\n\n\n<p><strong>Fisk University</strong>: Sajid Hussain (PI)</p>\n\n\n\n<p>While Learning Management Systems (LMS) offer numerous benefits such as the collection of extensive data on student performance, they also come with certain limitations. One limitation is the potential for a lack of personalization in delivering sufficient real-time evaluation and feedback to students. Currently, most of the existing LMS are focusing on improving the existing functionality and technology rather than focusing on the students&#8217; learning perspective [3]. LMS platforms often rely on standardized formats and can only provide numeric assessments, making it challenging to provide personalized and verbal evaluation and feedback, and tailor the educational content to individual student needs. Balancing the advantages of LMS with considerations for personalization is crucial for addressing these limitations effectively.</p>\n\n\n\n<p>The proposal involves leveraging advanced language models like GPT-4 or Orca 2 to create a real-time performance evaluation and feedback system for students. The comprehensive project comprises the following key components: Course Design, Data Collection, System Building, Evaluation and Feedback Generation.</p>\n\n\n\n<p>We will create a real-time evaluation and feedback system for students using Microsoft Azure Foundation Models.</p>\n\n\n\n\n\n<p><strong>UT Southwestern Medical Center</strong>: Andrew Jamieson (PI)</p>\n\n\n\n<p>The project aims to establish and evaluate the effectiveness of AI-generated feedback on medical students&#8217; post-encounter notes. Leveraging a dataset of 15,000 existing examples, the project plans to refine and evaluate feedback using metrics like factuality, fidelity, helpfulness, and actionability, ultimately aiming to improve feedback quality and timeliness.</p>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Dan Jurafsky (PI)</p>\n\n\n\n<p>The proposal aims to realign generated language from AI models with human needs, focusing on how uncertainty is expressed. It plans to conduct a three-part study: analyzing the linguistic miscalibration of language models, creating a dataset of transcribed human decision-making conversations, and training foundation models with artificially calibrated datasets.</p>\n\n\n\n\n\n<p><strong>University of Oxford</strong>: Scott Hale (PI)</p>\n\n\n\n<p>This proposal aims to align large language models (LLMs) with the diverse values and preferences of global users. It plans to build a large-scale, personalized, and diverse dataset of human feedback over LLM interactions with individuals from over 35 different countries, rating the responses of more than 20 commercial and open-source models. This rich dataset can be used for developing LLM that are more socio-culturally aware. The Microsoft support will enable the evaluation and training of models at scale, allowing more experiments, iterations, and refinements.</p>\n\n\n\n\n\n<p><strong>MIT</strong>: Mert Demirer (PI)</p>\n\n\n\n<p>*<a href=\"https://www.microsoft.com/en-us/research/collaboration/ai-cognition-and-the-economy-aice/overview/\" target=\"_blank\" rel=\"noreferrer noopener\"><em>AICE Accelerator collaboration</em></a></p>\n\n\n\n<p>As the adoption of generative AI tools becomes more widespread, it is crucial to anticipate the macroeconomic effects on labor and production. This requires both a whole-market view and a detailed accounting of the differences between jobs. We will approach this challenge by treating jobs as interconnected sequences of tasks that vary in how easily they can be automated and overseen. This results in some jobs being &#8220;more automatable&#8221; than others &#8212; even accounting for the level of skill required to complete the job manually &#8212; and suggests jobs where human-AI collaboration might be especially useful. We will use these models to study the general equilibrium impact of advances in AI automation across different job domains.</p>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Russ Altman (PI)</p>\n\n\n\n<p>The research focuses on using social media text data to track and understand the opioid epidemic. It includes two subprojects. The first subproject is a review of a broad set of social media platforms for their utility in analyzing opiate-related discussions. Because social media text is rife with slang and intentionally misspelled words for evasion of censorship (\u201calgospeak\u201d), knowledge of these nonstandard terms for illicit drugs is needed in order to quantify the amount of opiate-related discussion present on a particular social media platform. This subproject uses large language models (LLMs) and the Google API for algospeak generation. The expected outcome is a systematic assessment of volume of opiate-related discussion across different social media platforms. The second subproject is an analysis of sentiment regarding the opioid epidemic in different specific American cities. This subproject uses LLMs and Azure Open AI Embeddings for sentiment analysis, and the expected outcome is an understanding of how city-specific drug policies are reflected in citizen sentiment.</p>\n\n\n\n\n\n<p><strong>University of Oxford</strong>: Andrew Soltan (PI)</p>\n\n\n\n<p>This project will apply large language models to support decision making in cancer diagnosis and treatment, using real-world data from a digitally mature NHS Trust. We aim to (i) improve upon the quality of referrals made to specialist cancer multidisciplinary team meetings (MDTs) where treatment decisions are taken, (ii) predict outcomes of the meeting in advance, allowing factors that may delay decisions to be remedied before the meeting, and (iii) use multimodal models to generate MDT referrals from unreported imaging & pathology data. Our study aims to improve the patient experience and inter-professional communication, reduce delays within the clinical pathway, and offer a decision aid for low and middle income settings.</p>\n\n\n\n\n\n<p><strong>Savannah State University</strong>: Kai Shen (PI)</p>\n\n\n\n<p>This project seeks to advance healthcare by utilizing Large Language Models (LLMs) for the extraction of Social Determinants of Health (SDOH) from Electronic Health Records (EHR), focusing on improving care for individuals at risk of substance use disorder (SUD). Using the comprehensive All of Us (AOU) dataset, the project will assess and refine the use of GPT models for extracting SDOH, aiming to identify potential biases in these models and understand their impact on healthcare data interpretation across diverse demographics. The project involves two primary goals: 1) evaluating GPT model biases in extracting SDOH from EHR notes, ensuring equitable performance across race, ethnicity, and sex; 2) investigating the impact of adverse SDOH on SUD outcomes, employing advanced GPT models and statistical methods. The expected outcomes include a refined process for SDOH extraction using LLMs, an assessment of biases in LLM models, and insights into the impact of SDOH on SUD. This project is poised to enhance the understanding of SUD in minority communities, improve intervention strategies, and promote more equitable healthcare outcomes.</p>\n\n\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\"/>\n\n\n\n<p>*<a href=\"https://www.microsoft.com/en-us/research/collaboration/ai-cognition-and-the-economy-aice/overview/\"><em>A</em></a><a href=\"https://www.microsoft.com/en-us/research/collaboration/ai-cognition-and-the-economy-aice/overview/\" target=\"_blank\" rel=\"noreferrer noopener\"><em>ICE Accelerator collaboration</em></a></p>\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models. which increases trust, human ingenuity, creativity, and productivity, and decreases the digital divide while reducing the risks of developing AI which does not benefit individuals [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 995550,
        "date": "2024-01-05T08:03:59",
        "slug": "afmr-responsible-ai",
        "title": "AFMR: Responsible AI",
        "link": "https://www.microsoft.com/en-us/research/project/afmr-responsible-ai/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-page-header_1920x720-6594371926460.png\" class=\"attachment-full size-full\" alt=\"white icon of microchip surrounded by two hands on a green gradient background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-page-header_1920x720-6594371926460.png 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-page-header_1920x720-6594371926460-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-page-header_1920x720-6594371926460-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-page-header_1920x720-6594371926460-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-page-header_1920x720-6594371926460-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-page-header_1920x720-6594371926460-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-page-header_1920x720-6594371926460-240x90.png 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"Accelerating Foundation Models Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAccelerating Foundation Models Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"responsible-ai\">Responsible AI</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\">\n<p><strong><em>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models.</em></strong></p>\n<cite>Brad Smith, Vice Chair and President</cite></blockquote>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-95 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n\n\n\n<div class=\"wp-block-column is-style-default is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:50%\">\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"400\" height=\"400\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL.png\" alt=\"green icon of a person standing on a circle with four smaller circles connected\" class=\"wp-image-996477\" style=\"width:auto;height:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL.png 400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-300x300.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-150x150.png 150w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-180x180.png 180w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-360x360.png 360w\" sizes=\"(max-width: 400px) 100vw, 400px\" /></figure>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center h4\" id=\"afmr-goal-align-ai-with-shared-human-goals-values-and-preferences-via-research-on-models\">AFMR Goal: Align AI with shared human goals, values, and preferences via research on models</h2>\n\n\n\n<p class=\"has-text-align-center\">which enhances safety, robustness, sustainability, responsibility, and transparency, while ensuring rapid progress can be measured via new evaluation methods</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p>These projects aim to make AI more responsible by focusing on safety, preventing misinformation, and improving auditing in a way that&#8217;s easy to understand. They look into protecting against harmful attacks and inappropriate responses, using feedback and fact-checking to combat misinformation, and incorporating logical reasoning for better auditing. The plans also address the safety of personalized AI models, reducing bias by involving multiple perspectives, and creating a thorough evaluation system for responsible AI. The methods involve comparing different approaches, using fact-checking, integrating reasoning into the framework, involving human collaboration, and comparing benchmark data. The expected outcomes include better defenses against certain types of attacks, improved accuracy in information, safer personalized AI models, unbiased solutions, and an evolving evaluation system for responsible AI.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<p><strong>Alabama A&M University</strong>: Xiang (Susie) Zhao (PI)</p>\n\n\n\n<p>Environmental justice analysis fosters the fair treatment and involvement of all people, regardless of race, color, national origin, or income, in economic development and sustainability, resource allocation, environment protection, etc. Especially, it plays a critical role in intelligent disaster recovery and city planning which saves lives, assets, and energy. Many government agencies including NASA, NOAA, CDC, EPA provide full and open access to their datasets, which can be used to support environmental justice research and identify vulnerable populations and environmental challenges. However, it is difficult for researchers and students at HBCUs/MSIs to understand and use these datasets due to various or complex data formats, limited computing resources and heavy workload. This project aims to bridge this gap and strengthen the research and education capabilities at HBCUs/MSIs using Microsoft foundational models and Azure cloud platform. Azure OpenAI GPT-4 and DALL-E 2 will be used for natural language processing to survey and process scientific literature, government reports and blogs related to environmental justice, disaster recovery and city planning. A RA-Bot will be developed to assist the researchers and decision makers to answer inquires, generate summaries, and perform classification and sentiment analysis.</p>\n\n\n\n\n\n<p><strong>Monash University Malaysia</strong>: Sailaja Rajanala (PI)</p>\n\n\n\n<p>The proposal aims to enhance auditing of large language models (LLMs) by integrating causal and logical reasoning into the Selection-Inference (SI) framework, offering a deeper understanding of how LLMs function and make decisions. It looks to identify and mitigate biases, and ensure LLM-generated content is ethically compliant. The research also seeks to create auditing pipelines that could be transferred to other AI systems.</p>\n\n\n\n\n\n<p><strong>University of Texas at Arlington</strong>: Faysal Hossain Shezan (PI)</p>\n\n\n\n<p>The prevalence of vulnerable code poses a significant threat to software security, allowing attackers to exploit weaknesses and compromise systems. Traditional methods of manual vulnerability detection are expensive, requiring substantial domain expertise. Automated approaches, particularly those based on program analysis techniques like symbolic execution, have shown promise but face challenges in path convergence, scalability, accuracy, and handling complex language features. We propose to introduce a hybrid approach that combines a large language model (LLM), such as GPT-4, with a state-of-the-art symbolic execution tool like KLEE. Our approach aims to enhance symbolic execution by mitigating its inherent challenges. The strategy involves dynamically prioritizing execution paths based on contextual relevance and potential vulnerability disclosure. The LLM will guide symbolic execution towards paths likely to yield significant outcomes, adapting strategies based on evolving context and analysis information. Additionally, we will incorporate semantic information from the LLM to generate more meaningful constraints, reducing the complexity of constraints and directing symbolic execution towards pertinent paths.</p>\n\n\n\n\n\n<p><strong>Kean University</strong>: Yulia Kumar (PI)</p>\n\n\n\n<p>The research delves into the robustness of Large Language Models (LLMs) such as GPT-4-Turbo and Microsoft Copilot, augmented with tools like DALLE-3, against adversarial attacks in multimodal contexts that merge text and imagery. The objective is to unearth vulnerabilities in these advanced LLMs when they interpret manipulated textual and visual stimuli. Our approach involves the creation of adversarial test cases featuring subtle textual modifications and visually altered images, based on the orthogonal array coverage of most likely attack scenarios to AI models. These are designed to test the LLMs&#8217; capacity to process and react to multimodal data in misleading scenarios while examining the underlying Transformer architecture and self-attention mechanisms, if accessible. The study scrutinizes the models&#8217; vulnerability to both isolated and simultaneous cross-modal attacks, seeking to expose potential shortcomings in their ability to handle multimodal information and any biases in their outputs. Anticipated outcomes include valuable insights into AI&#8217;s resilience against sophisticated adversarial tactics, enhancing multimodal AI systems. This research is crucial for AI security, emphasizing the need to bolster the accuracy and dependability of AI across various applications. It aims to contribute to developing robust and secure AI systems, effectively navigating the complexities of an increasingly multimodal digital environment.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.preprints.org/manuscript/202401.1053/v1\" target=\"_blank\" rel=\"noreferrer noopener\">Robust Testing of AI Language Models Resilience with Novel Adversarial Prompts<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>IIT Kharagpur</strong>: Somak Aditya (PI)</p>\n\n\n\n<p>Explore both OpenAI and similar open-source models (such as FLAN-T5, LLAMA) to draw out comparisons of the effect of \u201cjailbreaks\u201d and their mitigation. Our goal for Foundation Models Academic Research is two folds: 1) analysis, categorization, and defense of prompt injection attacks, and 2) safeguarding against unethical, hateful, or adult responses. We provide a brief description of the two streams of the proposed research.</p>\n\n\n\n<p><strong>Related papers:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.12860\" target=\"_blank\" rel=\"noreferrer noopener\">Probing LLMs for hate speech detection: strengths and vulnerabilities<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2401.10065\" target=\"_blank\" rel=\"noreferrer noopener\">Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of North Texas</strong>: Yunhe Feng (PI)</p>\n\n\n\n<p>In light of increasing concerns over demographic biases in image-centric AI applications, this proposal introduces RAG-PreciseDebias, a novel framework designed to address these biases in image generation. Our approach integrates fine-tuned Large Language Models (LLMs) with text-to-image generative models within a retrieval augmented prompt generation architecture. This system autonomously refines generic text prompts to align with specified demographic distributions, as informed by an information retrieval system. This proposal builds upon prior methodologies in prompt engineering and model bias assessment, addressing the limitations of existing approaches that either rearrange existing images or require manual demographic specifications. RAG-PreciseDebias distinguishes itself by its capability to automatically provide contextually relevant demographic data, thereby improving the precision and adaptability of image generation. Our novel instruction-following LLM is central to this framework, designed to adapt prompts to reflect specific demographic groups at predetermined rates, thus guiding the biased image generation model towards more representative outputs. RAG-PreciseDebias leverages data from reliable sources, including the U.S. Bureau of Labor Statistics and the United Nations, to generate images that are not only effective but also fair and representative of diverse populations, marking an advancement in responsible AI development.</p>\n\n\n\n\n\n<p><strong>New York University</strong>: Mengye Ren (PI)</p>\n\n\n\n<p>In an era where Large Language Models (LLMs) are becoming integral to various applications, their safety and alignment with human values are paramount. LLMs have demonstrated remarkable progress in recent years, exhibiting unprecedented capabilities in understanding and generating natural language text. As chatbots and other applications increasingly adopt LLMs, there is a growing trend towards more personalized and customized models. Our project proposes to study the critical question: Is it safe to allow continuous and personalized finetuning on LLMs without compromising its previous values, such as learning biasedness, toxicity, and harmfulness? We hypothesize that a safety review process after custom finetuning can mitigate the risks associated with personalizing LLMs. We also propose to study several learning mechanisms for the sequential personalization and safety review procedure.</p>\n\n\n\n<p><strong>Related papers:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2312.12736\" target=\"_blank\" rel=\"noreferrer noopener\">Learning and Forgetting Unsafe Examples in Large Language Models<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2312.05269\" target=\"_blank\" rel=\"noreferrer noopener\">LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2403.15362\" target=\"_blank\" rel=\"noreferrer noopener\">CoLLEGe: Concept Embedding Generation for Large Language Models<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>Pennsylvania State University</strong>: Qingyun Wu (PI)</p>\n\n\n\n<p>The proposal aims to mitigate potential biases in solutions/decisions generated by LLM-based AI systems through human-in-the-loop, multi-agent collaboration. The project will explore effective ways to integrate additional agents into the AI system, enabling it to overcome inherent shortcomings, including blind spots and biases, while leveraging the strengths of each individual agent. The goal is to perform bias mitigation during inference and to fine-tune the model using data from multi-agent collaboration.</p>\n\n\n\n\n\n<p><strong>Illinois Institute of Technology</strong>: Kai Shu (PI)</p>\n\n\n\n<p>The project proposes to address the factuality issues in large language models (LLMs) that may result in hallucination and misinformation. It aims to leverage knowledge-grounded feedback to enhance the factuality of LLMs and investigates the fragility of LLMs&#8217; factuality property via factuality attacking methods and corresponding defense approaches.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.05253\" target=\"_blank\" rel=\"noreferrer noopener\">Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>Singapore University of Technology and Design</strong>: Soujanya Poria (PI)</p>\n\n\n\n<p>Address the challenge of ensuring safety and responsibility in foundational models. Our team proposes evaluating and comparing original models with safer versions on benchmark datasets, including a safety benchmark, to assess their performance and potential trade-offs. Through this evaluation, we can gain a comprehensive understanding of the performance of the foundational models and identify any potential trade-offs between safety, responsibility, and generalization. By doing so, we aim to establish a framework for evaluating the safety and responsibility of foundational models, which can pave the way for future advancements in this area.</p>\n\n\n\n<p><strong>Related papers:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2401.09395\" target=\"_blank\" rel=\"noreferrer noopener\">Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs&#8217; Mathematical and Coding Competency through Ontology-guided Interventions<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2311.08355\" target=\"_blank\" rel=\"noreferrer noopener\">Mustango: Toward Controllable Text-to-Music Generation<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.14303\" target=\"_blank\" rel=\"noreferrer noopener\">Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2305.13269\" target=\"_blank\" rel=\"noreferrer noopener\">Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2308.09662\" target=\"_blank\" rel=\"noreferrer noopener\">Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of North Texas</strong>: Tao Wang (PI)</p>\n\n\n\n<p>The growth amount of connected mobile devices urges a more efficient and effective resource management to improve spectrum efficiency and accommodate various user requirements in the next generation broadband wireless access networks. A recent trend in resource allocation is to incorporate sophisticated neural networks for decision making to improve the efficiency of the system. Nevertheless, current practices of resource allocation for emerging wireless techniques are typically implemented as black-box strategies in commercial products, which lack model interpretability and transparency, and may yield inscrutable predictions and biased decisions.</p>\n\n\n\n<p>The project aims at developing an AI-empowered automation toolkit that can systematically examine the potential risks of resource allocation schemes for emerging networking techniques and further developing effective countermeasures. Specifically, the PIs aim to accomplish the following three tasks:</p>\n\n\n\n<ol>\n<li>Developing a resource allocation strategy inspector to understand the internal workings of different black-box resource allocation strategies and improve their explainability and transparency.</li>\n\n\n\n<li>Uncovering potential attack surfaces and developing corresponding countermeasures.</li>\n</ol>\n\n\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models. which enhances safety, robustness, sustainability, responsibility, and transparency, while ensuring rapid progress can be measured via new evaluation methods These projects aim to make AI [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 995544,
        "date": "2024-01-05T08:03:11",
        "slug": "afmr-multimodal-and-crossmodal-learning",
        "title": "AFMR: Multimodal and Crossmodal Learning",
        "link": "https://www.microsoft.com/en-us/research/project/afmr-multimodal-and-crossmodal-learning/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multimodal-and-Crossmodal-Learning-page-header_1920x720.png\" class=\"attachment-full size-full\" alt=\"white circle icons connected by lines on a green gradient background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multimodal-and-Crossmodal-Learning-page-header_1920x720.png 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multimodal-and-Crossmodal-Learning-page-header_1920x720-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multimodal-and-Crossmodal-Learning-page-header_1920x720-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multimodal-and-Crossmodal-Learning-page-header_1920x720-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multimodal-and-Crossmodal-Learning-page-header_1920x720-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multimodal-and-Crossmodal-Learning-page-header_1920x720-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Multimodal-and-Crossmodal-Learning-page-header_1920x720-240x90.png 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"Accelerating Foundation Models Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAccelerating Foundation Models Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"multimodal-and-crossmodal-learning\">Multimodal and Crossmodal Learning</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\">\n<p><strong><em>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models.</em></strong></p>\n<cite>Brad Smith, Vice Chair and President</cite></blockquote>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-96 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n\n\n\n<div class=\"wp-block-column is-style-default is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:50%\">\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"400\" height=\"400\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL.png\" alt=\"green icon of a person standing on a circle with four smaller circles connected\" class=\"wp-image-996477\" style=\"width:auto;height:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL.png 400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-300x300.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-150x150.png 150w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-180x180.png 180w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-360x360.png 360w\" sizes=\"(max-width: 400px) 100vw, 400px\" /></figure>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center h4\" id=\"afmr-goal-align-ai-with-shared-human-goals-values-and-preferences-via-research-on-models\">AFMR Goal: Align AI with shared human goals, values, and preferences via research on models</h2>\n\n\n\n<p class=\"has-text-align-center\">which enhances safety, robustness, sustainability, responsibility, and transparency, while ensuring rapid progress can be measured via new evaluation methods</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p>The research projects focus on improving and applying Multi-Modal foundation models in various ways. Some projects focus on foundational aspects, such as enhancing the efficiency of foundational vision and language models, training audio-visual foundation models for tasks like segmentation and localization, and curating multimodal video datasets, and aligning multi-modal vision-language foundation models to understand their capabilities and limitations. Others address its applicational aspects, such as advancing traffic monitoring, geospatial data interaction, and predicting human mobility using Multi-Modal foundation models, enhancing video-based foundation models for reasoning, and addressing demographic bias in image generation by balancing model bias. This comprehensive approach ensures a broad and deep exploration of multimodal models and their potential applications.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<p><strong>NC A&T State University</strong>: Leila Hashemi-Beni (PI) </p>\n\n\n\n<p>Effective traffic monitoring is critical for transportation agencies and city planners to understand traffic patterns, congestion, and safety hazards. This proposal outlines a project to develop an advanced AI-based traffic monitoring system. The system leverages Physics-Informed Neural Networks (PINN) to model traffic state and employs Generative Pre-trained Transformer (GPT) models to interpret user input and PINN model outputs. To train the PINN models, a high-resolution dataset collected by unmanned aerial vehicles (UAVs) will be utilized. The primary goal of this project is to create a highly accurate and efficient traffic monitoring system capable of identifying traffic states and computing traffic state parameters.</p>\n\n\n\n\n\n<p><strong>University of Wisconsin-Madison</strong>: Yong Jae Lee (PI)</p>\n\n\n\n<p>The proposal aims to advance research on aligning multi-modal vision-language foundation models by building and improving upon the Large Language and Vision Assistant (LLaVA) framework. The research project aims to study, enhance, and understand the capabilities, limitations, and emergent behaviors of these models. Questions such as how to evaluate foundation models, mitigate risks and harms, and ensure their fidelity will be explored.</p>\n\n\n\n\n\n<p><strong>University of Bath</strong>: Vinay Namboodiri (PI)</p>\n\n\n\n<p>A proposal to improve the efficiency and adaptation of foundational vision and language models through the development of spectral operator-based models and improved adapters. These advancements may lead to a wide range of applications including assistive technology.</p>\n\n\n\n\n\n<p><strong>York University</strong>: Mojgan Jadidi (PI)</p>\n\n\n\n<p>This proposal aims to transform the interaction and analysis of geospatial data by integrating OpenAI NLP models. It plans to create a user-friendly interface allowing natural language prompts to be turned into actionable geospatial queries. The output will be map-based visualisations generated by the system. The system&#8217;s adaptability will promote cross-sector collaboration and responsible AI use. The proof of concept will employ the City of Toronto open data platform.</p>\n\n\n\n\n\n<p><strong>University of Washington</strong>: Ali Farhadi (PI)</p>\n\n\n\n<p>This proposal focuses on enhancing multi-modal video datasets using foundation models in an automated pipeline at scale. We aim to explore various aspects of pre-training and post-pretraining alignment data and their effects on optimizing the performance of large-scale video models. Some of these aspects include domain coverage and diversity of the videos, quality of the paired texts, and the temporality aspect of actions in the videos. We also propose a paradigm shift from traditional video-text model training to an LLM-based domain adaptation tailored to the down-stream task and dataset.</p>\n\n\n\n\n\n<p><strong>University of California, Santa Cruz</strong>: Xin Wang (PI)</p>\n\n\n\n<p>The rapid evolution of Multimodal Large Language Models (MLLMs) has spurred significant interest in their application across various fields. However, a critical gap exists in their ability to generate coherent images alongside relevant texts. To close this gap, we introduce VilGen, a novel framework that effectively unifies vision and language modalities for interleaved multimodal generation.</p>\n\n\n\n<p>Central to VilGen is the concept of &#8220;generative vokens&#8221;, a novel mechanism that adeptly bridges LLMs and diffusion-based text-to-image generation models. This bridge is established by aligning generative vokens with latent visual features in diffusion models, enhancing the coherence and relevance of the multimodal output. Moreover, VilGen incorporates a retrieval-augmented approach, leveraging contextual image-text pairs to further refine and ensure the fidelity of generated content.</p>\n\n\n\n<p>Our model will be rigorously evaluated across a variety of benchmarks, focusing on multimodal storytelling and dialog, to validate its efficacy. We foresee that VilGen will outperform existing baselines in generating more consistent and faithful multimodal outputs, paving the way for next-generation MLLMs to adopt our methodologies for enhanced in-context learning and more robust multimodal generation capabilities.</p>\n\n\n\n\n\n<p><strong>University of North Carolina at Chapel Hill</strong>: Mohit Bansal (PI)</p>\n\n\n\n<p>Explore Multimodal Reasoning and Explanation Generation using Large Language Models (LLMs) such as GPT-3 and GPT-4. Our goal is to enable LLMs to generate visualizations and textual explanations that complement each other, improving understanding and trustworthiness of responses.</p>\n\n\n\n<p><strong>Related papers:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://videodirectorgpt.github.io/\" target=\"_blank\" rel=\"noreferrer noopener\">VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2309.13007\" target=\"_blank\" rel=\"noreferrer noopener\">ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://diagrammergpt.github.io/\" target=\"_blank\" rel=\"noreferrer noopener\">DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of North Texas</strong>: Yunhe Feng (PI)</p>\n\n\n\n<p>The proposal presents an end-to-end framework, PreciseDebias, that aims to rectify demographic bias in image generation from text prompts. The core component of PreciseDebias is a novel instruction-following Large Language Model (LLM) designed with an emphasis on assessing and balancing model bias. The framework transforms generic text prompts to produce images that reflect specified demographic distributions. The proposed method autonomously refines prompts to match demographic distributions and guides the biased image generation model towards more statistically representative demographic outputs.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://openaccess.thecvf.com/content/WACV2024/html/Clemmer_PreciseDebias_An_Automatic_Prompt_Engineering_Approach_for_Generative_AI_To_WACV_2024_paper.html\" target=\"_blank\" rel=\"noreferrer noopener\">PreciseDebias: An Automatic Prompt Engineering Approach for Generative AI To Mitigate Image Demographic Biases<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of Virginia</strong>: Sheng Li (PI)</p>\n\n\n\n<p>The proposal aims to enhance the theory-of-mind (ToM) capabilities of large language models (LLMs) towards understanding video scenes. The main objective is to enable video-based LLMs to reason about characters&#8217; mental states in dynamic scenes, answer questions focused on social and emotional reasoning, and retrieve the key moments of transformation in the mental states of actors. The researchers propose leveraging a new multimodal temporal graph neural network (TGNN) architecture, Evolutionary Theory of Mind (EToM), that models the temporal evolution of mental states synthesized from video frames, transcripts, and video captioning data.</p>\n\n\n\n\n\n<p><strong>University of Rochester</strong>: Zhiyao Duan (PI)</p>\n\n\n\n<p>The proposal aims to develop a novel approach to train an audio-visual foundation model that models fine-grained dependencies within and across modalities to benefit various challenging downstream tasks, such as audio-visual segmentation, localization, and source separation. The key idea is to apply the masked auto-encoder (MAE) self-supervised learning paradigm to a large amount of unlabeled audio-visual data. The proposal also introduces the development of innovative masking strategies and auxiliary contrastive objectives to improve the effectiveness of the model training.</p>\n\n\n\n\n\n<p><strong>University of California, Riverside</strong>: Amr Magdy (PI)</p>\n\n\n\n<p>Visual foundation models (VFMs) are transforming various domains but face challenges in deployment on edge devices due to their high computational demands and memory requirements. These models, including advanced computer vision models like ViT and Microsofts Visual ChatGPT, require a significant number of floating-point operations (FLOPs), far exceeding the capabilities of resource-limited edge devices. Despite the potential of edge devices to enhance AI application responsiveness, their limited processing power restricts the use of such sophisticated models. To address this, researchers are investigating several techniques to reduce the computational burden of VFMs while maintaining their output quality. These strategies aim to deploy large foundation models on edge devices, ushering in a new era of efficient AI-powered edge computing applications.This project focuses on using knowledge distillation to optimize VFMs for edge devices. Knowledge distillation involves transferring knowledge from a larger model to a smaller one, a concept similar to the approach used in Microsofts Orca2 model for language knowledge. Our research aims to enhance inference cameras with self-inference capabilities, enabling VFMs to support near-real-time processing for various applications. We highlight one use case in sustainable agriculture and briefly mention other areas that could benefit from this technology.</p>\n\n\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models. which enhances safety, robustness, sustainability, responsibility, and transparency, while ensuring rapid progress can be measured via new evaluation methods The research projects focus on improving [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 995529,
        "date": "2024-01-05T08:02:22",
        "slug": "afmr-benchmarks-evaluation-and-measurement",
        "title": "AFMR: Benchmarks, Evaluation and Measurement",
        "link": "https://www.microsoft.com/en-us/research/project/afmr-benchmarks-evaluation-and-measurement/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Benchmarks-Evaluation-and-Measurement-page-header_1920x720.png\" class=\"attachment-full size-full\" alt=\"white bar graph with a magnifying glass icon on a green gradient background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Benchmarks-Evaluation-and-Measurement-page-header_1920x720.png 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Benchmarks-Evaluation-and-Measurement-page-header_1920x720-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Benchmarks-Evaluation-and-Measurement-page-header_1920x720-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Benchmarks-Evaluation-and-Measurement-page-header_1920x720-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Benchmarks-Evaluation-and-Measurement-page-header_1920x720-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Benchmarks-Evaluation-and-Measurement-page-header_1920x720-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Benchmarks-Evaluation-and-Measurement-page-header_1920x720-240x90.png 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"Accelerating Foundation Models Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAccelerating Foundation Models Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"benchmarks-evaluation-and-measurement\">Benchmarks, Evaluation and Measurement</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\">\n<p><strong><em>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models.</em></strong></p>\n<cite>Brad Smith, Vice Chair and President</cite></blockquote>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-97 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n\n\n\n<div class=\"wp-block-column is-style-default is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:50%\">\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"400\" height=\"400\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL.png\" alt=\"green icon of a person standing on a circle with four smaller circles connected\" class=\"wp-image-996477\" style=\"width:auto;height:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL.png 400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-300x300.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-150x150.png 150w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-180x180.png 180w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-360x360.png 360w\" sizes=\"(max-width: 400px) 100vw, 400px\" /></figure>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center h4\" id=\"afmr-goal-align-ai-with-shared-human-goals-values-and-preferences-via-research-on-models\">AFMR Goal: Align AI with shared human goals, values, and preferences via research on models</h2>\n\n\n\n<p class=\"has-text-align-center\">which enhances safety, robustness, sustainability, responsibility, and transparency, while ensuring rapid progress can be measured via new evaluation methods</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p>Evaluating the functionality, efficiency, and reliability of language models is the main theme of this group of research projects. They cover various scenarios and applications, such as assessing the models\u2019 comprehension, processing, and generation of better responses, with topics such as uncertainty quantification, abstraction & reasoning, knowledge distillation, structured pruning, and skills-based framework; developing the models\u2019 instruction-following ability, task-agnostic distillation, and sequential planning skills.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Christopher R\u00e9 (PI)</p>\n\n\n\n<p>This proposal suggests utilizing a skills-based framework to understand how foundation models acquire different capabilities from training data. This framework will then be used to select and order data to improve the performance of these models. The central question of the research is how best to understand the properties of skills in terms of scaling, data, and model architecture in order to develop a skills-based training paradigm for foundation models.</p>\n\n\n\n\n\n<p><strong>University of California, Los Angeles</strong>: Hongjing Lu (PI)</p>\n\n\n\n<p>This research proposal aims to evaluate and improve the reasoning capacities of large-scale AI systems using a cognitive science approach. The project seeks to evaluate these systems in three domains. First, multimodal vision-and-text models (GPT-4) will be evaluated on a series of visual reasoning tasks, to assess the extent to which they can reason about objects and visual relations. Second, generative text-to-image models (Dall-E 3) will be evaluated in tasks that require relational and compositional image generation. Third, language models (GPT-3 and GPT-4) will be evaluated for their ability to perform physical reasoning and problem solving. Finally, in addition to evaluating these capacities, the project also seeks to improve the reasoning and planning abilities of these systems through the development of modular architectures. A significant aspect of this research lies in its dependency on Microsoft Azure services, including language models and generative text-to-image models, as well as its commitment to releasing open-source benchmarks associated with these projects.</p>\n\n\n\n\n\n<p><strong>University of California, Berkeley</strong>: Ion Stoica (PI)</p>\n\n\n\n<p>This proposal seeks to develop a scalable, automatic approach for evaluating Foundation Models, specifically LLMs, on open-ended tasks. It proposes using &#8216;judge&#8217; LLMs, like GPT-4, to assess the quality of AI model responses. It aims to conduct both controlled and crowd-sourced experiments to develop a comprehensive benchmark for evaluation.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2306.05685\" target=\"_blank\" rel=\"noreferrer noopener\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>Pennsylvania State University</strong>: Qingyun Wu (PI)</p>\n\n\n\n<p>In this project, we aim to collaborate with researchers from Microsoft Research on the topic of AI agent evaluation and benchmarking. Recognizing the limitations of current benchmarks in providing a holistic evaluation of agent-based systems, we aim to develop: (1) a comprehensive test suite for debugging and evaluating agent-based systems; (2) construct novel agent-centric benchmarking datasets, and relevant evaluation metrics; and (3) deliver an evaluation framework for AutoGen around (1) and (2), providing an in-depth understanding of AI agent-based systems.</p>\n\n\n\n\n\n<p><strong>University of Massachusetts Amherst</strong>: Chuang Gan (PI)</p>\n\n\n\n<p>The proposal introduces innovative techniques to make Large Language Models more efficient and reliable, tackling the problem of their high resource consumption and aligning them better with human values. Advancements include the development of a novel Sparse Transformer incorporating the Mixture of Experts (MoE) and the implementation of a Principle-Driven Self-Alignment approach that reduces the need for extensive human annotations</p>\n\n\n\n\n\n<p><strong>University of Darmstadt</strong>: Iryna Gurevych (PI)</p>\n\n\n\n<p>Gain a comprehensive understanding of foundation models by conducting a comparative analysis of models of various sizes and exploring their reasoning, creative potential, and adaptability to new tasks. Our team will focus on evaluating self-explanations and emergent capabilities, including reasoning and creativity, through comparative studies and the design of novel datasets.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2401.10065\" target=\"_blank\" rel=\"noreferrer noopener\">Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>Mohamed bin Zayed University of Artificial Intelligence</strong>: Monojit Choudhury (PI)</p>\n\n\n\n<p>This research aims to address the cultural bias and awareness of Large Language Models (LLMs), which have been shown to predominantly favor Western culture and values. Current probing techniques relying on blackbox prompting are sensitive to the prompt and are limited to the study of objectives and values across cultures, where data is readily available. This proposal presents two objectives to address these concerns: (1) the development of a systematic probing technique that utilizes the internal states of the models (if available) or constructs and analyzes large-scale response matrices; and (2) creation of large-scale datasets that enable LLMs probing for cultural &#8216;common ground&#8217; and &#8216;aboutness&#8217;. The research strives to evaluate various popular LLMs using developed techniques and datasets. The proposed outcomes of the project include a multicultural and multilingual dataset assessing cultural aspects, methods and tools for systematic cultural study, and a comprehensive report on cultural awareness and bias in popular LLMs.</p>\n\n\n\n\n\n<p><strong>University of Cambridge</strong>: Adrian Weller (PI)</p>\n\n\n\n<p>Humans are very effective in modeling functions intuitively such as predicting the trajectory of a ball. Furthermore, humans have good intuition for what types of function forms are common in the real world. Do Large Language Models (LLMs), that are trained with massive human knowledge, have similar abilities? In particular, can LLMs model real-world functions accurately without gradient-based learning, utilizing the prior knowledge learned from the internet? One conjecture is that LLMs can acquire the capability to simulate the output of such systems without going through explicit computations. This is akin to how humans can predict (albeit with error) the output of such systems when modeling simple functions. In this research, we design a new evaluation paradigm to evaluate the ability of LLMs to model functions like humans. This will help to advance the development and the application of LLMs.</p>\n\n\n\n<p><strong>Highlights:</strong> Using carefully designed evaluation methods, we validate that LLMs indeed possess strong intuitions about real-world functions and can apply this understanding to downstream tasks. Specifically, we discover that LLM can not only identify a wide range of patterns in the data but also leverage domain knowledge to model the underlying function, all without necessitating gradient-based learning or precise computations. In settings where domain knowledge is pivotal or the data is scarce, LLMs can even outperform commonly-used machine learning models. At the same time, we also identify several drawbacks of LLMs for function modelling tasks. Our research highlights both the potentials and limitations of LLMs for data science.</p>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Diyi Yang (PI)</p>\n\n\n\n<p>Large Language Models (LLMs) have recently achieved unprecedented performance across diverse tasks. By pinpointing gaps for improvements, evaluation becomes the bedrock that enhances the understanding of current models and ensures AI\u2019s continued progression. Efforts to evaluate LLMs have intensified significantly. Despite the proliferation of LLMs evaluations, current evaluation benchmarks face multiple fundamental challenges such as static datasets and data contamination issues. This work investigates two main research questions: How can we dynamically evaluate the performance of LLMs in diverse domains with different levels of complexities? How can we generate evaluation protocols on the fly to support such dynamic evaluation? Through multiple dynamic evaluation thrusts, this work aims to mitigate potential data contamination issues and provide a holistic, new methodology for dynamic evaluation.</p>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Sanmi Koyejo (PI)</p>\n\n\n\n<p>The proposal outlines a project aimed at enhancing the robustness of foundation models through innovative, human-centric comparative oversight mechanisms. It addresses the limitations of current human feedback mechanisms, which often rely on simplistic binary judgments. Instead, it proposes a more nuanced approach called the &#8220;oversight agreement mechanism.&#8221; This mechanism introduces an additional layer of human evaluators who assess the agreement between the original task outputs, allowing for richer, natural language assessments. The project will use resources such as foundation models (GPT-3 and GPT-4 on Azure) to generate samples for comparative assessment. The outcomes will be evaluated based on agreement rates and predictive power, with successful outcomes expected to improve the models&#8217; accuracy, fairness, and interpretability. Additionally, the project is anticipated to provide insights into the differences between human and model-generated oversight. The potential impact of this project extends to establishing a new paradigm for human-AI collaboration where AI systems are critiqued and refined based on meaningful human oversight, making them more responsive to human values.</p>\n\n\n\n\n\n<p><strong>Morehouse College</strong>: Kinnis Gosha (PI)</p>\n\n\n\n<p>With the advent of ChatGPT, modern companies (i.e. shareholders) are infatuated with the use of AI in order to maintain competitiveness, expand employee productivity, and increase profit margins. When it comes to analyzing worker performance, trustfulness and transparency in the AI system is critical. Instead of using AI solely to determine employee performance, human feedback can be integrated into algorithms in a way that reduces the presence of various biases that could plague an evaluation. The following proposal will outline the development of a hybrid framework for performance evaluation driven by artificial intelligence for college faculty. Findings from the study can be used in the development of best practices for hybrid artificial intelligence performance evaluation across multiple employment sectors.</p>\n\n\n\n\n\n<p><strong>University of California, Santa Cruz</strong>: Cihang Xie (PI)</p>\n\n\n\n<p>This research aims at understanding how reliably LLMs can follow given algorithmic procedures and the effects of adversarial or corrupted prompts on this capability. The project will design environments where AI agents perform multi-step planning and reasoning to accomplish objectives, allowing for a quantitative measure of the reliability of LLMs in real-world applications. The proposed research will also investigate factors impacting the LLMs ability to follow algorithmic procedures.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2402.09404\" target=\"_blank\" rel=\"noreferrer noopener\">AQA-Bench: An Interactive Benchmark for Evaluating LLMs&#8217; Sequential Reasoning Ability<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of Arizona</strong>: Eduardo Blanco (PI)</p>\n\n\n\n<p>This proposal focuses on devising new methods for evaluating foundation models that consider linguistic capabilities rather than specific task benchmarks. Key aspects will include considering the effect of negation and factuality, augmenting existing benchmarks with these linguistic phenomena, and considering alternative solutions.</p>\n\n\n\n\n\n<p><strong>Carnegie Mellon University</strong>: Yiming Yang (PI)</p>\n\n\n\n<p>This proposal presents a research plan to improve the functionality of large language models (LLMs) such as GPT-3.5 and GPT-4 by leveraging feedback mechanisms. The research team at Carnegie Mellon University will investigate techniques for making LLMs more responsive to feedback and train an augmented universal reward model to enhance the models&#8217; judgements. Expected outcomes of this research include more adaptable LLMs that can perform better in real-world scenarios and more aligned LLMs that are guided by a powerful reward model</p>\n\n\n\n\n\n<p><strong>Princeton University</strong>: Danqi Chen </p>\n\n\n\n<p>The proposal aims to enhance the capabilities of large language models (LLMs) by focusing on two key areas: efficient pre-training and evaluation of instruction-following models. The first project revolves around creating efficient foundation models through structured pruning. The goal is to derive smaller models from existing LLMs, saving computational resources. The second project aims to develop a robust, unbiased benchmark for assessing LLMs&#8217; instruction-following abilities, hence better aligning them with human goals. The proposal suggests that access to resources, such as Azure compute, OpenAI APIs, and Azure-hosted LLaMA family, would greatly aid the projects.</p>\n\n\n\n\n\n<p><strong>Cornell University</strong>: Matthew Wilkens (PI)</p>\n\n\n\n<p>This project seeks to evaluate foundation models such as GPT-4 on a complex task that requires specialized domain knowledge: identifying types of legal interpretation. A large dataset of legal interpretations will be used to explore the efficacy of these models. The project will involve comparing the performance of various foundation models in conjunction with prompts, including those excluding legal jargon and chain-of-thought prompts.</p>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Tatsunori Hashimoto (PI)</p>\n\n\n\n<p>The proposal focuses on studying and improving the alignment of Reinforcement Learning from Human Feedback (RLHF) methods using simulated human feedback with an emphasis on the usefulness of simulation-based feedback and aspects of robustness such as reward hacking and distribution shift</p>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Chelsea Finn (PI) </p>\n\n\n\n<p>The proposal centers on developing efficient distillation techniques for Foundation Models (FMs) to reduce their computational demands and broaden their applicability in scenarios with latency constraints. In this project, knowledge from large FMs will be transferred to smaller, more manageable models, making FMs more accessible.</p>\n\n\n\n\n\n<p><strong>Georgia Institute of Technology</strong>: Chao Zhang (PI)</p>\n\n\n\n<p>The proposal focuses on enhancing the reliability of Large Language Models (LLMs) by calibrating their responses&#8217; confidence and leveraging estimated uncertainty for better decision-making and efficient exploration of LLM agents. The researchers aim to utilize a tool-augmented multi-agent debate mechanism to calibrate LLMs\u2019 confidence and to harness uncertainty estimates for improving LLM agents&#8217; planning efficiency.</p>\n\n\n\n\n\n<p><strong>University of Michigan, Ann Arbor</strong>: Rada Mihalcea (PI)</p>\n\n\n\n<p>Large language models (LLMs) have been demonstrated to lead to impressive performance and have been adopted for numerous NLP tasks. However, many of the models currently available overly represent certain cultures, at the cost of under-representing others. This can result in \u201ccultural bias\u201d in these models, as they can lack familiarity with certain cultural groups. This limitation is especially problematic when it comes to cultural commonsense \u2014 the practical knowledge that is commonly shared among most people in a group. In this project, we plan to plan to pursue the following two main goals:</p>\n\n\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models. which enhances safety, robustness, sustainability, responsibility, and transparency, while ensuring rapid progress can be measured via new evaluation methods Evaluating the functionality, efficiency, and reliability [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 995493,
        "date": "2024-01-05T08:01:42",
        "slug": "afmr-model-advancement",
        "title": "AFMR: Model Advancement",
        "link": "https://www.microsoft.com/en-us/research/project/afmr-model-advancement/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Model-Advancement-page-header_1920x720-659436af6c626.png\" class=\"attachment-full size-full\" alt=\"white half gear icon with an arrow pointing up on a green gradient background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Model-Advancement-page-header_1920x720-659436af6c626.png 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Model-Advancement-page-header_1920x720-659436af6c626-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Model-Advancement-page-header_1920x720-659436af6c626-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Model-Advancement-page-header_1920x720-659436af6c626-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Model-Advancement-page-header_1920x720-659436af6c626-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Model-Advancement-page-header_1920x720-659436af6c626-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Model-Advancement-page-header_1920x720-659436af6c626-240x90.png 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"Accelerating Foundation Models Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tAccelerating Foundation Models Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"model-advancement\">Model Advancement</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\">\n<p><strong><em>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models.</em></strong></p>\n<cite>Brad Smith, Vice Chair and President</cite></blockquote>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-98 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n\n\n\n<div class=\"wp-block-column is-style-default is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:50%\">\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"400\" height=\"400\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL.png\" alt=\"green icon of a person standing on a circle with four smaller circles connected\" class=\"wp-image-996477\" style=\"width:auto;height:150px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL.png 400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-300x300.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-150x150.png 150w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-180x180.png 180w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Human-Goals-Model-Research_FINAL-360x360.png 360w\" sizes=\"(max-width: 400px) 100vw, 400px\" /></figure>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center h4\" id=\"afmr-goal-align-ai-with-shared-human-goals-values-and-preferences-via-research-on-models\">AFMR Goal: Align AI with shared human goals, values, and preferences via research on models</h2>\n\n\n\n<p class=\"has-text-align-center\">which enhances safety, robustness, sustainability, responsibility, and transparency, while ensuring rapid progress can be measured via new evaluation methods</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:25%\"></div>\n</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p>A common theme among these research projects revolves around improving the LLM&#8217;s alignment with human goals, addressing challenges like hallucinations, unfaithful information generation, lack of control, and improving their robustness, interpretability, and generalizability. Several proposals also emphasize enhancement of specific reasoning capabilities, like logical, commonsense, syntactic, inductive, abductive reasoning, and multi-document reasoning. Other specific advancements include enabling LLMs to reason about time-series data, collaborate amongst themselves, simulate public responses to projected AI actions, interact with external environments, etc. In terms of techniques, reinforcement learning, human feedback, retrieval-based methods, fine-tuning, model compression, task-oriented dialogue, and sequence decision-making is being explored for improving LLM&#8217;s performance and utility.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<p><strong>IIT Kharagpur</strong>: Pawan Goyal (PI)</p>\n\n\n\n<p>The research proposes a novel automated evaluation framework for natural language generation via large language models. The framework aims to overcome the limitations of existing evaluation metrics that fail to fully capture the nuances of LLM-generated content. It also seeks to mitigate biases such as positional, length, and self-enhancement biases that are often present in such models. The framework will undergo rigorous testing across diverse tasks, including summarization and mathematical reasoning. The research also aims to explore the performance of various LLMs and develop new metrics for evaluating their outputs.</p>\n\n\n\n\n\n<p><strong>Harvard University</strong>: Stefano Maria Iacus (PI)</p>\n\n\n\n<p>The proposal aims to explore the capabilities of Foundation Models to facilitate the discovery of research and research data. The aim is to enrich metadata and build semantic knowledge graphs to accelerate knowledge acquisition. OpenAI models and fine-tuned open source models such as Llama-2, Falcon or MPT will be utilized.</p>\n\n\n\n\n\n<p><strong>University of California, Riverside</strong>: Amr Magdy (PI)</p>\n\n\n\n<p>This project will investigate how to generate linguistic summaries in natural language from real-time millimeter-wave and sub-terahertz radar data. Beamforming and range/speed tracking capabilities of radar systems will be leveraged to enable human and environmental context perception. Radar datasets will be pre-processed, and useful features will be utilized as input to large-scale language models. The ultimate objective of this project is to enable the fusion of wireless sensing with natural language, revealing concealed patterns within unstructured radar signatures. Radars have already been used as sensing modality for vital signs detection and human behavior recognition. However, existing research on radar-based vital signs detection and human activity monitoring focuses on traditional machine learning models tailored to the application use case, which may require considerable expertise from the end user and cannot adapt to the constant changes in realistic scenarios. To tackle these challenges, large language models will be utilized to assist radar signal analysis. In this study, commercially available radar platforms will be employed to obtain data associated with typical human activities. Algorithms for signal processing will be developed to combine radar data with Generative Pre-Trained Transformer models, and insights into the strengths and weaknesses of existing foundation models will be garnered.</p>\n\n\n\n\n\n<p><strong>Carnegie Mellon University</strong>: Carlee Joe-Wong (PI)</p>\n\n\n\n<p>This proposal presents a novel approach for improving alignment of Foundation Language Models (LMs) to human goals. The focus is on creating an ensemble of LMs incorporating cost constraints, human feedback, and strategic utilization of different LMs. The team plans to employ online learning mechanisms, particularly reinforcement learning, to optimize this process. The approach will be validated using various datasets.</p>\n\n\n\n\n\n<p><strong>Tokyo Institute of Technology</strong>: Naoaki Okazaki (PI)</p>\n\n\n\n<p>Naoaki Okazaki proposes research to examine whether large language models (LLMs) can benefit from a more dynamic, interactive, and bidirectional learning process that mirrors human cognition. This approach intends to improve how LLMs understand and generate language, addressing limitations observed in current LLMs&#8217; performances in complex reasoning tasks. The research introduces a paradigm of using real-time, adaptive discussions between two LLMs for training, with one LLM serving as the &#8216;learner&#8217; while the other as a &#8216;discussion partner&#8217;.</p>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Michael Frank (PI)</p>\n\n\n\n<p>The proposal aims to bridge the gap between large language models (LLMs) and human learning. The project plans to enhance the efficiency and interpretability of LLMs, lowering their data and model size requirements, and shedding light on human cognitive models and efficient language acquisition capabilities. The research will mainly focus on improving the quality of data used for training LLMs, enhancing evaluation benchmarks for comprehensive language understanding and employing innovative techniques to bring the capabilities of LLMs closer to those of human children.</p>\n\n\n\n\n\n<p><strong>Seoul National University</strong>: Seung-won Hwang (PI)</p>\n\n\n\n<p>Study how large language models (LLMs) may interleave interaction with external environments, such as search engines, more effectively, toward reducing widely known LLM weaknesses such as hallucinations in answering questions. However, this requires teaching LLMs, how to interact with environments, and which external skills can be leveraged from them. Existing work can be categorized by human-in-the-loop adaptation, requiring heavy human feedbacks (HF). Our distinction is mining \u201cthe rationale for how to perform such goal better\u201d from the environment itself, to replace expensive HF.</p>\n\n\n\n\n\n<p><strong>Carnegie Mellon University</strong>: Graham Neubig (PI)</p>\n\n\n\n<p>The project aims to extend the use cases for large language models (LLMs) by developing smaller, deployable models. The team plans to improve their Prompt2Model framework with advancements in automatic data wrangling, multilingual distillation, and better dataset generation algorithms. This research could lead to significant advancements in the application and reach of LLMs.</p>\n\n\n\n\n\n<p><strong>University of Maryland</strong>: Furong Huang (PI)</p>\n\n\n\n<p>This proposal focuses on the development of foundation models for sequence decision-making, with online and offline stages. The offline stage involves exposure to tasks, datasets, and domains for wide-ranging understanding, while the online stage fine-tunes the pretrained representations to specific tasks. The result is a foundation model that is beneficial for a diverse range of decision-making scenarios.</p>\n\n\n\n\n\n<p><strong>Cornell University</strong>: Jon Kleinberg (PI)</p>\n\n\n\n<p>The proposal aims to study the capability of Large Language Models (LLMs) in processing and remembering relational structures within texts. It sets to understand this through various research questions centered around LLMs ability to extract these structures, and investigate memory compression patterns for potential parallels with human cognitive processes.</p>\n\n\n\n\n\n<p><strong>IIT Delhi</strong>: Mausam (PI)</p>\n\n\n\n<p>This project proposes to study the potential of Large Language Models (LLMs) in solving complex reasoning and planning problems. The PI&#8217;s team plans to develop hybrid models that combine LLMs with traditional AI approaches, and study how their performance varies with increasing problem complexity. The underlying hypothesis to be tested is whether existing reasoning/planning models and LLMs can complement each other&#8217;s strengths and weaknesses. Proposed use-cases include solving Sudoku problems or other puzzles, planning problems, knowledge-graph answering, and task-oriented dialogue.</p>\n\n\n\n\n\n<p><strong>University of Chicago</strong>: Laura Gagliardi (PI)</p>\n\n\n\n<p>The proposal aims to improve the computational efficiency of multireference material simulations by interfacing Large Language Models (LLMs) with existing active space techniques. The proposed methods are expected to automate the efficient partitioning of Hilbert spaces by fine tuning foundational models to specialize in automated active space selection in Multiconfigurational Quantum Chemistry. We will investigate the capabilities of AI agents in performing electronic structure simulations and benchmark their performance based on model parameters and the level of fine-tuning. The study will investigate the strengths and weaknesses of LLMs in performing high-fidelity electronic structure calculations, aim to posit use cases, and suggest avenues for improvement in the domain of LLM-enhanced multireference electronic structure simulations.</p>\n\n\n\n\n\n<p><strong>University of California Berkeley</strong>: Alvin Cheung (PI)</p>\n\n\n\n<p>The primary objectives of this project are twofold. First, we aim to significantly accelerate the inference speed of Large Language Models (LLMs), thereby facilitating faster model serving and more efficient real-time interactions. The second goal is to substantially reduce the computational cost associated with LLM inference. By achieving this, we can make these advanced models more accessible and scalable, allowing for broader deployment across various industries and applications. Both objectives are integral to enhancing the utility and performance of Large Language Models, and thereby contribute to making them a more viable and effective solution for a wide range of challenges.</p>\n\n\n\n\n\n<p><strong>University of Southern California</strong>: Angela Zhou (PI)</p>\n\n\n\n<p>The proposal aims to democratize robust data analysis for causal inference using LLM-based code generation in natural language for interactive optimization. Through development of novel methods for in-context learning and structuring LLM outputs, it aims to transform scientific data interpretation and experimental data synthesis.</p>\n\n\n\n\n\n<p><strong>University of California Berkeley</strong>: Wei Zhan (PI)</p>\n\n\n\n<p>The proposal aims to combine 3D scene understanding with large language models to realize 3D scene generation, a pivotal aspect of simulation technology. The team is set to employ large language models and pre-trained behavior/diffusion models to generate high-fidelity 3D scenes and social behavior. The approach can find wide applications in autonomous driving, robotics, and prompt learning.</p>\n\n\n\n\n\n<p><strong>\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne</strong>: Robert West (PI)</p>\n\n\n\n<p>This project aims to address the challenges and benefits of collaboration among large language model (LLM) agents. We focus on the effectiveness, transparency, and safety aspects of LLM collaboration, with the goal of identifying principles for designing effective and transparent collaborations.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://dlab.epfl.ch/2024-01-10-evaluating-language-model-agency/\" target=\"_blank\" rel=\"noreferrer noopener\">Evaluating Language Model Agency through Negotiations<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of Washington</strong>: Hanna Hajishirzi (PI)</p>\n\n\n\n<p>The proposal presents Training and inference methods for retrieval-based large language models (LLMs) addressing issues like parameter inefficiency, lack of control, and factual incorrectness. The goal is to design LMs that are more controllable, efficient, factual and generalized towards multiple domains.</p>\n\n\n\n\n\n<p><strong>New York University</strong>: Tal Linzen (PI)</p>\n\n\n\n<p>Large language models (LLMs) are argued to be able to learn to perform a task from a handful of examples given \u201cin context\u201d, without weight updates. How robust is in-context learning to distribution shifts between the examples and the target instances of the task? We address this question via evaluations of LLMs\u2019 syntactic generalization and inductive biases, which can reveal whether models rely on linguistically principled structural features or unreliable surface features (such as word positions). We also investigate whether out-of-distribution generalization is affected by\u2014or can be improved by\u2014chain-of-thought prompting, where the model is provided with a sequence of intermediate computation steps that illustrate how the task ought to be performed. Highlights: Models pre-trained on code (like Codex) perform exceptionally well\u2014better than models with an order of magnitude more parameters! Their high performance also correlates well with whether chain-of-thought prompting can improve performance further. We conclude that in-context learning and chain-of-thought prompting may be more robust in models trained on code.</p>\n\n\n\n\n\n<p><strong>University of Virginia</strong>: Thomas Hartvigsen (PI)</p>\n\n\n\n<p>The project aims to enable large language models (LLMs) to reason about time-series data. This involves interactive reasoning, data description and summarization, and statistical query assessment. The work involves the generation of large-scale multi-domain synthetic pairs of time-series and related user queries, and the development of time-series encoders.</p>\n\n\n\n\n\n<p><strong>University of Wisconsin-Madison</strong>: Junjie Hu (PI)</p>\n\n\n\n<p>The proposal targets at enhancing the faithfulness of large language models (LLMs) through the use of structured Chain-of-Thought (CoT) technique. Existing CoT methods often create unfaithful information without grounding to actual world evidence, the proposed project seeks to resolve this by utilizing structured and unstructured data to enhance the truthfulness of LLMs in open-domain multi-hop question-answering.</p>\n\n\n\n\n\n<p><strong>University of Leeds</strong>: Anthony Cohn (PI)</p>\n\n\n\n<p>We aim to improve the logical reasoning capabilities of large language models (LLMs) by enhancing semantic parsing, integrating symbolic reasoning techniques, providing customized training, and conducting evaluation. The focus will be on developing a logic reasoning module and integrating it with LLMs, fine-tuning LLMs on logic-based data, and evaluating the performance of the enhanced AI system on complex first-order logic reasoning tasks.</p>\n\n\n\n\n\n<p><strong>University of Leeds</strong>: Anthony Cohn (PI)</p>\n\n\n\n<p>We will work to evaluate and improve the commonsense reasoning abilities of large language models (LLMs) through a variety of approaches. This will involve building new benchmarks to evaluate specific aspects of commonsense reasoning and the development of a new dialectical approach that uses multi-turn conversations to test a system\u2019s understanding and consistency. We will focus on assessing comprehension of extended initial texts, making commonsense entailments, using synthetic data for inference, and testing the robustness of responses to variations in queries.</p>\n\n\n\n\n\n<p><strong>Indian Institute of Science</strong>: Siddhartha Gadgil (PI)</p>\n\n\n\n<p>The proposed research aims to integrate Foundation Models with Interactive Theorem Provers to improve reliability in mathematical reasoning. The project will focus on two primary aspects: proof automation and an approach referred to as autoformalization. The expected outcome is a reasoning system that can be utilized by individuals with limited or no programming language and AI knowledge. Both prompt engineering and fine-tuning techniques will be applied for this project.</p>\n\n\n\n\n\n<p><strong>University of California Berkeley</strong>: Joseph Gonzalez (PI)</p>\n\n\n\n<p>The proposal focuses on training Large Language Models (LLMs) to use tools to take actions on the external world, particularly through discovering and invoking public APIs. The goal is to enable LLMs to understand brief natural language descriptions and autonomously compose and execute the required API calls. The project aims to overcome challenges such as the vast and changing space of APIs, LLM hallucination, and the comprehension and composition of multiple API calls together.</p>\n\n\n\n\n\n<p><strong>KAIST</strong>: Steven Euijong Whang (PI)</p>\n\n\n\n<p>Hallucination in language models refers to generating nonsensical or unfaithful content. Foundation models are especially vulnerable to information leakage through hallucination, posing serious privacy risks for their training data. An emerging line of research to combat hallucination adopts Chain of Thought (CoT) prompting, which offers additional reasoning steps by manually constructed prompts [Wei et al., NIPS\u201922]. Very recently, automated CoT prompt generation (Auto-CoT) has been proposed to eliminate the need for human effort [Zhang et al., ICLR\u201923]. While most of works focuses on arithmetic and commonsense reasoning tasks, it is essential to explore with various NLP tasks to better understand the hallucination in different contexts. Hence, our research aims to (1) develop strategies to enhance Auto-CoT\u2019s performance using GPT-3.5 and GPT-4, and (2) extend our investigation to various NLP tasks that can also have privacy issues.</p>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Noah Goodman (PI)</p>\n\n\n\n<p>The proposal focuses on enhancing the inductive reasoning capabilities of LLMs by generating explicit inductive hypotheses. This is achieved by translating these into Python programs that can be validated against examples and can also be generalized to novel inputs. Initial experiments display improved accuracies and the aim is to further extend these results with the aid of additional compute resources.</p>\n\n\n\n\n\n<p><strong>University of Michigan, Ann Arbor</strong>: Lu Wang (PI)</p>\n\n\n\n<p>We aim to analyze the intermediate reasoning capabilities of large language models (LLMs) for long multi-document inputs, particularly when applied to the task of multi-document summarization (MDS). Recent work has shown that summarization-specific pre-trained language models (PLMs) exhibit poor multi-document reasoning and synthesis capabilities, motivating the need for improved modeling of the MDS task. Unfortunately, these limitations have significant implications for sensitive real-world applications requiring holistic understanding of inputs from heterogeneous sources. To this end, we will study whether state-of-the-art LLMs are capable of multi-document reasoning and information synthesis, and investigate whether prompt-based probing of LLMs can yield insights into their decision-making processes for the MDS task. Specifically, we seek to elicit chain-of-thought explanations within promptable LLMs for the purpose of interpreting and improving multi-document summarization approaches.</p>\n\n\n\n\n\n<p><strong>Texas A&M University Corpus Christi</strong>: Chandra Sekharan (PI)</p>\n\n\n\n<p>Recent advances in foundational large language models (LLMs) are already positively impacting application domains such as healthcare, finance, and education. The next evolutionary leap in LLMs lies in their integration with physical systems. This evolution is both exciting and challenging: it simplifies the application layer&#8217;s design, development, and deployment while posing the challenge of ensuring alignment and safety. This research introduces a novel 3-layer software architecture, designed to foster safe and aligned autonomous systems. It incorporates a hybridized fine-tuning/RAG code generator to initiate intelligent missions for systems to operate autonomously and safely. A key aspect of this project is an extensive Pilot implementation, which leverages cloud-based LLM API providers (Azure&#8217;s OpenAI), Vector Databases, ArduPilot open-source software, and commercial hardware for drones. We will conduct tests and missions at the Lone Star UAS Center of Excellence and Innovation, affiliated with Texas A&M University-Corpus Christi. This center, one of seven FAA-authorized sites in the U.S., provides a regulatory-compliant and safe environment for achieving the project&#8217;s goals. The research will be impactful in reducing the complexity of launching missions to help improve the environmental resilience of the Gulf of Mexico region, while engaging under-represented students in research on LLM technologies.</p>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Tobias Gerstenberg (PI)</p>\n\n\n\n<p>The proposal from Stanford&#8217;s Causality in Cognition Lab outlines research plans to use Foundation Models fory studying causal reasoning, moral judgments, and social cognition. The proposal outlines methods to automate the construction of causal models, understand the development of category representations, simulate the cultural transmission of scientific knowledge and align these models with human social and moral reasoning.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2403.19154\" target=\"_blank\" rel=\"noreferrer noopener\">STaR-GATE: Teaching Language Models to Ask Clarifying Questions<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>IIT Hyderabad</strong>: Vineeth N Balasubramanian (PI)</p>\n\n\n\n<p>This proposal aims to increase the robustness, interpretability, and generalizability of deep learning models using causal principles. The study intends to leverage Large Language Models (LLMs) for robust causal inference and causal grounding of representations learned by deep learning models.</p>\n\n\n\n\n\n<p><strong>Gwangju Institute of Science And Technology</strong>: Sundong Kim (PI)</p>\n\n\n\n<p>The project attempts to leverage LLMs to tackle the unsolved Abstraction Reasoning Corpus (ARC) task and to utilize LLMs for data augmentation purposes. The primary aim is to derive a logical pattern for each task, similar to how a human would do, aided by ARC dataset and LLM APIs.</p>\n\n\n\n\n\n<p><strong>IIT Bombay</strong>: Sunita Sarawagi (PI)</p>\n\n\n\n<p>Our goal is to tackle three challenges in integrating private relational databases with foundation models for conversational querying and exploration. These include: (1) Efficiently and accurately retrieving the relevant subset of a database schema for a query. (2) Benchmarking the performance of LLMs under ambiguity and generating follow-up questions for clarification. (3) On the fly adaptation of Text-to-SQL generation on complex schema subgraphs.</p>\n\n\n\n\n\n<p><strong>University of Chicago</strong>: James Evans (PI)</p>\n\n\n\n<p>LLMs have the ability to accurately reproduce systems of beliefs and attitudes corresponding to different segments of the public. By leveraging this capability, this project seeks to implement &#8220;simulated public opinion&#8221; analyses as an LLM guardrail. Before an AI automated task is initiated, the LLM will simulate public responses to the proposed actions, generating a diverse set of responses corresponding to a heterogeneous stakeholder population which could be local, national, or global. By this mechanism, actions that would be deemed widely unacceptable by the relevant public can be identified and halted before proceeding. This guardrail may prove to be an important step toward aligning AI systems with the diverse set of values and interests held by the publics affected by algorithmic decision making.</p>\n\n\n\n\n\n<p><strong>University of Pennsylvania</strong>: Surbhi Goel (PI)</p>\n\n\n\n<p>This proposal targets the issue of poor out-of-distribution (OoD) performance in large language models on algorithmic reasoning tasks which serve as elementary building blocks for more intricate reasoning problems. The overarching goal is the development of a rigorous understanding of OoD failures and new algorithmic strategies for their mitigation using synthetic setups as a testbed.</p>\n\n\n\n\n\n<p><strong>MIT</strong>: Song Han (PI)</p>\n\n\n\n<p>Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general- purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56\u00d7 speedup and 2\u00d7 memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2305.11348\" target=\"_blank\" rel=\"noreferrer noopener\">In the Name of Fairness: Assessing the Bias in Clinical Record De-identification<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>University of California, Irvine</strong>: Brian Demsky (PI)</p>\n\n\n\n<p>This project aims to explore the use of LLMs in software development, their limitations, and the potential development of automated tools to improve the efficiency of using LLM for software development. The areas of focus include LLM&#8217;s handling of novel programming tasks, ways around the token limit, ensuring code correctness, their ability to help with debugging, and their effectiveness at modifying existing code. Our approach entails developing a set of programming tasks, understanding effective approaches at manually using LLMs for various tasks, and gradually transitioning from manual queries to automated processes.\u201d</p>\n\n\n\n\n\n<p><strong>University of California Berkeley</strong>: Amir Gholami (PI)</p>\n\n\n\n<p>The proposal focuses on improving the inference of Large Language Models (LLMs), particularly Transformer Models, by developing a novel LLM serving framework and two research thrusts: Systematic LLM Quantization and Pruning, and Speculative Decoding with Big Little Hierarchical methods. Through these approaches, the project aims to tackle the growing computational and memory bandwidth requirements of these models, thus enhancing their deployment in applications such as virtual assistants, chat bots and machine translation.</p>\n\n\n\n\n\n<p><strong>Georgia Institute of Technology</strong>: Alan Ritter (PI)</p>\n\n\n\n<p>The project will evaluate the effectiveness of INSTRUCTE, an open-domain method for extracting structured records from tables in scientific literature. A new dataset called ARXIVTABLES, consisting of 3,792 annotated cells across 122 tables from 25 machine learning papers, will be developed for evaluating the proposed task. INSTRUCTE will also be extended to the leaderboard extraction task by linking the extracted data with predefined leaderboards. The contributions of the project include defining the new TABLE2JSON task, introducing the INSTRUCTE prompting method, and constructing the ARXIVTABLES dataset.</p>\n\n\n\n<p><strong>Related papers:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2305.14336\" target=\"_blank\" rel=\"noreferrer noopener\">Schema-Driven Information Extraction from Heterogeneous Tables<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.02224\" target=\"_blank\" rel=\"noreferrer noopener\">Can Language Models be Instructed to Protect Personal Information?<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2311.09538\" target=\"_blank\" rel=\"noreferrer noopener\">Reducing Privacy Risks in Online Self-Disclosures with Language Models<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>Princeton University</strong>: Danqi Chen (PI)</p>\n\n\n\n<p>Interested in understanding the limitations of these powerful LLMs, as well as developing effective methods to improve their capabilities. Our team is studying the limitations of large language models (LLMs) like GPT-3, ChatGPT, and GPT-4, aiming to enhance their capabilities. We are working on two sub-projects: one involves creating a benchmark and modeling methods to improve LLMs\u2019 ability to cite references accurately, while the other focuses on teaching LLMs to excel at long-text reading compression and summarization tasks, which they currently struggle with.</p>\n\n\n\n<p><strong>Related paper:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2305.14627\" target=\"_blank\" rel=\"noreferrer noopener\">Enabling Large Language Models to Generate Text with Citations<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<p><strong>Stanford University</strong>: Tengyu Ma (PI)</p>\n\n\n\n<p>This project aims to enhance the reasoning capacity of Large Language Models (LLMs), enabling them to effectively perform theorem proving and mathematical question answering. Using proofs in formal languages like Lean, Isabelle, and Coq, the models will be fine-tuned using Reinforcement Learning from Human Feedback. This process will be extended to natural language as well. New LLM architectures will also be explored to optimize reasoning capabilities.</p>\n\n\n\n\n\n<p><strong>Polytechnique Montr\u00e9al</strong>: Sarath Chandar (PI)</p>\n\n\n\n<p>The proposal aims to develop continuous learning strategies for foundation models, focusing on addressing data distribution shifts, designing effective optimization methods, and aligning models with evolving human preferences. The project focuses on reducing wasteful retraining processes and increasing the time effectiveness of these models.</p>\n\n\n\n\n\n<p><strong>Princeton University</strong>: Thomas Griffiths (PI)</p>\n\n\n\n<p>This proposal aims to apply cognitive science methodologies to foundation models, specifically large language models (LLMs), to gain insights valuable to both social sciences and foundation model research. By relating LLMs behavior to human decision-making processes, the project aims to both observe and mitigate current LLMs weaknesses.</p>\n\n\n\n<p><strong>Related papers:</strong></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2402.06992\" target=\"_blank\" rel=\"noreferrer noopener\">A Rational Analysis of the Speech-to-Song Illusion<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2402.04105\" target=\"_blank\" rel=\"noreferrer noopener\">Measuring Implicit Bias in Explicitly Unbiased Large Language Models<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://osf.io/preprints/psyarxiv/3jvxw\" target=\"_blank\" rel=\"noreferrer noopener\">Studying the Effect of Globalization on Color Perception using Multilingual Online Recruitment and Large Language Models<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2402.07282\" target=\"_blank\" rel=\"noreferrer noopener\">How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<div style=\"height:25px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Academic research plays such an important role in advancing science, technology, culture, and society. This grant program helps ensure this community has access to the latest and leading AI models. which enhances safety, robustness, sustainability, responsibility, and transparency, while ensuring rapid progress can be measured via new evaluation methods A common theme among these research [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 995412,
        "date": "2023-12-27T22:56:21",
        "slug": "societal-ai",
        "title": "Societal AI",
        "link": "https://www.microsoft.com/en-us/research/project/societal-ai/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-catalina-blue card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"societal-ai\">Societal AI</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Over the past year, artificial intelligence has exhibited remarkable advancements, surpassing previously held expectations. Amidst the excitement, a crucial question arises: Is technology itself neutral in terms of values? After all, the intelligence of Large Language Models (LLMs) is based on human-generated corpora, which inevitably are embedded with human biases and values, influencing the reasoning and judgment of machines.</p>\n\n\n\n<p>To ensure that AI adheres to the principle of benefiting humanity, Xing Xie and his colleagues at Microsoft Research Asia believe it\u2019s imperative to not only develop technologies aligned with this objective but also to establish rules and methodologies that extend beyond the technological realm. Their area of study involves value orientations as well as AI safety, verifiability, copyright, and model evaluation, which are all closely related to social responsibility.</p>\n\n\n\n<div style=\"height:50px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Over the past year, artificial intelligence has exhibited remarkable advancements, surpassing previously held expectations. Amidst the excitement, a crucial question arises: Is technology itself neutral in terms of values? After all, the intelligence of Large Language Models (LLMs) is based on human-generated corpora, which inevitably are embedded with human biases and values, influencing the reasoning [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 967236,
        "date": "2023-12-11T15:19:29",
        "slug": "query-optimization-for-database-systems",
        "title": "Query Optimization for Database Systems",
        "link": "https://www.microsoft.com/en-us/research/project/query-optimization-for-database-systems/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"query-optimization\">Query Optimization </h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>The query optimizer is a crucial component in a relational database system and is responsible for finding a good execution plan for a SQL query. For cloud database service providers, the importance of query optimization is amplified due to the scale (e.g., millions of databases hosted) and variety of different workloads for which the query optimizer is expected to work well &#8220;out-of-the-box&#8221;. Query optimization is challenging due to the richness of SQL queries that contain operators such as joins, group-by, aggregation, and nested sub-queries, the limited data statistics available during query optimization, and the need to keep time and resources for query optimization small. We are interested in a variety of problems related to query optimization. We highlight below some examples of our recent work in query optimization. </p>\n\n\n\n<p>We conducted an empirical study to <a href=\"https://www.microsoft.com/en-us/research/publication/analyzing-the-impact-of-cardinality-estimation-on-execution-plans-in-microsoft-sql-server/\">quantify the impact of cardinality estimation</a> on plan quality in a state-of-the-art query optimizer (in the Microsoft SQL Server database engine), built using the Volcano/Cascades framework. We have also explored the viability of using <em>data-driven techniques (e.g., ML models) </em>that exploit knowledge of the workload for problems such as<a href=\"https://www.microsoft.com/en-us/research/publication/leveraging-query-logs-and-machine-learning-for-parametric-query-optimization/\"> parameteric query optimization</a> and <a href=\"https://www.microsoft.com/en-us/research/publication/selectivity-estimation-for-range-predicates-using-lightweight-models/\">cardinality estimation</a>.  With the increasing focus in research and industry on leveraging data-driven techniques for query optimization, we feel it is important to evaluate such systems fairly by taking into consideration the costs of obtaining training data as well as robustness to changing workload characteristics. For this purpose, we developed and <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/dsb\">open-sourced</a> the <a href=\"https://www.microsoft.com/en-us/research/publication/dsb-a-decision-support-benchmark-for-workload-driven-and-traditional-database-systems/\">DSB benchmark</a>. </p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>The query optimizer is a crucial component in a relational database system and is responsible for finding a good execution plan for a SQL query. For cloud database service providers, the importance of query optimization is amplified due to the scale (e.g., millions of databases hosted) and variety of different workloads for which the query [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 979329,
        "date": "2023-11-26T21:16:53",
        "slug": "healthcare",
        "title": "Healthcare Research at MSR India",
        "link": "https://www.microsoft.com/en-us/research/project/healthcare/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"1024\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/healthcare.jpg\" class=\"attachment-full size-full\" alt=\"Showing healthcare\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/healthcare.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/healthcare-300x300.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/healthcare-150x150.jpg 150w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/healthcare-768x768.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/healthcare-180x180.jpg 180w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/healthcare-360x360.jpg 360w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"healthcare-research-at-msr-india\">Healthcare Research at MSR India</h1>\n\n\n\n<p>At Microsoft Research Lab India, we conduct a variety of healthcare related research, including smartphone-based low-cost diagnostics, generative AI chatbots to support the healthcare ecosystem, and promote mental wellbeing of employees.</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p><strong>Low-Cost Diagnostics</strong></p>\n\n\n\n<p>Healthcare is not accessible to a huge population across the globe. There are a variety of reasons for that including skewed doctor-to-patient ratio, unskilled to semi-skilled healthcare workers, long waiting and long commute to see a doctor which also results in loss of income for daily wage earners. Our aim is to democratize healthcare. And our vision is to develop <strong>low-cost smartphone-based diagnostic solutions</strong> with AI assistant embedded in them, to enable community health workers, teachers, primary clinicians, and even Swiggy/Amazon delivery personnel, to perform preliminary screening of certain diseases with minimal training. With that vision in mind, over the past four years, we have been working towards developing a variety of diagnostics tool&#8212;detecting keratoconus using a smartphone-based corneal topographer (<a href=\"https://www.microsoft.com/en-us/research/project/smartkc-a-smartphone-based-corneal-topographer/\">SmartKC<span class=\"sr-only\"> (opens in new tab)</span></a>), estimating refractive errors (<a href=\"https://www.microsoft.com/en-us/research/project/auto-retinoscopy-automating-retinoscopy-for-refractive-error-diagnosis/\">Auto-Retinoscopy<span class=\"sr-only\"> (opens in new tab)</span></a>), computing the dryness level of the eye, classifying crackle and wheeze lung sound (<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/RespireNet\">RespireNet<span class=\"sr-only\"> (opens in new tab)</span></a>), and <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://mohitjaindr.github.io/pdfs/c29-embc-2021.pdf\">estimating height<span class=\"sr-only\"> (opens in new tab)</span></a> of children for malnutrition prediction&#8212;all using smartphone. These works has been done in close collaboration with hospitals like Sankara Eye Hospital and NGO\u2019s like WeltHungerHilfe.</p>\n\n\n\n<p><strong>HealthBots</strong></p>\n\n\n\n<p>Recent studies have highlighted that over 80% of patients and their caregivers require timely, trustworthy, detailed, and accurate information about their treatment. The provision of such information holds the potential to alleviate pre- and post-operative anxiety. To address this issue, we designed and developed chatbots, powered by state-of-the-art generative AI models fine-tuned on the doctor\u2019s provided knowledge base. These HealthBots aims to assist patients and their caregivers in obtaining answers to their queries regarding pre- and post-treatment. Throughout the process, doctors and patient coordinators need to actively participate, offering feedback on the bot&#8217;s generated answers to improve the quality of responses over time. The bots are designed to be multimodal and multilingual. Learn more about the <a href=\"https://www.microsoft.com/en-us/research/project/health-bots/\">HealthBots </a>project.</p>\n\n\n\n<p><strong>Mental Health</strong></p>\n\n\n\n<p>How can organizations take responsibility for Employees\u2019 Mental Wellbeing? Learn more about the <a href=\"https://www.microsoft.com/en-us/research/project/zen/\">Zen</a> project.</p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>At Microsoft Research Lab India, we conduct a variety of healthcare related research, including smartphone-based low-cost diagnostics, generative AI chatbots to support the healthcare ecosystem, and promote mental wellbeing of employees. Low-Cost Diagnostics Healthcare is not accessible to a huge population across the globe. There are a variety of reasons for that including skewed doctor-to-patient [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 979335,
        "date": "2023-11-26T21:00:15",
        "slug": "health-bots",
        "title": "HealthBots",
        "link": "https://www.microsoft.com/en-us/research/project/health-bots/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"338\" height=\"654\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Screenshot-2023-11-27-101125.png\" class=\"attachment-full size-full\" alt=\"Screenshot of a chatbot answering a patient question\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Screenshot-2023-11-27-101125.png 338w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Screenshot-2023-11-27-101125-155x300.png 155w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Screenshot-2023-11-27-101125-93x180.png 93w\" sizes=\"(max-width: 338px) 100vw, 338px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"healthbots\">HealthBots</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Recent studies have highlighted that more than 70% of patients and their caregivers experience anxiety prior to undergoing an invasive treatment. Additionally, over 80% of them require timely, trustworthy, detailed, and accurate information about their treatment. The provision of such information holds the potential to alleviate pre- and post-operative anxiety.</p>\n\n\n\n<p>To address this issue, we designed and developed chatbots, powered by state-of-the-art generative AI models fine-tuned on the doctor\u2019s provided knowledge base. These HealthBots aims to assist patients and their caregivers in obtaining answers to their queries regarding pre- and post-treatment. Throughout this process, doctors and patient coordinators need to actively participate, offering feedback on the bot&#8217;s generated answers to improve the quality of responses over time. The bots are designed to be multimodal, supporting both voice and text interactions, and it is also multilingual.</p>\n\n\n\n<p>Right now, we are working on developing and evaluating chatbots for patients undergoing Cataract surgeries (<strong>CataractBot</strong>, in collaboration with the Sankara Eye Hospital) and patients undergoing Cancer treatment (<strong>OncoBot</strong>).</p>\n\n\n\n<p>Apart from that, we are also exploring the usage of generative AI in training community health workers and nurses (in collaboration with local NGOs), and in supporting pre-consultation to help information exchange between healthcare providers and patients (in collaboration with the University of Toronto).</p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Recent studies have highlighted that more than 70% of patients and their caregivers experience anxiety prior to undergoing an invasive treatment. Additionally, over 80% of them require timely, trustworthy, detailed, and accurate information about their treatment. The provision of such information holds the potential to alleviate pre- and post-operative anxiety. To address this issue, we [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 978063,
        "date": "2023-11-24T01:00:00",
        "slug": "project-maira",
        "title": "Project MAIRA",
        "link": "https://www.microsoft.com/en-us/research/project/project-maira/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MAIRA_header_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"female radiologist analyzing an MRI image of the head\" style=\"object-position: 61% 42%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MAIRA_header_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MAIRA_header_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MAIRA_header_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MAIRA_header_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MAIRA_header_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MAIRA_header_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MAIRA_header_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"project-maira\">Project MAIRA</h1>\n\n\n\n<p>Multimodal AI for Radiology Applications</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Project MAIRA is a research project from <a href=\"https://www.microsoft.com/en-us/research/lab/microsoft-health-futures/\">Microsoft Health Futures</a> that builds innovative, multimodal AI technology to assist radiologists in delivering effective patient care and to empower them in their work. The goal of the project is to leverage rich healthcare data \u2013 including medical domain knowledge, temporal sequences of medical images and corresponding radiology reports, and other clinical context information \u2013 as inputs to developing multimodal frontier models that can be scaled and fine-tuned to many different radiology applications.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/publication/exploring-the-boundaries-of-gpt-4-in-radiology/\" target=\"_blank\" rel=\"noreferrer noopener\">Read GPT-4 benchmark paper</a></div>\n\n\n\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://aka.ms/maira-report\" target=\"_blank\" rel=\"noreferrer noopener\">Read MAIRA model technical report</a></div>\n</div>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Maira-Technology-V7_final-1024x576.png\" alt=\"Schematic illustration of multimodal healthcare data as input to the MAIRA foundation model which enables multiple different user applications such as draft report generation, disease classification or error detection. \" class=\"wp-image-985179\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Maira-Technology-V7_final-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Maira-Technology-V7_final-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Maira-Technology-V7_final-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Maira-Technology-V7_final-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Maira-Technology-V7_final-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Maira-Technology-V7_final-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Maira-Technology-V7_final-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Maira-Technology-V7_final-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Maira-Technology-V7_final-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Maira-Technology-V7_final-1280x720.png 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Maira-Technology-V7_final.png 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<p>New approaches combining various data modalities and their temporal connections enable AI tasks such as: detecting reporting errors; auto-generating draft reports; or improving disease progression assessments and their quantification over time. Such innovations will play a crucial role in detecting missed clinical observations; improving reporting capacity in an already heavily over-burdened radiology workforce; and assisting reporting consistency, accuracy and equity \u2013 all of which serve to reduce shortcomings in existing imaging workflows and to increase patient safety and care quality. &nbsp;</p>\n\n\n\n<p>To advance this work requires a human-centred, responsible approach to AI development that places clinical utility and careful workflow integration at its core. This involves close stakeholder engagements and clinical collaborations within real-world healthcare contexts; the creation of a new research frontier in evaluating large multimodal models in a clinically relevant manner; and an overall drive to move from technical AI innovation towards successful healthcare delivery.&nbsp;</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Multimodal AI for Radiology Applications Project MAIRA is a research project from Microsoft Health Futures that builds innovative, multimodal AI technology to assist radiologists in delivering effective patient care and to empower them in their work. The goal of the project is to leverage rich healthcare data \u2013 including medical domain knowledge, temporal sequences of [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 986193,
        "date": "2023-11-21T06:26:01",
        "slug": "accessibility-and-assistive-technology",
        "title": "Accessibility and Assistive technology",
        "link": "https://www.microsoft.com/en-us/research/project/accessibility-and-assistive-technology/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"2500\" height=\"1667\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/torino.jpg\" class=\"attachment-full size-full\" alt=\"Two children sitting on the floor playing with CodeJumper while a facilitator looks on\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/torino.jpg 2500w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/torino-300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/torino-1024x683.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/torino-768x512.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/torino-1536x1024.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/torino-2048x1366.jpg 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/torino-240x160.jpg 240w\" sizes=\"(max-width: 2500px) 100vw, 2500px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"accessibility-and-assistive-technology\">Accessibility and Assistive Technology</h1>\n\n\n\n<p>With Emphasis on the Global South</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>The work on accessibility at MSR India has spanned the range from spatial audio with HoloLens to the use of feature phones to reach children with vision impairments and a spectrum of tangible toys to enhance numeracy for them, to a quiz platform for the Deaf or Hard of Hearing community, with an overarching new methodology called Ludic Design for Accessibility.</p>\n\n\n\n<p>Most of this work has been focused on the people with disabilities (PwDs) in the Global South and firmly rooted in the lived experience of PwDs by strong partnerships with disabled peoples organizations. The interdisciplinary nature of the work needed to address the complex challenges has naturally attracted a diverse set of people to work on these projects.</p>\n\n\n\n<p></p>\n\n\n\n<p>The following are the currently active projects with details in the respective links.</p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/project/ludicdesign/\">Ludic Design for Accessibility</a> is a novel methodology that puts play and playfulness at the center of any design for accessibility. The key idea is that a solution for accessibility designed with this methodology will be in the form of an engaging and inclusive game. By extended and joyful play with the game the players will acquire the designed in skills purely as a side effect.</p>\n\n\n\n<p></p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/project/scalable-early-education-with-digital-scaffolding-seeds/\">SEEDS<span class=\"sr-only\"> (opens in new tab)</span></a>, Scalable educational experiences with digital scaffolding, is a project that builds on top of the work over the past four years with <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"http://visionempowertrust.org\">Vision Empower Trust<span class=\"sr-only\"> (opens in new tab)</span></a>, a DPO that has now reached about a100 schools for the blind across a dozen states of India, codesigning the solutions with us and taking them to the end users. The goal of the project is to introduce digital technologies to children in schools for the blind from the primary stage onwards.</p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/project/sign-language-inclusive-play/\">Sign Language Inclusive Play </a> is a broad effort to apply the ludic design principles to create solutions for the DHH community. </p>\n\n\n\n<p>SPICE-IN</p>\n\n\n\n<p></p>\n\n\n\n<p>Disability and Gender</p>\n\n\n\n<p></p>\n\n\n\n<p></p>\n\n\n\n<p>The following are projects that are no longer active.</p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/project/project-avare/\">Project AVARE</a>: Audio-Augmented virtual and real environments is a project with a goal of empowering VIPs to experience rich multi-modal interactions in everyday activities ranging from work and social interactions to entertainment and exploration.&nbsp;</p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/project/codetalk/\">Project CodeTalk</a>: CodeTalk is a VisualStudio plugin that systematically addresses barriers in discoverability, navigability, alertability and glanceability.</p>\n\n\n\n<p></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>With Emphasis on the Global South The work on accessibility at MSR India has spanned the range from spatial audio with HoloLens to the use of feature phones to reach children with vision impairments and a spectrum of tangible toys to enhance numeracy for them, to a quiz platform for the Deaf or Hard of [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 983295,
        "date": "2023-11-20T18:02:56",
        "slug": "orca",
        "title": "Orca",
        "link": "https://www.microsoft.com/en-us/research/project/orca/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca-project_header_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"ORCA project header : AI-generated whale graphic over an abstract background of data waves\" style=\"object-position: 75% 47%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca-project_header_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca-project_header_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca-project_header_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca-project_header_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca-project_header_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca-project_header_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca-project_header_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"orca\">Orca</h1>\n\n\n\n<p>Redefining small LMs performance</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>In this project, we develop technologies for creating, improving, and specializing small LMs (~10B parameters or less). Our research involves self-improvement strategies, feedback-driven teaching methods between large and small models and utilizing domain specific data to specialize LMs. We focus on using richer training signals for teaching small LMs to do more with less capacity with emphasis on creating tailored and high-quality synthetic data for post-training and alignment of LMs.</p>\n\n\n\n<p>Orca focuses on:</p>\n\n\n\n<ul>\n<li><strong>Synthetic data creation</strong>: create tailored and high-quality synthetic data for small model training</li>\n\n\n\n<li><strong>Better reasoning capabilities</strong>: give smaller LMs enhanced reasoning abilities, typically found only in much larger models</li>\n\n\n\n<li><strong>Model specialization</strong>: create specialized models that&nbsp;gives the model specialized capabilities or custom behaviors</li>\n</ul>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<hr class=\"wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background\" />\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center\" id=\"models\">Models</h2>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-99 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h3 class=\"wp-block-heading\" id=\"orca-progressive-learning-from-complex-explanation-traces\">Orca: Progressive Learning from Complex Explanation Traces</h3>\n\n\n\n<p>Imitate reasoning processes of larger models with explanation tuning; improvements over models like Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42% on AGIEval.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/\">Read the paper</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h3 class=\"wp-block-heading\" id=\"orca-2-teaching-small-language-models-how-to-reason\">Orca-2: Teaching Small Language Models How To Reason</h3>\n\n\n\n<p>Enhance smaller language models with reasoning abilities traditionally seen in larger models by teaching models to choose different strategies for varied tasks; performance levels similar or better than those of models 5-10x larger on complex tasks that test advanced reasoning abilities in zero-shot settings.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-43 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://huggingface.co/microsoft/Orca-2-7b\" target=\"_blank\" rel=\"noreferrer noopener\">Orca-2-7B</a></div>\n\n\n\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://huggingface.co/microsoft/Orca-2-13b\" target=\"_blank\" rel=\"noreferrer noopener\">Orca-2-13B</a></div>\n\n\n\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/publication/orca-2-teaching-small-language-models-how-to-reason/\">Read the paper</a></div>\n</div>\n</div>\n</div>\n\n\n\n<p>Orca models were designed for research settings, and its testing has only been carried out in such environments. It should not be used in downstream applications, as additional analysis is needed to assess potential harm or bias in the proposed application.</p>\n\n\n\n<hr class=\"wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background\" />\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"800\" height=\"1054\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca_Figure2.jpg\" alt=\"This image from the paper \"Orca-2: Teaching Small Language Models How To Reason\" showcases differences in how Orca 2, LLaMA-2, LLaMA-2-Chat, and ChatGPT (GPT-3.5-Turbo) process and answer a logic-based question. The LLaMA-2 and LLaMA-2-Chat outputs were generated via replicate.com/meta/llama-2-13b and chat.lmsys.org, employing standard settings (temperature=0, top_p=1). ChatGPT's response was retrieved from chat.openai.com, providing a clear comparison of how each model approaches problem-solving.\" class=\"wp-image-985878\" style=\"width:800px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca_Figure2.jpg 800w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca_Figure2-228x300.jpg 228w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca_Figure2-777x1024.jpg 777w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca_Figure2-768x1012.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Orca_Figure2-137x180.jpg 137w\" sizes=\"(max-width: 800px) 100vw, 800px\" /><figcaption class=\"wp-element-caption\">This image from the paper &#8220;Orca-2: Teaching Small Language Models How To Reason&#8221; showcases differences in how Orca 2, LLaMA-2, LLaMA-2-Chat, and ChatGPT (GPT-3.5-Turbo) process and answer a logic-based question. The LLaMA-2 and LLaMA-2-Chat outputs were generated via replicate.com/meta/llama-2-13b and chat.lmsys.org, employing standard settings (temperature=0, top_p=1). ChatGPT&#8217;s response was retrieved from chat.openai.com, providing a clear comparison of how each model approaches problem-solving.</figcaption></figure>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Redefining small LMs performance In this project, we develop technologies for creating, improving, and specializing small LMs (~10B parameters or less). Our research involves self-improvement strategies, feedback-driven teaching methods between large and small models and utilizing domain specific data to specialize LMs. We focus on using richer training signals for teaching small LMs to do [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 984894,
        "date": "2023-11-15T10:15:50",
        "slug": "understanding-virtual-hybrid-meetings",
        "title": "Understanding Virtual & Hybrid Meetings",
        "link": "https://www.microsoft.com/en-us/research/project/understanding-virtual-hybrid-meetings/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1280\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/11/video_Panel-Perspectives-on-the-new-future-of-hybrid-meetings.jpg\" class=\"attachment-full size-full\" alt=\"video: Panel: Perspectives on the new future of hybrid meetings\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/11/video_Panel-Perspectives-on-the-new-future-of-hybrid-meetings.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2021/11/video_Panel-Perspectives-on-the-new-future-of-hybrid-meetings-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2021/11/video_Panel-Perspectives-on-the-new-future-of-hybrid-meetings-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2021/11/video_Panel-Perspectives-on-the-new-future-of-hybrid-meetings-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2021/11/video_Panel-Perspectives-on-the-new-future-of-hybrid-meetings-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2021/11/video_Panel-Perspectives-on-the-new-future-of-hybrid-meetings-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2021/11/video_Panel-Perspectives-on-the-new-future-of-hybrid-meetings-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2021/11/video_Panel-Perspectives-on-the-new-future-of-hybrid-meetings-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2021/11/video_Panel-Perspectives-on-the-new-future-of-hybrid-meetings-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2021/11/video_Panel-Perspectives-on-the-new-future-of-hybrid-meetings-960x540.jpg 960w\" sizes=\"(max-width: 1280px) 100vw, 1280px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/project/blended-reality-for-effective-workflows/past-projects/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"BREW Past Projects\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tBREW Past Projects\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"understanding-virtual-hybrid-meetings\">Understanding Virtual & Hybrid Meetings</h1>\n\n\n\n<p>Formative studies in distributed meeting behavior and technologies</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>This is a collection of studies in distributed meeting behavior and technologies, covering virtual meetings, hybrid meetings, mobile robotic telepresence, and avatars.</p>\n\n\n\n<p>See <strong><a href=\"https://www.microsoft.com/en-us/research/project/understanding-virtual-hybrid-meetings/publications/\">Publications</a></strong>.</p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Formative studies in distributed meeting behavior and technologies This is a collection of studies in distributed meeting behavior and technologies, covering virtual meetings, hybrid meetings, mobile robotic telepresence, and avatars. See Publications. Opens in a new tab</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 967230,
        "date": "2023-11-08T16:46:43",
        "slug": "query-acceleration-for-data-lakes",
        "title": "Query Acceleration for Data Lakes",
        "link": "https://www.microsoft.com/en-us/research/project/query-acceleration-for-data-lakes/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-catalina-blue card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"query-acceleration-for-data-lakes\">Query Acceleration for Data Lakes</h1>\n\n\n\n<p>Accelerating query processing on open data formats</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>As businesses become more data-driven, there is an increasing interest in adopting data lakes (e.g., <a href=\"https://www.microsoft.com/en-us/microsoft-fabric\">Microsoft Fabric</a>) in large enterprises. A data lake is a large storage repository that stores a vast amount of data in a variety of open data formats, making it accessible for all use cases (e.g., AI/data science/BI/reporting) that have arisen or could arise. This includes text-based raw data formats such as CSV and JSON, row-wise binary formats such as Apache Avro, and batched column-wise formats such as Apache Parquet and ORC. In data lakes, data is ingested in its native open format without expensive and time-consuming data preparation. </p>\n\n\n\n<p>We are innovating on the storage tier of this emerging architecture to accelerate query processing on various open data formats. Our research has been commercialized and widely used in several products of Microsoft. Example techniques we developed include:</p>\n\n\n\n<ul>\n<li><strong>Mison</strong>. Mison is a fast parser for raw data formats such as CSV and JSON. It is order of magnitude faster than the traditional finite state machine-based approach. Our new parsing technique allows query engines to push down projections and filters of queries into the parser, and thus avoids a great deal of wasted work by only parsing fields that are relevant to the queries. It also breaks the dependences in state transitions of the traditional approach and thus enables the parser to parse a vector of characters in parallel with SIMD instructions.</li>\n\n\n\n<li><strong>Parquet-select</strong>. Parquet-select is an Apache Parquet reader that is up to one order of magnitude faster than the open-source Parquet reader. It enables predicate pushdown in Parquet and thus avoids expensive decompression on unnecessary compressed column values. Our techniques extensively use Bit Manipulation Instructions (BMI), a special instruction set extension of the X86 architecture, widely available in Intel/AMD CPUs.</li>\n</ul>\n\n\n\n\n\n<p> </p>\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Accelerating query processing on open data formats As businesses become more data-driven, there is an increasing interest in adopting data lakes (e.g., Microsoft Fabric) in large enterprises. A data lake is a large storage repository that stores a vast amount of data in a variety of open data formats, making it accessible for all use [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 967218,
        "date": "2023-11-08T14:36:00",
        "slug": "self-service-data-preparation",
        "title": "Self-service Data Preparation",
        "link": "https://www.microsoft.com/en-us/research/project/self-service-data-preparation/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"self-service-data-preparation\"><em>Self-service Data Preparation</em></h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>It is often cited that data scientists spend a significant portion of their time (up to 80%), cleaning and preparing data. For less-technical users, who may be less proficient in writing code (e.g., in Excel, Power-BI and Tableau), the tasks of preparing and cleaning data are not just time-consuming, but also technically challenging. </p>\n\n\n\n<p></p>\n\n\n\n<p>In the &#8220;<em>Self-service Data Preparation</em>&#8221; project, our goal is to develop technologies that can automate common data-preparation tasks, in the context of data science and business intelligence workflows. We aim to empower technical and non-technical users alike, towards the democratization of data.</p>\n\n\n\n<p></p>\n\n\n\n<p>Our research has been recognized with best paper awards at VLDB and SIGMOD. Some of our technologies have been integrated into Microsoft products such as <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://docs.microsoft.com/en-us/power-query/power-query-what-is-power-query\">Power Query<span class=\"sr-only\"> (opens in new tab)</span></a> for <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://powerbi.microsoft.com/en-us/\">Power BI<span class=\"sr-only\"> (opens in new tab)</span></a> (program synthesis, operator recommendations, fuzzy join, fuzzy deduplication), <a href=\"https://www.microsoft.com/en-us/microsoft-365/excel\">Excel<span class=\"sr-only\"> (opens in new tab)</span></a> (error detection, data cleansing), <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/dataprep/intro?view=azure-dataprep-py\">Azure Machine Learning<span class=\"sr-only\"> (opens in new tab)</span></a> (data prep sdk), <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://azure.microsoft.com/en-us/services/purview/\">Azure Purview<span class=\"sr-only\"> (opens in new tab)</span></a> (auto-tagging in data lake), <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://azure.microsoft.com/en-us/products/data-factory\">Azure Data Factory<span class=\"sr-only\"> (opens in new tab)</span></a> (fuzzy join), and <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://dynamics.microsoft.com/en-us/ai/customer-insights/\">Dynamics 365 Customer Insights<span class=\"sr-only\"> (opens in new tab)</span></a> (fuzzy join, fuzzy deduplication). </p>\n\n\n\n<p></p>\n\n\n\n<p></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>It is often cited that data scientists spend a significant portion of their time (up to 80%), cleaning and preparing data. For less-technical users, who may be less proficient in writing code (e.g., in Excel, Power-BI and Tableau), the tasks of preparing and cleaning data are not just time-consuming, but also technically challenging. In the [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 967224,
        "date": "2023-11-08T14:29:56",
        "slug": "fra-flexible-resource-allocation-in-multi-tenant-relational-database-as-a-service",
        "title": "FRA: Flexible Resource Allocation in Multi-Tenant Relational Database-as-a-Service",
        "link": "https://www.microsoft.com/en-us/research/project/fra-flexible-resource-allocation-in-multi-tenant-relational-database-as-a-service/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"fra-flexible-resource-allocation-in-multi-tenant-relational-database-as-a-service\">FRA: Flexible Resource Allocation in Multi-Tenant Relational Database-as-a-Service</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Oversubscription is an essential cost management strategy in multi-tenant, cloud Database-as-a-Service (DBaaS), and its importance is magnified by the emergence of serverless databases. In the FRA project, we have developed novel resource management techniques that enables cloud DBaaS providers to oversubscribe resources in DBaaS clusters while controlling the impact on performance and availability. The controls are realized at different levels in the cluster infrastructure: across cores within a single node, across nodes within a cluster, and across clusters within a data center.</p>\n\n\n\n<p>The concrete techniques proposed in this context were developed and validated in the context of a commercial service: Microsoft\u2019s Azure SQL Database. The project is a collaboration between Microsoft Research, the Azure SQL DB team, and has benefited from the contributions of multiple interns. A paper providing an overview of the project and key flexible resource allocation mechanisms and policies <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.vldb.org%2Fpvldb%2Fvol16%2Fp4202-narasayya.pdf&data=05%7C01%7Cviveknar%40microsoft.com%7C49ebe462cd5547bf373d08dbe0a7dd6e%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638350783799374777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=2bMWXZzwaVOkkM4xjCO5327rNkcKbsnp72SJpGydzOk%3D&reserved=0\" target=\"_blank\" rel=\"noreferrer noopener\">appears in VLDB 2024 (opens in new tab)<span class=\"sr-only\"> (opens in new tab)</span></a>. Additional publications covering specific aspects of the projects can be found in the Publications tab.</p>\n\n\n\n<p></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Oversubscription is an essential cost management strategy in multi-tenant, cloud Database-as-a-Service (DBaaS), and its importance is magnified by the emergence of serverless databases. In the FRA project, we have developed novel resource management techniques that enables cloud DBaaS providers to oversubscribe resources in DBaaS clusters while controlling the impact on performance and availability. The controls [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 982158,
        "date": "2023-11-08T12:39:02",
        "slug": "finding-adversarial-inputs-for-heuristics",
        "title": "MetaOpt: A Comprehensive Heuristic Analysis and Optimization Tool",
        "link": "https://www.microsoft.com/en-us/research/project/finding-adversarial-inputs-for-heuristics/",
        "content": "\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-100 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:100%\"><section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-gable-green card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"695\" height=\"242\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/mm9poj8f.png\" class=\"attachment-full size-full\" alt=\"MetaOpt heading\" style=\"object-position: 79% 40%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/mm9poj8f.png 695w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/mm9poj8f-300x104.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/mm9poj8f-240x84.png 240w\" sizes=\"(max-width: 695px) 100vw, 695px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"metaopta-comprehensive-heuristic-analysis-and-optimization-tool\">MetaOpt</h1>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"metaopta-comprehensive-heuristic-analysis-and-optimization-tool\">Towards efficient heuristic design where we achieve quantifiable and confident performance</h2>\n\n\n\n<p></p>\n\n\n\n<p>We use heuristics all the time across many systems including those that are critical to production services. Production systems use heuristics because they are faster or scale better than their optimal counterparts. But practitioners often don\u2019t know the performance gap between the heuristic and the optimal, or another heuristic in realistic scenarios. We present <strong>MetaOpt</strong>, a system that helps analyze heuristics.</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n</div>\n</div>\n</div>\n\n\n\n\n\n<p>We use heuristics all the time across many systems, including those that are critical to production services. Production systems use heuristics because they are faster or scale better than their optimal counterparts. But practitioners often don\u2019t know the performance gap between the heuristic and the optimal, or another heuristic in realistic scenarios.</p>\n\n\n\n<p>In many cases, theoreticians also have been unable to prove tight bounds for such heuristics: the first fit decreasing heuristic for vector bin packing is one such example &#8212; <strong>theoreticians have studied it for over 30 years and have not yet found a tight bound that worked across all problem sizes</strong> (teaser: our tool, MetaOpt, enabled us to prove a tighter bound and design a constructive proof to do so).</p>\n\n\n\n<p>A heuristic analyzer not only helps practitioners understand the performance gaps for their heuristics, but also helps them find practical situations where their heuristics underperform and deploy workarounds to mitigate their impact. They can analyze the decisions their heuristics make that cause these performance gaps and modify them to prevent problematic cases. They can also show what happens to the performance gap if we combine multiple heuristics to derive a better one. <strong>But the use cases for such a tool don\u2019t stop here.</strong></p>\n\n\n\n<blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\">\n<p class=\"has-text-align-center\">MetaOpt pioneers an area of research on <strong><em>scalable, general, and easy-to-use</em></strong> tools that enable users to analyze and explain the performance differences across competing algorithms and to improve these algorithms before they are deployed in high-stakes situations.</p>\n</blockquote>\n\n\n\n<p>We present, MetaOpt, a system that helps us analyze heuristics we have in production. MetaOpt enables the use cases mentioned earlier and much more. We use it to analyze some of the popular machine learning models that we have in production at Microsoft; we have used it as a <strong>\u201ca helper for theorem proving\u201d</strong> to prove tighter performance bounds for heuristics in domains such as vector bin packing and packet scheduling; we have used it to analyze probabilistic heuristics as well; and we are now investigating how to use it for capacity planning in our WAN.</p>\n\n\n\n<p>MetaOpt pioneers an area of research on<em> scalable, general, and easy-to-use</em> tools that enable users to analyze and explain the performance differences between competing algorithms and to help operators improve their heuristics before they deploy to production. While others have studied individual heuristics (e.g., congestion control and network traffic), none are general, provably optimal, or easy to use. MetaOpt is the first <em>general-purpose</em> and <em>scalable</em> tool that enables users to analyze a broad class of heuristics <em>through easy-to-use abstractions</em> that apply to a broad range of practical heuristics.  </p>\n\n\n\n<blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\">\n<p class=\"has-text-align-center\">We believe MetaOpt can have significant benefits as <strong>productivity booster</strong> for those designing/studying heuristics; as a <strong>risk-analysis-engine</strong> for evaluating heuristics before we deploy them in production; and as a <strong>tool for explainable-AI and active learning.</strong></p>\n</blockquote>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"how-do-people-use-metaopt-1\">How do people use MetaOpt?</h2>\n\n\n\n<p>To analyze their heuristics, users specify the heuristic and the optimal (or another heuristic) as inputs to MetaOpt, and MetaOpt automatically encodes these efficiently as inputs to a solver to find performance gaps and adversarial inputs that cause them. Not all users know optimization theory; we have designed a higher-level abstraction for MetaOpt that allows users to input their heuristics using a few simple building blocks. Using this abstraction, we are also able to search through <strong>decisions the heuristic makes that cause it to underperform or properties of the inputs that trigger the heuristic to make bad decisions</strong>.</p>\n\n\n\n<div class=\"wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile has-light-gray-background-color has-background\" data-bi-an=\"media-text\"><figure class=\"wp-block-media-text__media\"><img loading=\"lazy\" decoding=\"async\" width=\"887\" height=\"158\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MetaOpt_structure_2.png\" alt=\"MetaOpt structure\" class=\"wp-image-982782 size-full\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MetaOpt_structure_2.png 887w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MetaOpt_structure_2-300x53.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MetaOpt_structure_2-768x137.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MetaOpt_structure_2-240x43.png 240w\" sizes=\"(max-width: 887px) 100vw, 887px\" /></figure><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<p>The MetaOpt workflow involves 4 steps: (1) users encode the heuristic; (2) MetaOpt automatically does re-writes to obtain a single-level optimization; (3) it partitions the problem into smaller sub-problems to achieve scale; (4) it uses existing solvers to find the highest performance gap.</p>\n</div></div>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"how-does-metaopt-work\">How does MetaOpt work?</h2>\n\n\n\n<p>MetaOpt is based on the notion of Stackelberg games \u2013 a well known class of leader-follower games in game theory. In such games, a leader maximizes their payoff and controls the inputs to one or more followers. With this <em>fixed </em>input from the leader, the followers have to respond and optimize for their own payoff and decide values for the variables in their control, which the leader does not control but which influences their payoff. In the context of MetaOpt, the leader maximizes the performance difference between two followers and decides the inputs to these followers to achieve this goal; the followers are algorithms that we want to compare and decide internal variables which in-turn influence the leader&#8217;s payoff.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"784\" height=\"269\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MetaOpt_formulation.png\" alt=\"MetaOpt formulation\" class=\"wp-image-982191\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MetaOpt_formulation.png 784w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MetaOpt_formulation-300x103.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MetaOpt_formulation-768x264.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/MetaOpt_formulation-240x82.png 240w\" sizes=\"(max-width: 784px) 100vw, 784px\" /></figure>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"so-are-we-done-1\">So are we done?</h2>\n\n\n\n<p>Not even close. Stackelberg games apply to a broad class of leaders and followers. Our current realization of MetaOpt is based on a subset of these games that we can represent and solve through bi-level optimization (these cover heuristics we can represent as a feasibility or convex optimization problem which already covers a broad range of heuristics in packet scheduling, traffic engineering, bin-packing, and many more). We plan to extend MetaOpt and add support for other types of heuristics that do not fit into this paradigm.</p>\n\n\n\n<p>The MetaOpt Team is investigating these continuations to the work and is extending MetaOpt to enable new usecases. Who knows, we may even design a GPT-based plug-in for it \ud83d\ude09</p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"why-did-we-build-metaopt\">Why did we build MetaOpt?</h2>\n\n\n\n<p>We started the MetaOpt in the spring of 2022 in response to a production need: to analyze one of the heuristics deployed in our Wide Area Network\u2019s traffic engineering solution. The team then focused on enabling users who do not have a background in optimization theory to model their use cases in MetaOpt. We are currently working on improving MetaOpt\u2019s ability to scale to larger problem instances, improve its usability, extend the types of heuristics it can support, and open sourcing it to the community.</p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"who-are-we\">Who are we?</h2>\n\n\n\n<p>The core team that works on MetaOpt in Microsoft are part of the Networking Research and Azure for Operators groups. We are: Behnaz Arzani, Ryan Beckett, Siva Kakarla, and Srikanth Kandula. MetaOpt was done in collaboration with Santiago Segarra from Rice University and our interns Pooria Namyar from USC and Solal Pirelli from EPFL.</p>\n\n\n\n<p>We collaborate extensively with production teams to help analyze heuristics we have in production. Specifically, we work with Elnaz Jalilipour, Sina Taheri, Himanshu Raj, and Umesh Krishnaswamy.</p>\n\n\n\n<p>Rodrigo Fonseca, Daniel Berger, Ankur Mallick, and Kevin Hsieh collaborate with us on using MetaOpt for explainable AI.</p>\n\n\n\n<p>The work on MetaOpt is the latest in a series of tools our team has designed to help analyze deployed heuristics. Our prior work in this space includes: Aragog (a runtime verifier for distributed network functions); GRoot (the first verifier for DNS zone-files), FixRoute (a scalable, optimization-based, solution to ensuring loop-free routing), among others. </p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"who-should-i-contact-if-i-have-a-use-case-metaopt-can-help-with\">Who should I contact if I have a use-case MetaOpt can help with? </h2>\n\n\n\n<p>Behnaz Arzani (bearzani@microsoft.com) and Pooria Namyar (Namyar@usc.edu) are the main contacts for the MetaOpt project.</p>\n\n\n\n\n\n<h2 class=\"wp-block-heading\" id=\"do-you-have-examples-where-you-used-metaopt\">Do you have examples where you used MetaOpt?</h2>\n\n\n\n<p>Here are some example use-cases and results:</p>\n\n\n\n<p><strong>Comparing Heuristics in Traffic Engineering. </strong>We used MetaOpt to find the performance gaps for demand pinning and POP where we compare them to the optimal multi-commodity flow algorithm. We compute the performance gap &#8212; the difference between the heuristic and the optimal which we normalize by the total network capacity. The performance gap is a lower bound on the optimality gap, the worst-case gap between the two. </p>\n\n\n\n<p>We find the demand pinning (DP) heuristic Microsoft uses for WAN traffic engineering &nbsp;and POP incur 33.9% and 20% relative performance gaps on a large topology. This means there exists (and we can find) adversarial traffic demands that cause the demand pinning heuristic we use in Microsoft to use 33.9% more capacity compared to optimal. Network operators that use DP may need to over-provision the network by that much to satisfy this demand. MetaOpt by default, searches for adversarial demands among all possible demands.</p>\n\n\n\n<p>We can constrain MetaOpt to search over realistic demands. These exhibit temporal locality where few node pairs communicate. The gap for PoP and DP reduces by less than 1% when we run MetaOpt with this constraint.</p>\n\n\n\n<p><strong>Comparing heuristics in Packet scheduling.</strong> We compared the packet scheduling algorithm SP-PIFO to PIFO. We compute and compare the priority-weighted average packet delay between the two algorithms which penalizes them if they increase the delay of high-priority packets.</p>\n\n\n\n<p>MetaOpt shows there exists an input packet sequence where SP-PIFO is 3X worse than PIFO. We used MetaOpt to compare SP-PIFO and AIFO (two heuristics). AIFO emulates PIFO through a single FIFO queue. MetaOpt finds inputs for which AIFO incurs 6X more priority inversions than SP-PIFO. Such analyses can help designers weigh performance trade-offs against switch resource usage.</p>\n\n\n\n<p><strong>Proving properties of and improving heuristics.</strong> We used MetaOpt to find adversarial inputs for various heuristics and analyzed these inputs to prove performance bounds for these heuristics or to improve them.</p>\n\n\n\n<p><em>We proved a new bound for vector bin-packing.</em> Vector bin packing (VBP) heuristics try to minimize the number of bins they use. Theoreticians prove bounds on their approximation ratio: the worst-case ratio, over any input, of the number of bins the heuristic uses compared to the optimal. </p>\n\n\n\n<p>A heuristic&#8217;s approximation ratio is the worst-case ratio, over any input, of the number of bins needed for the heuristic to that needed for the optimal. Recent work showed 2-dimensional FFDSum (which is a greedy heuristic that scores and sorts balls based on the sum across all of their dimensions and adds them to the first bin they can fit in) asymptotically approaches an approximation ratio of 2 (where the optimal uses nearly infinite bins). We proved the approximation ratio is always at least 2&#8212;even when the optimal requires a finite number of bins!</p>\n\n\n\n<p><em>Proving a new bound for packet scheduling and improving the heuristic.</em> We analyzed adversarial inputs MetaOpt found for SP-PIFO and proved a lower bound on its priority-weighted average delay relative to PIFO. The bound is a function of the priority range and the number of packets.</p>\n\n\n\n<p>Adversarial inputs to SP-PIFO trigger priority inversions, which queue high priority packets behind low priority ones. We tested a Modified-SP-PIFO, which splits queues into groups; we assign each group a priority range, and run SP-PIFO on each group independently. Modified-SP-PIFO reduces the performance gap of SP-PIFO by 2.5X.</p>\n\n\n\n<p>We have also used MetaOpt in many other contexts such as WAN capacity planning, analyzing machine learning models, analyzing caching algorithms, and many other use cases. We will publish soon.</p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"who-do-i-contact-with-questions\">Who do I contact with questions?</h2>\n\n\n\n<p>Behnaz Arzani (bearzani@microsoft.com) and Pooria Namyar (Namyar@usc.edu) are the main contacts for the MetaOpt project.</p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>We use heuristics all the time across many systems including those that are critical to production services. Production systems use heuristics because they are faster or scale better than their optimal counterparts. But practitioners often don\u2019t know the performance gap between the heuristic and the optimal, or another heuristic in realistic scenarios. We present MetaOpt, [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 981045,
        "date": "2023-11-07T12:55:00",
        "slug": "mosaic",
        "title": "Mosaic",
        "link": "https://www.microsoft.com/en-us/research/project/mosaic/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-grey card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/mosaic_bannerFinal2.jpg\" class=\"attachment-full size-full\" alt=\"Mosaic Faces\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/11/mosaic_bannerFinal2.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/mosaic_bannerFinal2-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/mosaic_bannerFinal2-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/mosaic_bannerFinal2-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/mosaic_bannerFinal2-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/mosaic_bannerFinal2-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/mosaic_bannerFinal2-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"project-mosaic\">Project MOSAIC</h1>\n\n\n\n<p>A Generative AI experience designed to capture and dynamically display public discourse around AI as Art.</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<h4 class=\"wp-block-heading\" id=\"vision\">Vision</h4>\n\n\n\n<p>AI is evolving incredibly fast and its impact on society promises to be as great as its one-hundred-year predecessor, the Industrial Revolution. As AI rapidly evolves and seeps into daily life, so does our experience with it. People\u2019s perceptions of AI, and how they see it changing their lives today, is a critical area of inquiry for researchers. Our challenge is that traditional methods of surveys are often static, less inclusive, and ill-suited to accurately capture the breadth and depth of the AI event. </p>\n\n\n\n<p>To address this issue, we introduce Project Mosaic &#8211; a Generative AI experience designed to capture and dynamically display public discourse around AI. It implicitly acts as an active visual barometer while also sampled with other input metrics (economic, sociopolitical) can serve to infer the collective prediction helping to measure and shape societal impact of AI.</p>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"experience\">Experience</h4>\n\n\n\n<p>Mosaic leverages the speed and creativity of Generative AI to elevate and highlight narratives around public sentiment while promoting a more inclusive experience. It encourages the public to engage with it by answering survey questions and seeing their response reflected as responsive art. The framework acts as a living survey model that poses questions over time creating new data verticals and generative experiences for ongoing societal engagement and research. It\u2019s novel approach to visualize an individual\u2019s response using AI and tell a visual story via the interactive Mosaic experience that we think will create a unique public display of interactive Art. </p>\n\n\n\n<p></p>\n\n\n\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"Project Mosaic\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/AHEI3rjt-3c?feature=oembed&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</div></figure>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"technical-approach\">Technical approach</h4>\n\n\n\n<p>The technical approach involves a modular architecture that leverages Azure cloud services for scalability and reliability. The system consists of a responsive web-based front-end user experience, a serverless back-end API implementing an AI orchestrator, and a scalable data storage layer. At its core, the orchestrator is responsible for coordinating and executing multiple AI services performing sentiment analysis and art generation. </p>\n\n\n\n<p>To support research experimentation the solution is extensible in the three areas of visualization, AI models and data storage. Visualizations are driven by a data-API offering scalable content delivery (CDN) of large volumes of artwork and metadata artifacts for interactive (near-)real-time rendering and exploration. The orchestrator leverages a plug and play mechanism (e.g., Semantic Kernel) coordinating multiple multi-modal sentiment extraction as well as image generation agents. The storage layer supports easy addition of new data points without requiring significant changes to the underlying information architecture through document-oriented storage paired with Azure storage.</p>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"public-beta-coming-soon-dec-2023\">Public beta: Coming soon (Dec 2023)</h4>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>A Generative AI experience designed to capture and dynamically display public discourse around AI as Art. AI is evolving incredibly fast and its impact on society promises to be as great as its one-hundred-year predecessor, the Industrial Revolution. As AI rapidly evolves and seeps into daily life, so does our experience with it. People\u2019s perceptions [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 979767,
        "date": "2023-10-26T06:05:45",
        "slug": "ai-and-the-future-of-work-in-africa",
        "title": "AI and the Future of Work in Africa",
        "link": "https://www.microsoft.com/en-us/research/project/ai-and-the-future-of-work-in-africa/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"2560\" height=\"1707\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MSC21_PICHA_mobile_35124-scaled.jpg\" class=\"attachment-full size-full\" alt=\"Young African man in a car holding a mobile phone and laughing\" style=\"object-position: 37% 44%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MSC21_PICHA_mobile_35124-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MSC21_PICHA_mobile_35124-300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MSC21_PICHA_mobile_35124-1024x683.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MSC21_PICHA_mobile_35124-768x512.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MSC21_PICHA_mobile_35124-1536x1024.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MSC21_PICHA_mobile_35124-2048x1365.jpg 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MSC21_PICHA_mobile_35124-240x160.jpg 240w\" sizes=\"(max-width: 2560px) 100vw, 2560px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"creating-a-dignified-future-of-work-for-all\">Creating a dignified future of work for all</h1>\n\n\n\n<p>Bringing together thought-leaders from across the continent to discuss the opportunities and challenges presented by generative AI in the future of work in Africa </p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>A workshop organized by the Africa Oxford Initiative at the University of Oxford; African Union Development Agency NEPAD, Center for the Future of Work, University of Pretoria, IBM Research Africa, LELAPA AI, Microsoft Africa Research Institute (MARI) and Microsoft East Africa. The workshop will bring together thought leaders, such as yourself, from across the continent to explore the impact of generative AI on the future of work in Africa, and to identify the skills, policies, and design principles that can support dignified work on the continent.</p>\n\n\n\n<p>The workshop will address the following topics:</p>\n\n\n\n<ul>\n<li>Macroeconomics: How will generative AI affect the economic growth, trade, and development of African countries?</li>\n\n\n\n<li>Jobs, labour markets and skills: How will generative AI transform the demand and supply of labour, the quality and quantity of jobs, and the skills and education needed for the workforce?</li>\n\n\n\n<li>Workers&#8217; perspectives on AI: How can AI be designed for worker agency and empowerment? What new interaction mechanisms will ensure seamless interaction with AI to support rather than constrain or disrupt ongoing work?&nbsp;</li>\n\n\n\n<li>Africa-centric AI tools and platforms: How can generative AI be developed and deployed in a way that is responsive to the cultures, needs, values, and contexts of African workers and communities?</li>\n</ul>\n\n\n\n<div style=\"height:15px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<p>We are delighted to introduce these amazing keynote speakers:</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"538\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Bitange-Ndemo-1-1024x538.png\" alt=\"Flyer for Prof Bitange Ndemo's talk\" class=\"wp-image-980508\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Bitange-Ndemo-1-1024x538.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Bitange-Ndemo-1-300x158.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Bitange-Ndemo-1-768x403.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Bitange-Ndemo-1-1536x807.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Bitange-Ndemo-1-2048x1075.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Bitange-Ndemo-1-240x126.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:15px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"538\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Prof-Landre-Signe-1024x538.png\" alt=\"Flyer for Prof Landre Signe's talk\" class=\"wp-image-980493\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Prof-Landre-Signe-1024x538.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Prof-Landre-Signe-300x158.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Prof-Landre-Signe-768x403.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Prof-Landre-Signe-1536x807.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Prof-Landre-Signe-2048x1076.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AI-and-FOW-Prof-Landre-Signe-240x126.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:15px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Bringing together thought-leaders from across the continent to discuss the opportunities and challenges presented by generative AI in the future of work in Africa.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 979650,
        "date": "2023-10-26T00:51:47",
        "slug": "science-of-ai",
        "title": "Science of AI",
        "link": "https://www.microsoft.com/en-us/research/project/science-of-ai/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-purple card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"science-of-ai\">Science of AI</h1>\n\n\n\n<p>Design, analysis and interpretability of large language models</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Transformers and large language models (LLMs) have had enormous success in recent years. Yet they remain poorly understood, in particular why and how they work. We are trying to answer such questions using tools such as mathematical analysis and mechanistic interpretability. One area where these models perform poorly is Continual Learning. We are working on alternate solutions, for example based on models such as biological neural networks. </p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Design, analysis and interpretability of large language models Transformers and large language models (LLMs) have had enormous success in recent years. Yet they remain poorly understood, in particular why and how they work. We are trying to answer such questions using tools such as mathematical analysis and mechanistic interpretability. One area where these models perform [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 979587,
        "date": "2023-10-25T21:34:34",
        "slug": "kahani",
        "title": "Kahani",
        "link": "https://www.microsoft.com/en-us/research/project/kahani/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Kahani_header_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"Project Kahani header image - young boy looking mesmerized by the camera\" style=\"object-position: 64% 36%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Kahani_header_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Kahani_header_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Kahani_header_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Kahani_header_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Kahani_header_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Kahani_header_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Kahani_header_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading is-style-default\" id=\"kahani-visual-storytelling\">Kahani: Visual Storytelling</h1>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Image generation models like MidJourney, DALL-E 3, and SDXL, have made remarkable progress recently to produce visually stunning images from natural language descriptions. However, most of these models are limited by their lack of cultural awareness and diversity, and often fail to capture the subtle nuances and variations that exist in different cultures and languages.</p>\n\n\n\n<p>To generate an image that matches the user\u2019s expectations and preferences, one has to either provide extensive prompts and edit the output using sophisticated tools like Adobe Photoshop or fine-tune a model using large amounts of data and skills. Both of these approaches are time-consuming, costly, and inaccessible for most people.</p>\n\n\n\n<p><strong>Kahani: Visual Storytelling</strong> is a research prototype that allows the user to create visually striking and culturally nuanced images just by describing them in their local languages. Kahani leverages state-of-the-art techniques like Inpainting, and models like Segment Anything and GPT-4 vision to generate feedback for the candidate images.</p>\n\n\n\n<div class=\"wp-block-media-text has-video  has-vertical-padding-none  is-stacked-on-mobile is-style-border is-style-offset-media--top is-style-offset-media--offset-\" data-bi-an=\"media-text\"><figure class=\"wp-block-media-text__media video-wrapper\"><iframe class=\"media-text__video\" src=\"https://www.youtube-nocookie.com/embed/uXyt_E2_myA?enablejsapi=1&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></figure><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<h3 class=\"wp-block-heading\" id=\"our-goal\">Our goal?</h3>\n\n\n\n<p>To democratize image generation and empower users to express their creativity and identity through visual storytelling.</p>\n</div></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Kahani: Visual Storytelling is a research prototype that allows the user to create visually striking and culturally nuanced images just by describing them in their local languages.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 918261,
        "date": "2023-10-25T20:54:47",
        "slug": "work-well-being",
        "title": "Work & well-being",
        "link": "https://www.microsoft.com/en-us/research/project/work-well-being/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/working-from-home.jpg\" class=\"attachment-full size-full\" alt=\"woman attending a remote meeting\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/working-from-home.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/working-from-home-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/working-from-home-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/working-from-home-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/working-from-home-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/working-from-home-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/working-from-home-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/working-from-home-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/working-from-home-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/working-from-home-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/working-from-home-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/covid-19-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"COVID-19 Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tCOVID-19 Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"work-well-being\">Work & well-being</h2>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>The COVID-19 pandemic changed the lives of people around the world at home and at work, with effects lasting beyond the lockdowns. Along with Microsoft\u2019s New Future of Work Report 2022, this research looks at the impact of remote work on the personal productivity and well-being of Microsoft\u2019s own employees, the effects of remote work on cross-group collaboration, how working from home affects work relationships, how the pandemic has affected farmers and farmworkers and exploring social talk and remote collegiality in video conferencing.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-101 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/microsoft-new-future-of-work-report-2022/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Covid-Work_6-1_1400x788-1024x576.jpg\" alt=\"Microsoft New Future of Work Report 2022\u200b\" class=\"wp-image-926457\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Covid-Work_6-1_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Covid-Work_6-1_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Covid-Work_6-1_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Covid-Work_6-1_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Covid-Work_6-1_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Covid-Work_6-1_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Covid-Work_6-1_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Covid-Work_6-1_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Covid-Work_6-1_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Covid-Work_6-1_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Covid-Work_6-1_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"microsoft-new-future-of-work-report-2022\"><a href=\"https://www.microsoft.com/en-us/research/publication/microsoft-new-future-of-work-report-2022/\">Microsoft New Future of Work Report 2022</a></h4>\n\n\n\n<p>The pandemic&#8217;s remote work experiment has caused work to change faster than it has in a generation. As people return to work and experiment with hybrid work, Microsoft and other researchers have been investigating evolving hybrid work practices and developing technologies to address new challenges and opportunities. This report summarizes recent research related to hybrid work, highlights themes that have emerged, and encourages knowledge sharing. The purpose of the report is to help the community build on what has been learned through the pandemic and create a new future of work that is meaningful, productive, and equitable.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/the-effects-of-remote-work-on-collaboration-among-information-workers/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-2_1400x788-1024x576.jpg\" alt=\"chart showing the amount of time Microsoft employees spent collaborating to drop by 25% after Microsoft implemented a work-from-home mandate in March 2020\" class=\"wp-image-926460\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-2_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-2_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-2_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-2_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-2_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-2_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-2_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-2_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-2_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-2_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-2_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"the-effects-of-remote-work-on-collaboration-among-information-workers\"><a href=\"https://www.microsoft.com/en-us/research/publication/the-effects-of-remote-work-on-collaboration-among-information-workers/\">The effects of remote work on collaboration among information workers</a></h4>\n\n\n\n<p>Before the pandemic, no more than 5% of Americans worked from home more than three days per week. To understand the impacts of working remotely, Microsoft examined its employees\u2019 work patterns before and after its company-wide work-from-home mandate in March 2020. It found that remote work caused the time employees spent collaborating across groups to drop by about 25%. This suggests that companies will need to proactively help workers acquire and share new information across groups, so productivity and innovation are not impacted.</p>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-102 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/what-a-year-of-wfh-has-done-to-our-relationships-at-work/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-3_1400x788-1024x576.jpg\" alt=\"man holding a flashlight camping in small tent in the middle of an empty field \" class=\"wp-image-926466\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-3_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-3_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-3_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-3_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-3_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-3_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-3_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-3_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-3_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-3_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-3_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"the-impacts-of-working-from-home-on-work-relationships\"><a href=\"https://www.microsoft.com/en-us/research/publication/what-a-year-of-wfh-has-done-to-our-relationships-at-work/\">The impacts of working from home on work relationships</a></h4>\n\n\n\n<p>After over a year of the COVID-19 pandemic and enforced remote work, teams across Microsoft conducted over 50 studies to understand how the nature of work itself has changed since early 2020. Microsoft\u2019s annual Work Trend Index is part of this initiative and includes an analysis of trillions of productivity signals \u2014 think emails, meetings, chats, and posts \u2014 across Microsoft and LinkedIn\u2019s user base. It also includes a survey of more than 30,000 people in 31 countries. The research showed employees feeling isolated, and teams becoming much more siloed. No gatherings or chance encounters meant connections outside one\u2019s team shrank, with fewer interactions around innovative ideas and opportunities to build social capital. This research suggests helpful approaches for workers and leaders.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/bridging-social-distance-during-social-distancing-exploring-social-talk-and-remote-collegiality-in-video-conferencing/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/virtual-meeting_1400x788-1024x576.jpg\" alt=\"woman wearing a headset on a virtual video meeting\" class=\"wp-image-926472\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/virtual-meeting_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/virtual-meeting_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/virtual-meeting_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/virtual-meeting_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/virtual-meeting_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/virtual-meeting_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/virtual-meeting_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/virtual-meeting_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/virtual-meeting_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/virtual-meeting_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/virtual-meeting_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"meeting-the-pandemic-videoconferencing-fatigue-and-evolving-tensions-of-sociality-in-video-meetings-during-covid-19\"><a href=\"https://www.microsoft.com/en-us/research/publication/meeting-the-pandemic-videoconferencing-fatigue-and-evolving-tensions-of-sociality-in-enterprise-video-meetings-during-covid-19/\">Meeting (the) Pandemic: Videoconferencing Fatigue and Evolving Tensions of Sociality in Video Meetings During COVID-19</a></h4>\n\n\n\n<p>When COVID-19 led to mandatory working from home, significant blind spots in supporting the sociality of working life were revealed in video meetings. This study of a global technology company\u2019s employees\u2019 experiences of all-remote video meetings during the COVID-19 pandemic explored the tensions expressed by employees around effectiveness and sociality, as well as their strategies to cope with these tensions. We argue that videoconferencing fatigue arose partly due to work practices and technologies designed with assumptions of steady states and taken-for-granted balances between task and social dimensions of work relationships. Our analysis offers a social lens on videoconferencing fatigue and suggests the need to reconceptualize ideas around designing technologies and practices to enable both effectiveness and sociality in the context of video meetings.</p>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-103 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/farmer-and-farm-worker-illnesses-and-deaths-from-covid-19-and-impacts-on-agricultural-output/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-5_1400x788-1024x576.jpg\" alt=\"line chart showing the number of farmer and farm worker illnesses and deaths from COVID-19 and the impact on agricultural output\" class=\"wp-image-926484\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-5_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-5_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-5_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-5_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-5_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-5_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-5_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-5_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-5_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-5_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Work_6-5_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"farmer-and-farm-worker-illnesses-and-deaths-from-covid-19-and-impacts-on-agricultural-output\"><a href=\"https://www.microsoft.com/en-us/research/publication/farmer-and-farm-worker-illnesses-and-deaths-from-covid-19-and-impacts-on-agricultural-output/\">Farmer and farm worker illnesses and deaths from COVID-19 and impacts on agricultural output</a></h4>\n\n\n\n<p>Farmers and farm workers are critical to the secure supply of food, yet this population is potentially at high risk to acquire COVID-19. This study estimated the prevalence of COVID-19 among U.S. farmers and farmworkers in the United States by coupling farm worker numbers relative to the general population with data on confirmed COVID-19 cases and deaths. It found the COVID-19 incidence rate is significantly higher in counties with more agricultural workers. Reduced labor availability from COVID-19 is estimated to reduce U.S. agricultural output by about $309 million.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Managing-Tasks-Work-Life-Boundary_1400x788-1024x576.jpg\" alt=\"COVID research - Managing Tasks Across the Work-Life Boundary: Opportunities, Challenges, and Directions\" class=\"wp-image-944454\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Managing-Tasks-Work-Life-Boundary_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Managing-Tasks-Work-Life-Boundary_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Managing-Tasks-Work-Life-Boundary_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Managing-Tasks-Work-Life-Boundary_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Managing-Tasks-Work-Life-Boundary_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Managing-Tasks-Work-Life-Boundary_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Managing-Tasks-Work-Life-Boundary_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Managing-Tasks-Work-Life-Boundary_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Managing-Tasks-Work-Life-Boundary_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Managing-Tasks-Work-Life-Boundary_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Managing-Tasks-Work-Life-Boundary_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"managing-tasks-across-the-work-life-boundary-opportunities-challenges-and-directions\"><a href=\"https://www.microsoft.com/en-us/research/publication/managing-tasks-across-the-work-life-boundary-opportunities-challenges-and-directions-2/\">Managing Tasks Across the Work-Life Boundary: Opportunities, Challenges, and Directions</a></h4>\n\n\n\n<p>The COVID-19 pandemic has led to a shift in how people manage their work and personal tasks. This study examines and probes the practices of for managing task-related information across the work-life boundary during the COVID-19 pandemic. The study surveyed 150 information workers in Summer 2019 (i.e., pre-pandemic) and 70 from the same organization in Summer 2020 (i.e., mid-pandemic). Across both survey cohorts, cross-boundary task management practices are characterized, exploring the central role that physical and digital tools play in managing task-related information that arises at inopportune times. The study concludes by discussing the opportunities and challenges for future productivity tools that aid people in managing task-related information across their personal and work contexts.</p>\n</div>\n</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>The COVID-19 pandemic changed the lives of people around the world at home and at work, with effects lasting beyond the lockdowns. Along with Microsoft\u2019s New Future of Work Report 2022, this research looks at the impact of remote work on the personal productivity and well-being of Microsoft\u2019s own employees, the effects of remote work [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 918255,
        "date": "2023-10-25T20:53:06",
        "slug": "global-response-information",
        "title": "Global response & information",
        "link": "https://www.microsoft.com/en-us/research/project/global-response-information/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/handshake.jpg\" class=\"attachment-full size-full\" alt=\"two men shaking hands over a table surrounded by other people talking amongst one another\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/handshake.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/handshake-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/handshake-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/handshake-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/handshake-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/handshake-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/handshake-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/handshake-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/handshake-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/handshake-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/handshake-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/covid-19-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"COVID-19 Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tCOVID-19 Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"global-response-information\">Global response & information</h2>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>The COVID-19 pandemic has affected nearly every aspect of life around the world, from healthcare and economics to diet and social needs. As a result, researchers have turned to a variety of methods to understand the impacts of the pandemic and inform policies and recovery efforts. These methods include analyzing internet search data to track shifts in human needs and dietary interests, using self-supervised learning to improve vertical search in the biomedical literature, and studying the prevalence and impact of misinformation related to COVID-19. By applying these methods, researchers are gaining insights into the complex and evolving effects of the pandemic on individuals and societies.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-104 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/digital-health-covid-19-impact-assessment-lessons-learned-and-compelling-needs/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/digital-health_1400x788.jpg\" alt=\"patient in a hospital bed being attended to by nurse with a doctor outside the room on a video appointment\" class=\"wp-image-926244\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/digital-health_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/digital-health_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/digital-health_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/digital-health_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/digital-health_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/digital-health_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/digital-health_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/digital-health_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/digital-health_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/digital-health_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/digital-health_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"digital-health-covid-19-impact-assessment-lessons-learned-and-compelling-needs\"><a href=\"https://www.microsoft.com/en-us/research/publication/digital-health-covid-19-impact-assessment-lessons-learned-and-compelling-needs/\">Digital health COVID-19 impact assessment: Lessons Learned and Compelling Needs</a></h4>\n\n\n\n<p>The adoption of digital health technologies, such as electronic medical records, telemedicine, and artificial intelligence, has transformed the healthcare industry over the past decade. These technologies are allowing for a more people-centered, collaborative approach to continuous health and wellness, with the potential to provide more holistic care and address social equity issues. The COVID-19 pandemic has accelerated the adoption of telemedicine and other digital health solutions but has also highlighted the need for a coherent and accessible data infrastructure. Despite progress made in digitizing healthcare information, critical data sources are not yet ready for use, which can hinder the effectiveness of digital technologies in delivering care. This discussion paper is part of the National Academy of Medicine\u2019s Emerging Stronger\u202fCOVID-19: Priorities for Health System Transformation initiative.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/disparate-impacts-on-online-information-access-during-the-covid-19-pandemic/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-2_1400x788.jpg\" alt=\"three illustrations of the united states - one with grey concentration spots, one with red concentration spots, and one with black concentration spots\" class=\"wp-image-926250\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-2_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-2_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-2_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-2_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-2_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-2_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-2_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-2_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-2_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-2_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-2_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"differences-by-community-in-enlisting-online-resources-during-the-covid-19-pandemic\"><a href=\"https://www.microsoft.com/en-us/research/publication/disparate-impacts-on-online-information-access-during-the-covid-19-pandemic/\">Differences by community in enlisting online resources during the COVID-19 pandemic</a></h4>\n\n\n\n<p>The COVID-19 pandemic has increasingly pushed people to rely on online resources for health, economic, and educational needs. A study of 55 billion web search interactions across 25,150 US ZIP codes during the pandemic showed that the extent to which different communities enlisted digital resources varied depending on socioeconomic and environmental factors. It found that lower income ZIP codes intensified their access to health information to a smaller extent than ZIP codes with higher incomes, and that ZIP codes with higher proportions of Black or Hispanic residents intensified their access to unemployment resources to a greater extent. These differences raise important questions about real-world implications of differential information search behaviors.</p>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-105 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/population-scale-study-of-human-needs-during-the-covid-19-pandemic-analysis-and-implications/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-3_1400x788.jpg\" alt=\"scatter plot trend map with markers for various states\" class=\"wp-image-926256\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-3_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-3_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-3_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-3_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-3_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-3_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-3_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-3_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-3_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-3_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-3_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-scale-study-of-human-needs-during-the-covid-19-pandemic-based-on-maslow-s-hierarchy\"><a href=\"https://www.microsoft.com/en-us/research/publication/population-scale-study-of-human-needs-during-the-covid-19-pandemic-analysis-and-implications/\">Population-scale study of human needs during the COVID-19 pandemic based on Maslow\u2019s hierarchy</a></h4>\n\n\n\n<p>To mitigate the pandemic\u2019s effects, decision-makers need to consider its broader impact on people and their needs. A new computational framework, based on Maslow&#8217;s hierarchy of human needs, offers a holistic view. It was applied to characterize changes in human needs across physiological, socioeconomic, and psychological realms, based on over 35 billion search interactions across 36,000 US ZIP codes. It revealed an exponential increase in the expression of basic human needs while higher-level aspirations declined. It showed that potential barriers to addressing needs such as support for unemployment and domestic violence can be identified through web search interactions. It suggests that population-scale monitoring of shifts in human needs can inform policies and recovery efforts.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/population-scale-dietary-interests-during-the-covid-19-pandemic/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-4_1400x788.jpg\" alt=\"chart showing the population shift away from restaurants to recipes made and consumed at home during COVID-19\" class=\"wp-image-926259\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-4_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-4_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-4_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-4_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-4_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-4_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-4_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-4_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-4_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-4_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-4_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-scale-dietary-interests-during-the-covid-19-pandemic\"><a href=\"https://www.microsoft.com/en-us/research/publication/population-scale-dietary-interests-during-the-covid-19-pandemic/\">Population-scale dietary interests during the COVID-19 pandemic</a></h4>\n\n\n\n<p>This research examines population-wide shifts in dietary interests during the pandemic, as revealed through time series of Google search volumes in 18 countries in 2020. It found that the shock of decreased mobility manifested as a drastic increase in interest in consuming food at home and a corresponding decrease in consuming food outside of home. The largest increases occurred for calorie-dense carbohydrate-based foods such as pastries, bakery products, bread, and pies. The observed shifts in dietary interests have the potential to globally affect food consumption and health outcomes.</p>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-106 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/domain-specific-pretraining-for-vertical-search-case-study-on-biomedical-literature/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-5_1400x788.jpg\" alt=\"flowchart showing how Domain-Specific Pretraining works\" class=\"wp-image-926268\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-5_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-5_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-5_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-5_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-5_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-5_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-5_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-5_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-5_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-5_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Response_2-5_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"vertical-biomedical-literature-searches-using-domain-specific-pertaining\"><a href=\"https://www.microsoft.com/en-us/research/publication/domain-specific-pretraining-for-vertical-search-case-study-on-biomedical-literature/\">Vertical biomedical literature searches using domain-specific pertaining</a></h4>\n\n\n\n<p>Biomedical literature on COVID-19 has swelled to over a million new papers every year, making searches in the biomedical and many other vertical domains challenging. Self-supervised learning has emerged as a promising direction to overcome the annotation bottleneck. This research suggests a general approach for vertical search based on domain-specific pretraining and presents a case study for the biomedical domain. This method performs comparably or better than the best systems in an official COVID-19-related biomedical search competition. Using distributed computing, it can scale to tens of millions of articles.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/COVID-research_Telehealth-Role_1400x788-1024x576.jpg\" alt=\"a female doctor sitting at a table using a laptop computer for a teleconference\" class=\"wp-image-944460\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/COVID-research_Telehealth-Role_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/COVID-research_Telehealth-Role_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/COVID-research_Telehealth-Role_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/COVID-research_Telehealth-Role_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/COVID-research_Telehealth-Role_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/COVID-research_Telehealth-Role_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/COVID-research_Telehealth-Role_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/COVID-research_Telehealth-Role_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/COVID-research_Telehealth-Role_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/COVID-research_Telehealth-Role_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/COVID-research_Telehealth-Role_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"telehealth-s-changing-role-during-covid-19-insights-from-urban-india\"><a href=\"https://www.microsoft.com/en-us/research/publication/infrastructuring-telehealth-in-informal-patient-doctor-contexts/\">Telehealth&#8217;s Changing Role During COVID-19: Insights from Urban India</a></h4>\n\n\n\n<p>The COVID-19 pandemic has made telehealth technologies a crucial means of providing non-COVID-19, non-emergency care. A qualitative analysis of 181 survey responses and 18 interviews with doctors and patients in urban India reveals the infrastructural changes that supported this transition. The study analyzed the technology infrastructures in place and being created, the changes in consultation practices required for doctors and patients to adapt to teleconsultations, and the shifting dynamics of the human infrastructures that enable healthcare. The findings suggest that teleconsultations have the potential to provide inclusive and equitable care in the future. The study highlights implications for the design of telehealth infrastructures to better support healthcare in a post-pandemic world.</p>\n</div>\n</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>The COVID-19 pandemic has affected nearly every aspect of life around the world, from healthcare and economics to diet and social needs. As a result, researchers have turned to a variety of methods to understand the impacts of the pandemic and inform policies and recovery efforts. These methods include analyzing internet search data to track [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 918231,
        "date": "2023-10-25T20:51:53",
        "slug": "forecasting-modeling",
        "title": "Forecasting & modeling",
        "link": "https://www.microsoft.com/en-us/research/project/forecasting-modeling/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/presentation.jpg\" class=\"attachment-full size-full\" alt=\"a multi-slide presentation with charts and graphs\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/presentation.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/presentation-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/presentation-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/presentation-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/presentation-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/presentation-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/presentation-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/presentation-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/presentation-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/presentation-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/presentation-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/covid-19-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"COVID-19 Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tCOVID-19 Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"forecasting-modeling\">Forecasting & modeling</h2>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>The ability to model and forecast disease transmission, behavior, risk factors, illness and mortality is important for making public health decisions and allocating resources that can help mitigate the impact of a pandemic. The modeling community around the world continues to develop and evolve their techniques, which can be challenging in the fact of uncertainty in many forms. The research here between researchers at Microsoft and collaborators around the world demonstrates innovative new methods for mathematical modeling and forecasting different aspects of a pandemic.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-107 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/evaluation-of-individual-and-ensemble-probabilistic-forecasts-of-covid-19-mortality-in-the-us-2/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-1_1400x788.jpg\" alt=\"pixels of various purple hue\" class=\"wp-image-926199\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-1_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-1_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-1_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-1_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-1_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-1_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-1_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-1_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-1_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-1_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-1_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"improving-covid-19-mortality-forecasts-by-using-an-ensemble-model\"><a href=\"https://www.microsoft.com/en-us/research/publication/evaluation-of-individual-and-ensemble-probabilistic-forecasts-of-covid-19-mortality-in-the-us-2/\">Improving COVID-19 mortality forecasts by using an ensemble model</a></h4>\n\n\n\n<p>Pandemic forecasting models inform leaders and the general public on critical decisions such as healthcare staffing needs, school closures, and medical supply allocations. Results from individual COVID-19 forecasting models in the U.S. can be highly variable in their accuracy in predicting incident deaths. In contrast, an ensemble model that collected and synthesized tens of millions of specific forecasts from more than 90 different research groups provided greatly improved accuracy in predicting COVID-19 mortality. The project highlights the importance of cooperation and coordination between governmental public health agencies, academic modeling teams, and industry partners to develop modern modeling capabilities to assist in responding to outbreaks at the local, state, and federal levels.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/the-united-states-covid-19-forecast-hub-dataset/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-2_1400x788.jpg\" alt=\"U.S. COVID-19 Forecast Hub chart, an open-source dataset with forecasts of COVID-19 cases, hospitalizations and deaths at the county, state, and national levels. \" class=\"wp-image-926202\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-2_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-2_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-2_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-2_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-2_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-2_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-2_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-2_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-2_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-2_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-2_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"the-united-states-covid-19-forecast-hub-dataset\"><a href=\"https://www.microsoft.com/en-us/research/publication/the-united-states-covid-19-forecast-hub-dataset/\">The United States COVID-19 Forecast Hub Dataset</a></h4>\n\n\n\n<p>Academic researchers, government agencies, and industry groups produced a myriad of forecasts during the COVID-19 pandemic. To leverage these forecasts, the U.S. Centers for Disease Control and Prevention (CDC) partnered with an academic research lab at the University of Massachusetts Amherst to create the U.S. COVID-19 Forecast Hub, an open-source dataset with forecasts of COVID-19 cases, hospitalizations, and deaths at the county, state, and national levels. Its goal is to establish a standardized set of short-term forecasts from modeling teams that can be used to develop ensemble models, communicate to the public, and inform mitigation policies.</p>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-108 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/p0-progress-to-zero-a-simple-metric-to-measure-covid-19-progress-by-country-region/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-4_1400x788.jpg\" alt=\"world map with color indicators of progress to zero COVID-19 cases on May 8th, 2020\" class=\"wp-image-926208\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-4_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-4_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-4_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-4_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-4_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-4_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-4_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-4_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-4_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-4_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-4_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"a-simple-metric-to-measure-covid-19-progress-by-country-region\"><a href=\"https://www.microsoft.com/en-us/research/publication/p0-progress-to-zero-a-simple-metric-to-measure-covid-19-progress-by-country-region/\">A simple metric to measure COVID-19 progress by country/region</a></h4>\n\n\n\n<p>R(t), a metric epidemiologists use in managing the COVID-19 crisis, conveys whether the number of cases is growing, has plateaued, or is declining. But two geographies with similar R(t) numbers could have very different experiences due to individual, biological, and environmental nuances. Here, an alternative metric called \u201cprogress to zero\u201d is proposed which describes the percentage decline in case numbers from a previously recorded peak level. This metric is comparable across geographies and better helps leaders visualize progress.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/predicting-the-effect-of-confinement-on-the-covid-19-spread-using-machine-learning-enriched-with-satellite-air-pollution-observations/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-6_1400x788.jpg\" alt=\"graph comparing nitrogen dioxide (NO2) concentrations during the pandemic to those from 2016-2019 across countries\" class=\"wp-image-926214\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-6_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-6_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-6_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-6_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-6_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-6_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-6_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-6_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-6_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-6_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Modeling_1-6_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"predicting-the-effect-of-confinement-on-the-covid-19-spread-using-machine-learning-enriched-with-satellite-air-pollution-observations\"><a href=\"https://www.microsoft.com/en-us/research/publication/predicting-the-effect-of-confinement-on-the-covid-19-spread-using-machine-learning-enriched-with-satellite-air-pollution-observations/\">Predicting the effect of confinement on the COVID-19 spread using machine learning enriched with satellite air pollution observations</a></h4>\n\n\n\n<p>This study shows how satellite observations of nitrogen dioxide (NO2) can provide surrogate data to monitor economic activity reduction during a pandemic and to monitor the effectiveness of containment measures on the transmission of a virus before vaccines become widely available. By comparing NO2 concentrations during the pandemic to those from 2016 to 2019, researchers were able to track the weekly anomalies in economic activity. The research found that the application of near-real-time satellite NO2 observations produced a much better prediction of the deceleration of COVID-19 cases than other predictors.</p>\n</div>\n</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>The ability to model and forecast disease transmission, behavior, risk factors, illness and mortality is important for making public health decisions and allocating resources that can help mitigate the impact of a pandemic. The modeling community around the world continues to develop and evolve their techniques, which can be challenging in the fact of uncertainty [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 918240,
        "date": "2023-10-25T20:50:13",
        "slug": "prevention-control",
        "title": "Prevention & control",
        "link": "https://www.microsoft.com/en-us/research/project/prevention-control/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/subway-1400x788_v2.jpg\" class=\"attachment-full size-full\" alt=\"woman wearing a face mask waiting in the subway\" style=\"object-position: 68% 35%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/subway-1400x788_v2.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/subway-1400x788_v2-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/subway-1400x788_v2-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/subway-1400x788_v2-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/subway-1400x788_v2-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/subway-1400x788_v2-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/subway-1400x788_v2-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/subway-1400x788_v2-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/subway-1400x788_v2-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/subway-1400x788_v2-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/subway-1400x788_v2-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/covid-19-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"COVID-19 Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tCOVID-19 Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"infection-prevention-control\">Infection prevention & control</h2>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Infectious diseases, including COVID-19, continue to pose a significant public health threat, and identifying new cases and tracking disease trends are crucial for effective prevention and control measures. In recent studies, researchers have used various data sources, such as internet search trends, emergency department visits, and online surveys, to assess the impact of public health interventions and recruit study participants. These studies cover a variety of topics from passive surveillance of SARS-CoV-2 on buses to highlighting the feasibility of using online platforms to understand infectious disease trends in real time.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-109 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/association-of-covid-19-vaccination-prioritization-and-hospitalization-among-older-washingtonians/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-1_1400x788.jpg\" alt=\"chart showing a drop-off of hospitalizations for those 65 years and older after the prioritization of vaccinations for those 65+ years of age in Washington state on January 18, 2021\" class=\"wp-image-926301\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-1_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-1_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-1_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-1_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-1_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-1_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-1_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-1_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-1_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-1_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-1_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"association-of-covid-19-vaccination-prioritization-and-hospitalization-among-older-washingtonians\"><a href=\"https://www.microsoft.com/en-us/research/publication/association-of-covid-19-vaccination-prioritization-and-hospitalization-among-older-washingtonians/\">Association of COVID-19 vaccination prioritization and hospitalization among older Washingtonians</a></h4>\n\n\n\n<p>Soon after COVID-19 vaccines became available, Washington prioritized Washingtonians aged 65 and older (65+) for COVID-19 vaccination. We sought to explore whether reduced COVID-19 hospitalizations could be discerned in real-world conditions following vaccination prioritization for that population.&nbsp;We found that vaccination prioritization of those aged 65+ was associated with a substantial and statistically significant decrease in COVID-19 hospitalizations, potentially saving between 98 and 146 older Washingtonians&#8217; lives. Our study suggests that, in the United States, local policymakers might see brisk declines in hospitalization rates among those prioritized for vaccination.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/passively-sensing-sars-cov-2-rna-in-public-transit-buses/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-2_1400x788.jpg\" alt=\"stacked bar chart of various colors and patterns\" class=\"wp-image-926304\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-2_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-2_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-2_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-2_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-2_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-2_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-2_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-2_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-2_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-2_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-2_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"passively-sensing-sars-cov-2-rna-in-public-transit-buses\"><a href=\"https://www.microsoft.com/en-us/research/publication/passively-sensing-sars-cov-2-rna-in-public-transit-buses/\">Passively sensing SARS-CoV-2 RNA in public transit buses</a></h4>\n\n\n\n<p>For this study, researchers developed a method to detect SARS-CoV-2 RNA in the air filters of public buses, revealing that air filters could be used as passive sensors to track the transmission of respiratory infectious diseases in urban transport infrastructure. The study found that SARS-CoV-2 RNA was present in 14% of public bus filters tested in Seattle, Washington, from August 2020 to March 2021. This method could provide a unique way to track the geographically relevant transmission of SARS-CoV-2 through public transit rider vectors, pooling samples of riders over time in a passive manner without installing any additional systems on transit vehicles.</p>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-110 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/medicare-data-reveal-wide-variety-across-regions-of-covid-19-fatality-risk/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-3_1400x788.jpg\" alt=\"United States map indicating the percentage of population vulnerable to COVID-19 fatality risk \" class=\"wp-image-926328\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-3_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-3_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-3_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-3_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-3_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-3_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-3_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-3_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-3_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-3_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-3_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"medicare-data-reveal-wide-regional-variety-of-covid-19-fatality-risk\"><a href=\"https://www.microsoft.com/en-us/research/publication/medicare-data-reveal-wide-variety-across-regions-of-covid-19-fatality-risk/\">Medicare data reveal wide regional variety of COVID-19 fatality risk</a></h4>\n\n\n\n<p>Evidence shows some Americans are at much greater risk than others for severe COVID-19 symptoms, hospitalization, and death; less is known about which regions are especially vulnerable. This research examined Medicare beneficiary data from across the US to develop an index measuring the risk of severe COVID-19, by capturing the age and likelihood of key chronic illnesses. The analysis and map showed the concentration of highest-risk people ranged from less than 2 percent to just over 22 percent, with the highest rates found in certain regions in Florida and Arizona. This suggests potential value in coordinating public health efforts across local political entities.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/the-risk-of-hospitalization-and-mortality-after-breakthrough-sars-cov-2-infection-by-vaccine-type-observational-study-of-medical-claims-data/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/CPAP-PS_1400x788.jpg\" alt=\"a CPAP/PS machine hooked up to a patient out of focus in the background\" class=\"wp-image-926334\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/CPAP-PS_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/CPAP-PS_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/CPAP-PS_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/CPAP-PS_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/CPAP-PS_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/CPAP-PS_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/CPAP-PS_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/CPAP-PS_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/CPAP-PS_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/CPAP-PS_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/CPAP-PS_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"the-risk-of-hospitalization-and-mortality-after-breakthrough-sars-cov-2-infection-by-vaccine-type\"><a href=\"https://www.microsoft.com/en-us/research/publication/the-risk-of-hospitalization-and-mortality-after-breakthrough-sars-cov-2-infection-by-vaccine-type-observational-study-of-medical-claims-data/\">The risk of hospitalization and mortality after breakthrough SARS-CoV-2 infection by vaccine type</a></h4>\n\n\n\n<p>This research examined the risks of hospitalization and mortality in fully vaccinated individuals who had breakthrough SARS-CoV-2 infections. The study analyzed a cohort of 19,815 patients who had received either the Pfizer, Moderna, or Janssen vaccines. It found those who received the Moderna vaccine had the lowest hazard rates, followed by the Pfizer vaccine. The top risk factors for severe breakthrough infections were age over 50, male gender, chronic lung disease, and moderate or severe renal failure, severe liver disease, leukemia, chronic lung disease, coagulopathy, and alcohol abuse. These results at a population level will be helpful to public health policymakers.</p>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-111 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/association-of-public-health-measures-during-the-covid-19-pandemic-with-the-incidence-of-infectious-conjunctivitis/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-5_1400x788.jpg\" alt=\"line chart showing the association between public health measures and incidence of infectious conjunctivitis during the COVID-19 pandemic\" class=\"wp-image-926343\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-5_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-5_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-5_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-5_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-5_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-5_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-5_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-5_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-5_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-5_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-5_1400x788-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"association-of-public-health-measures-during-the-covid-19-pandemic-with-the-incidence-of-infectious-conjunctivitis\"><a href=\"https://www.microsoft.com/en-us/research/publication/association-of-public-health-measures-during-the-covid-19-pandemic-with-the-incidence-of-infectious-conjunctivitis/\">Association of public health measures during the COVID-19 pandemic with the incidence of infectious conjunctivitis</a></h4>\n\n\n\n<p>While local mitigation strategies for infectious conjunctivitis have been successful, population-wide decreases in spread are rare. This research investigated an association between the adaptation of public health measures for COVID-19 and decreased emergency department visits for infectious conjunctivitis as well as internet search interest. 1156 emergency department encounters for conjunctivitis from January 2015 to February 2021 were studied. It found that following widespread implementation of pandemic public health interventions including social distancing and travel restrictions, emergency department encounters for conjunctivitis decreased by 37.3%, while search interest decreased by 34.2%, providing evidence of a sustained population-wide decrease in infectious conjunctivitis.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/capturing-covid-19-like-symptoms-at-scale-using-banner-ads-on-an-online-news-platform-pilot-survey-study/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-6_1400x788-1024x576.jpg\" alt=\"color-coded map of the United States indicating total responses to banner on an online news platform seeking volunteers willing to report COVID-19-like symptoms\" class=\"wp-image-926346\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-6_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-6_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-6_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-6_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-6_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-6_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-6_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-6_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-6_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-6_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Prevention_3-6_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"capturing-covid-19-like-symptoms-at-scale-using-banner-ads-on-an-online-news-platform\"><a href=\"https://www.microsoft.com/en-us/research/publication/capturing-covid-19-like-symptoms-at-scale-using-banner-ads-on-an-online-news-platform-pilot-survey-study/\">Capturing COVID-19\u2013like symptoms at scale using banner ads on an online news platform</a></h4>\n\n\n\n<p>Identifying new COVID-19 cases is challenging because testing kits and other equipment are limited in many parts of the world. This study assessed the use of an online news platform to recruit volunteers willing to report COVID-19\u2013like symptoms and behaviors. An online epidemiologic survey used banner ads to recruit 87,322 respondents across a 3-week span at the end of April 2020 in the U.S. and Japan. Of the total, 22.3% reported at least one COVID-19 symptom, and symptom reporting rates positively correlated with per capita COVID-19 testing rates. It showed that news platforms can be used to collect infectious disease symptoms at scale and with populations older than those found through social media platforms.</p>\n</div>\n</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Infectious diseases, including COVID-19, continue to pose a significant public health threat, and identifying new cases and tracking disease trends are crucial for effective prevention and control measures. In recent studies, researchers have used various data sources, such as internet search trends, emergency department visits, and online surveys, to assess the impact of public health [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 918249,
        "date": "2023-10-25T20:49:14",
        "slug": "treatment-diagnostics",
        "title": "Treatment & diagnostics",
        "link": "https://www.microsoft.com/en-us/research/project/treatment-diagnostics/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/swab_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"woman wearing a surgical mask swabbing the inside of an elderly man's nose\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/swab_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/swab_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/swab_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/swab_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/swab_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/swab_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/swab_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/covid-19-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"COVID-19 Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tCOVID-19 Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"treatment-diagnostics-research\">Treatment & diagnostics research</h2>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\">\n<p>&#8220;<em>The T-Detect COVID Test is a novel technology that assesses the T cell immune response to COVID-19. Information and scientific data that deepen our understanding of SARS-CoV-2 remain important keys to get ahead of this global pandemic</em>.&#8221;</p>\n<cite>&#8211;Jeff Shuren, M.D., J.D., director of <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.fda.gov/news-events/press-announcements/coronavirus-covid-19-update-fda-authorizes-adaptive-biotechnologies-t-detect-covid-test\" target=\"_blank\" rel=\"noreferrer noopener\">FDA\u2019s Center for Devices and Radiological Health<span class=\"sr-only\"> (opens in new tab)</span></a></cite></blockquote>\n\n\n\n<p>In March 2020, Microsoft and Adaptive Biotechnologies expanded their existing collaboration to map and measure the immune response to multiple diseases and started applying their combined capabilities to COVID-19. In May, Adaptive started enrollment for a virtual clinical study, <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.adaptivebiotech.com/?utm_source=domain&utm_medium=referral&utm_campaign=immunerace\" target=\"_blank\" rel=\"noreferrer noopener\">ImmuneRACE<span class=\"sr-only\"> (opens in new tab)</span></a>, to measure the presence of T cells that identify the disease early on and proliferate to combat the infection. The results of <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7418734/\" target=\"_blank\" rel=\"noreferrer noopener\">this study<span class=\"sr-only\"> (opens in new tab)</span></a> were published in July 2020, accompanied by a large release of population-level data analyzed to reveal T cell signatures of disease for COVID-19, known as <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.adaptivebiotech.com/immunecode/\" target=\"_blank\" rel=\"noreferrer noopener\">ImmuneCODE<span class=\"sr-only\"> (opens in new tab)</span></a>. This was quickly followed by Adaptive\u2019s submission to the FDA in September. The FDA granted emergency use authorization for T-Detect\u2122 COVID in March 2021 as the first T cell-based diagnostic.</p>\n\n\n\n<p>That is one example of the development of diagnostic tools and treatments, along with assessments of how these can be applied in practice, that was a focus for much research around the world. This included the application of machine learning and other novel approaches for analyzing large volumes of data to help with triaging COVID-19 patients, predicting outcomes for COVID-19 cases, and estimating treatment effectiveness. The research highlighted here applies such tools to \u201clong Covid\u201d, predictions of death and renal failure, modeling probability for positive SARS-CoV-2 tests, classifying chest X-rays, and predicting the effectiveness of different medical treatments on patients with varying risk factors.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-112 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/longitudinal-analysis-of-t-cell-receptor-repertoires-reveals-shared-patterns-of-antigen-specific-response-to-sars-cov-2-infection/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-1024x576.jpg\" alt=\"Flowchart of SARS-CoV-2 oubreak in Vo'\" class=\"wp-image-926427\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"longitudinal-analysis-of-t-cell-receptor-tcr-repertoires-reveals-shared-patterns-of-antigen-specific-response-to-sars-cov-2-infection\"><a href=\"https://www.microsoft.com/en-us/research/publication/longitudinal-analysis-of-t-cell-receptor-repertoires-reveals-shared-patterns-of-antigen-specific-response-to-sars-cov-2-infection/\">Longitudinal analysis of T cell receptor (TCR) repertoires reveals shared patterns of antigen-specific response to SARS-CoV-2 infection</a></h4>\n\n\n\n<p>T cells play a prominent role in the immune response to viral diseases, but their role in subsequent immunity to SARS-CoV-2 infection remains poorly understood. This research studied the assortments (repertoires) of TCRs in an Italian population following a SARS-CoV-2 outbreak. An individual\u2019s TCR repertoire changes as immunity is developed following exposure. Samples taken at 2, 9 and 15 months found elevated levels of TCRs associated with SARS-CoV-2, demonstrating the T cells\u2019 central role in mounting a persistent immune defense against SARS-CoV-2.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/t-cell-receptor-sequencing-identifies-prior-sars-cov-2-infection-and-correlates-with-neutralizing-antibody-titers-and-disease-severity/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-1024x576.jpg\" alt=\"Venn diagram of T cell test, nAb, EUROIMMUN, and Abbott ARCHITECT\" class=\"wp-image-926430\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"t-cell-receptor-tcr-sequencing-identifies-prior-sars-cov-2-infection-and-correlates-with-neutralizing-antibody-titers-and-disease-severity\"><a href=\"https://www.microsoft.com/en-us/research/publication/t-cell-receptor-sequencing-identifies-prior-sars-cov-2-infection-and-correlates-with-neutralizing-antibody-titers-and-disease-severity/\">T-cell receptor (TCR) sequencing identifies prior SARS-CoV-2 infection and correlates with neutralizing antibody titers and disease severity</a></h4>\n\n\n\n<p>The blood levels of neutralizing antibody titers (nAb) closely correlate with the protection provided by an effective vaccination. But nAb assays are challenging to perform at a large scale. This research instead applied a TCR sequencing assay on a standard blood sample to assess T-cell response to SARS-CoV-2 infection. It found that the magnitude of the SARS-CoV-2-specific T-cell response strongly correlates with nAb titer, as well as with clinical indicators of disease severity including hospitalization, fever, or difficulty breathing, thus demonstrating the utility of a TCR-based assay.</p>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-113 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-5-1024x576.jpg\" alt=\"Chest radiography (CXR)\" class=\"wp-image-923784\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-5-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-5-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-5-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-5-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-5-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-5-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-5-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-5-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-5-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-5-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-5.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"supporting-clinicians-to-assess-covid-19-severity-using-ai-and-chest-x-rays\"><a href=\"https://www.microsoft.com/en-us/research/publication/hierarchical-analysis-of-visual-covid-19-features-from-chest-radiographs/\"></a><a href=\"https://www.microsoft.com/en-us/research/publication/hierarchical-analysis-of-visual-covid-19-features-from-chest-radiographs/\" target=\"_blank\" rel=\"noreferrer noopener\">Supporting clinicians to assess COVID-19 severity using AI and Chest X-rays</a></h4>\n\n\n\n<p>COVID-19 X-rays have been a recommended procedure for patient triaging and resource management in intensive care units (ICUs) throughout the COVID-19 pandemic. Microsoft Research team worked closely with our clinicians at University Hospitals Birmingham NHS Foundation Trust to model radiological features with a human-interpretable class hierarchy that aligns with the radiological decision process. The model outperformed the clinicians across all hierarchical and multi-class tasks. To better understand the model\u2019s failure patterns, the team employed an error analysis tool in Azure Machine Learning that is not often found in healthcare-related ML studies and is crucial for providing transparency and actionable insights about a model\u2019s behavior.&nbsp; The analysis may also be useful after deployment if presented as reliability information alongside the model\u2019s predictions.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-3-1024x576.jpg\" alt=\"four scatter plots of various colors\" class=\"wp-image-923769\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-3-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-3-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-3-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-3-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-3-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-3-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-3-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-3-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-3-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-3-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-3.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"removing-biases-from-deep-learning-based-models-of-covid-19-chest-x-rays\"><a href=\"https://www.microsoft.com/en-us/research/publication/deep-learning-models-for-covid-19-chest-x-ray-classification-preventing-shortcut-learning-using-feature-disentanglement/\"></a><a href=\"https://www.microsoft.com/en-us/research/publication/deep-learning-models-for-covid-19-chest-x-ray-classification-preventing-shortcut-learning-using-feature-disentanglement/\" target=\"_blank\" rel=\"noreferrer noopener\">Removing biases from deep learning-based models of COVID-19 chest X-rays</a></h4>\n\n\n\n<p>Recent research has proposed creating deep learning-based models that use chest radiographs (CXRs) in a variety of clinical tasks to help manage the COVID-19 crisis. However, due to the small size of existing COVID-19 CXR datasets, data is often pooled from multiple sources under different scenarios. Models trained on such datasets can \u201coverfit\u201d to erroneous features instead of learning pulmonary characteristics. This research adds feature disentanglement to the training process, resulting in better generalization performance on unseen data and outperforming other proposed methods.</p>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-114 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/04/Covid-Treatment_4-7_1400x788-1024x576.png\" alt=\"Covid research - treatment - sociodemographic research\" class=\"wp-image-937215\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/04/Covid-Treatment_4-7_1400x788-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/04/Covid-Treatment_4-7_1400x788-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/04/Covid-Treatment_4-7_1400x788-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/04/Covid-Treatment_4-7_1400x788-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/04/Covid-Treatment_4-7_1400x788-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/04/Covid-Treatment_4-7_1400x788-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/04/Covid-Treatment_4-7_1400x788-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/04/Covid-Treatment_4-7_1400x788-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/04/Covid-Treatment_4-7_1400x788-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/04/Covid-Treatment_4-7_1400x788-1280x720.png 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/04/Covid-Treatment_4-7_1400x788.png 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"sociodemographic-and-clinical-features-predictive-of-sars-cov-2-test-positivity-across-healthcare-visit-types\"><a href=\"https://www.microsoft.com/en-us/research/publication/sociodemographic-and-clinical-features-predictive-of-sars-cov-2-test-positivity-across-healthcare-visit-types/\"></a><a href=\"https://www.microsoft.com/en-us/research/publication/sociodemographic-and-clinical-features-predictive-of-sars-cov-2-test-positivity-across-healthcare-visit-types/\" target=\"_blank\" rel=\"noreferrer noopener\">Sociodemographic and clinical features predictive of SARS-CoV-2 test positivity across healthcare visit-types</a>&nbsp;</h4>\n\n\n\n<p>Despite increased testing efforts and the deployment of vaccines, COVID-19 cases and death toll continued to rise at record rates. Health systems routinely collect clinical and non-clinical information in electronic health records (EHR), yet little is known about how the minimal or intermediate spectra of EHR data can be leveraged to characterize patient SARS-CoV-2 pretest probability in support of interventional strategies. This research used machine learning to model patient pretest probability for SARS-CoV-2 test positivity and determined which features most contributed to predicted pretest probability for patients triaged in inpatient, outpatient, and telehealth/drive-up settings. The research found that geographic and sociodemographic factors were more important predictors of SARS-CoV-2 positivity than individual clinical characteristics.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a href=\"https://www.microsoft.com/en-us/research/publication/automated-interpretable-discovery-of-heterogeneous-treatment-effectiveness-a-covid-19-case-study/\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-6.jpg\" alt=\"four abstract charts with a blue line \" class=\"wp-image-923787\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-6.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-6-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-6-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-6-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-6-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-6-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-6-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-6-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-6-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-6-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-6-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"automatically-predicting-the-effectiveness-of-heterogeneous-treatments-for-covid-19-patients\"><a href=\"https://www.microsoft.com/en-us/research/publication/automated-interpretable-discovery-of-heterogeneous-treatment-effectiveness-a-covid-19-case-study/\">Automatically predicting the effectiveness of heterogeneous treatments for COVID-19 patients</a></h4>\n\n\n\n<p>The effects of medical treatments can vary for different patients based on many different underlying risk factors, making independent testing of the effects of multiple treatments unwieldy. This research uses multitask machine learning to automatically predict the heterogeneous (varying) effectiveness of different medical treatments, and trains additive models to estimate personalized treatment benefits.&nbsp; When applied to mortality risk models of COVID-19 patients, this method uncovered evidence supporting two pathways of mortality: inflammation and thrombosis, and achieved state-of-the-art predictive power and interpretable identification of heterogeneous treatment benefits.</p>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-115 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-1-1024x576.jpg\" alt=\"a grey background with pixels of yellow and dark purple\" class=\"wp-image-923757\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-1-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-1-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-1.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"long-term-effects-of-sars-cov-2-and-their-associations-with-social-determinants-of-health\"><a href=\"https://www.microsoft.com/en-us/research/publication/identifying-long-term-effects-of-sars-cov-2-and-their-association-with-social-determinants-of-health-in-a-cohort-of-over-one-million-covid-19-survivors-2/\"></a><a href=\"https://www.microsoft.com/en-us/research/publication/identifying-long-term-effects-of-sars-cov-2-and-their-association-with-social-determinants-of-health-in-a-cohort-of-over-one-million-covid-19-survivors-2/\" target=\"_blank\" rel=\"noreferrer noopener\">Long-term effects of SARS-CoV-2 and their associations with social determinants of health</a></h4>\n\n\n\n<p>Much remains unknown about the complications that can follow SARS-CoV-2 infection (\u201clong Covid\u201d). This research analyzed a medical claims database of over one million COVID-19 survivors to study long-term symptoms and their associations with various social and medical risk factors. It identified the ICD-10 codes whose proportions were significantly increased and included a logistic regression-based association analysis. It found associations of long-term effects with age and gender, but not with race, income and education levels.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-2-1024x576.jpg\" alt=\"a chart with a blue line on a bumpy path up and to the right\" class=\"wp-image-923760\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-2-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-2-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-2-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-2-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-2-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-2-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-2-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-2-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-2-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-2-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/4-2.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"using-machine-learning-to-predict-death-and-organ-failure-in-hospitalized-patients-with-covid-19\"><a href=\"https://www.microsoft.com/en-us/research/publication/machine-learning-based-derivation-and-external-validation-of-a-tool-to-predict-death-and-development-of-organ-failure-in-hospitalized-patients-with-covid-19/\"></a><a href=\"https://www.microsoft.com/en-us/research/publication/machine-learning-based-derivation-and-external-validation-of-a-tool-to-predict-death-and-development-of-organ-failure-in-hospitalized-patients-with-covid-19/\" target=\"_blank\" rel=\"noreferrer noopener\">Using machine learning to predict death and organ failure in hospitalized patients with COVID-19</a></h4>\n\n\n\n<p>To date, COVID-19 prediction models have largely focused on mortality rather than risks for other outcomes such as shock, renal failure or respiratory failure requiring mechanical ventilation. To address these concerns, this research created separate models, based on demographic and clinical information collected upon hospital admission, to predict risks for in-hospital mortality, ICU transfer, shock, and renal replacement therapy, with high accuracy. These models could help improve triage decisions and resource allocation and support clinical trial enrichment.</p>\n</div>\n</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>&#8220;The T-Detect COVID Test is a novel technology that assesses the T cell immune response to COVID-19. Information and scientific data that deepen our understanding of SARS-CoV-2 remain important keys to get ahead of this global pandemic.&#8221; In March 2020, Microsoft and Adaptive Biotechnologies expanded their existing collaboration to map and measure the immune response [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 921738,
        "date": "2023-10-25T20:46:56",
        "slug": "understanding-sars-cov-2",
        "title": "Understanding SARS-CoV-2",
        "link": "https://www.microsoft.com/en-us/research/project/understanding-sars-cov-2/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"788\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/understanding.jpg\" class=\"attachment-full size-full\" alt=\"female scientist working in a clean lab pipetting liquid into test tubes\" style=\"object-position: 66% 38%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/understanding.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/understanding-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/understanding-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/understanding-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/understanding-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/understanding-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/understanding-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/understanding-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/understanding-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/understanding-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/understanding-1280x720.jpg 1280w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/covid-19-research/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"COVID-19 Research\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tCOVID-19 Research\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"understanding-sars-cov-2\">Understanding SARS-CoV-2</h2>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>The novelty of the SARS-CoV-2 coronavirus led to a concerted effort by the global research community to better understand how the virus operates at the molecular level. The highlighted research here suggested answers to questions such as how the SARS-CoV-2 may have evolved from animals to humans, how to improve the potency of peptide vaccines, examining the role of T-cells in immunity, using T-cell receptor sequencing to predict disease severity, and simulating the mechanism by which the virus docks with the body\u2019s receptors.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-116 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/loss-of-spike-n370-glycosylation-as-an-important-evolutionary-event-for-the-enhanced-infectivity-of-sars-cov-2/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-1_1400x788-1024x576.jpg\" alt=\"illustration of NAG234, Down RBD, and NAG370\" class=\"wp-image-926415\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-1_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-1_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-1_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-1_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-1_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-1_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-1_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-1_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-1_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-1_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-1_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"loss-of-glycosylation-ability-in-spike-n370-glycoprotein-found-to-be-an-important-evolutionary-event-for-the-enhanced-infectivity-of-sars-cov-2\"><a href=\"https://www.microsoft.com/en-us/research/publication/loss-of-spike-n370-glycosylation-as-an-important-evolutionary-event-for-the-enhanced-infectivity-of-sars-cov-2/\">Loss of glycosylation ability in Spike N370 glycoprotein found to be an important evolutionary event for the enhanced infectivity of SARS-CoV-2</a></h4>\n\n\n\n<p>The SARS-CoV-2 coronavirus may have spread from animals such as bats and pangolins to humans. But the molecular changes that enabled this host expansion remain unclear. This study examines the role of the SARS-CoV-2 spike (S) glycoprotein, which mediates viral entry by binding to the host receptor and fusing viral and cellular membranes. The research pinpoints a mutation in SARS-CoV-2 that causes the spike N370 glycoprotein to lose the ability to perform a process involved in receptor-binding called glycosylation. This was likely an important evolutionary event enabling a higher capacity for infection of humans.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/pharmacokinetic-tuning-of-protein-antigen-fusions-enhances-the-immunogenicity-of-t-cell-vaccines/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-2_1400x788-1024x576.jpg\" alt=\"illustration of MSA, Peptide antigen, His_6 tag, and Flexible linkers\" class=\"wp-image-926418\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-2_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-2_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-2_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-2_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-2_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-2_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-2_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-2_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-2_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-2_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-2_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"enhancing-the-potency-of-peptide-vaccines-by-fusing-the-peptides-epitopes-to-carrier-proteins\"><a href=\"https://www.microsoft.com/en-us/research/publication/pharmacokinetic-tuning-of-protein-antigen-fusions-enhances-the-immunogenicity-of-t-cell-vaccines/\">Enhancing the potency of peptide vaccines by fusing the peptides\u2019 epitopes to carrier proteins</a></h4>\n\n\n\n<p>The potency of peptide-based vaccines is limited by poor transport to lymph nodes following injection. This research illustrates a method for improving antigen delivery by fusing the portion of the peptide that is capable of stimulating an immune response, the epitope, to carrier proteins. Doing so makes pharmacokinetic \u201ctuning\u201d possible; for instance, the research showed that the carrier protein transthyretin simultaneously optimized three factors involved in antigen uptake and protection of antigen payloads. This method can increase vaccine potency by up to 90-fold, and maximize the responses to viral antigens, tumor-associated antigens, oncofetal antigens and shared neoantigens.</p>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-117 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/longitudinal-analysis-of-t-cell-receptor-repertoires-reveals-shared-patterns-of-antigen-specific-response-to-sars-cov-2-infection/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-1024x576.jpg\" alt=\"Flowchart of SARS-CoV-2 oubreak in Vo'\" class=\"wp-image-926427\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-3_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"longitudinal-analysis-of-t-cell-receptor-tcr-repertoires-reveals-shared-patterns-of-antigen-specific-response-to-sars-cov-2-infection\"><a href=\"https://www.microsoft.com/en-us/research/publication/longitudinal-analysis-of-t-cell-receptor-repertoires-reveals-shared-patterns-of-antigen-specific-response-to-sars-cov-2-infection/\">Longitudinal analysis of T cell receptor (TCR) repertoires reveals shared patterns of antigen-specific response to SARS-CoV-2 infection</a></h4>\n\n\n\n<p>T cells play a prominent role in the immune response to viral diseases, but their role in subsequent immunity to SARS-CoV-2 infection remains poorly understood. This research studied the assortments (repertoires) of TCRs in an Italian population following a SARS-CoV-2 outbreak. An individual\u2019s TCR repertoire changes as immunity is developed following exposure. Samples taken at 2, 9 and 15 months found elevated levels of TCRs associated with SARS-CoV-2, demonstrating the T cells\u2019 central role in mounting a persistent immune defense against SARS-CoV-2.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/t-cell-receptor-sequencing-identifies-prior-sars-cov-2-infection-and-correlates-with-neutralizing-antibody-titers-and-disease-severity/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-1024x576.jpg\" alt=\"Venn diagram of T cell test, nAb, EUROIMMUN, and Abbott ARCHITECT\" class=\"wp-image-926430\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-4_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"t-cell-receptor-tcr-sequencing-identifies-prior-sars-cov-2-infection-and-correlates-with-neutralizing-antibody-titers-and-disease-severity\"><a href=\"https://www.microsoft.com/en-us/research/publication/t-cell-receptor-sequencing-identifies-prior-sars-cov-2-infection-and-correlates-with-neutralizing-antibody-titers-and-disease-severity/\">T-cell receptor (TCR) sequencing identifies prior SARS-CoV-2 infection and correlates with neutralizing antibody titers and disease severity</a></h4>\n\n\n\n<p>The blood levels of neutralizing antibody titers (nAb) closely correlate with the protection provided by an effective vaccination. But nAb assays are challenging to perform at a large scale. This research instead applied a TCR sequencing assay on a standard blood sample to assess T-cell response to SARS-CoV-2 infection. It found that the magnitude of the SARS-CoV-2-specific T-cell response strongly correlates with nAb titer, as well as with clinical indicators of disease severity including hospitalization, fever, or difficulty breathing, thus demonstrating the utility of a TCR-based assay.</p>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-118 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/risk-of-rapid-evolutionary-escape-from-biomedical-interventions-targeting-sars-cov-2-spike-protein/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-5_1400x788-1024x576.jpg\" alt=\"images of SARS-CoV-2 with color-indicators of targeting the spike protein\" class=\"wp-image-926439\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-5_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-5_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-5_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-5_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-5_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-5_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-5_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-5_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-5_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-5_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Covid-Understanding_5-5_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"risk-of-rapid-evolutionary-escape-from-biomedical-interventions-targeting-sars-cov-2-spike-protein\"><a href=\"https://www.microsoft.com/en-us/research/publication/risk-of-rapid-evolutionary-escape-from-biomedical-interventions-targeting-sars-cov-2-spike-protein/\">Risk of rapid evolutionary escape from biomedical interventions targeting SARS-CoV-2 spike protein</a></h4>\n\n\n\n<p>The spike protein receptor-binding domain (RBD) of SARS-CoV-2 is the molecular target for many vaccines and antibody-based prophylactics for COVID-19. But a narrow molecular focus could simplify viral immune evasion. This research examines how easily the virus escapes neutralizing antibodies (nAbs) that target the spike RBD by combining an analysis of the RBD structure-function with an evolutionary modeling framework. The modeling suggests that SARS-CoV-2 mutants are expected to exist in high numbers due to neutral genetic variation, and consequently resistance can develop quickly -and repeatedly- under positive selection.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a href=\"https://www.microsoft.com/en-us/research/publication/exploring-the-regulatory-function-of-the-n-terminal-domain-of-sars-cov-2-spike-protein-through-molecular-dynamics-simulation/\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/SARS-CoV-2_1400x788-1024x576.jpg\" alt=\"Protein synthesis. Illustration of a ribosome (centre) producing a protein (red) from an mRNA (messenger ribonucleic acid, multicoloured) template. This process is known as translation. mRNA consists of groups of three nucleotide bases that code for different amino acids, the building blocks of proteins. The ribosome attaches to the mRNA and reads its code. A transfer RNA (tRNA) molecule (dark purple) carrying an amino acid (red) corresponding to the code then binds to the ribosome. When the tRNA dissociates it leaves the amino acid behind, and the ribosome moves onto the next bases. As the ribosome moves along the mRNA the protein grows from the ribosome.\n\" class=\"wp-image-926451\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/SARS-CoV-2_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/SARS-CoV-2_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/SARS-CoV-2_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/SARS-CoV-2_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/SARS-CoV-2_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/SARS-CoV-2_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/SARS-CoV-2_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/SARS-CoV-2_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/SARS-CoV-2_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/SARS-CoV-2_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/SARS-CoV-2_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"a-wedge-model-using-molecular-dynamics-simulation-that-shows-how-the-sars-cov-2-spike-protein-s-n-terminal-domain-regulates-the-motions-of-the-receptor-binding-domain\"><a href=\"https://www.microsoft.com/en-us/research/publication/exploring-the-regulatory-function-of-the-n-terminal-domain-of-sars-cov-2-spike-protein-through-molecular-dynamics-simulation/\">A \u201cwedge\u201d model using molecular dynamics simulation that shows how the SARS-CoV-2 spike protein\u2019s N-terminal domain regulates the motions of the receptor-binding domain</a></h4>\n\n\n\n<p>Two key segments of the SARS-CoV-2 spike (S) protein are the receptor-binding domain (RBD) and neighboring N-terminal domain (NTD). The RBD allows the S protein to dock to a body\u2019s receptors to gain entry into cells and cause infection. This research applied a molecular dynamics simulation to the S protein to model the movements of these domains. It revealed that the NTD acts as a \u201cwedge\u201d to prohibit the RBD&#8217;s movements, and it occasionally moves out to allow the RBD to tilt downward. This NTD \u201cwedge\u201d model suggests that the NTD\u2013RBD interface should become a potential drug target.</p>\n</div>\n</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>The novelty of the SARS-CoV-2 coronavirus led to a concerted effort by the global research community to better understand how the virus operates at the molecular level. The highlighted research here suggested answers to questions such as how the SARS-CoV-2 may have evolved from animals to humans, how to improve the potency of peptide vaccines, [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 978909,
        "date": "2023-10-24T09:02:02",
        "slug": "ai-chat-log-research",
        "title": "AI Chat Log Research",
        "link": "https://www.microsoft.com/en-us/research/project/ai-chat-log-research/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--inset-right\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"400\" height=\"301\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/b2449f43-0b5b-4514-a04d-156604420b11-6537e7c8b2752.webp\" class=\"attachment-full size-full\" alt=\"Conversations with feedback icon\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/b2449f43-0b5b-4514-a04d-156604420b11-6537e7c8b2752.webp 400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/b2449f43-0b5b-4514-a04d-156604420b11-6537e7c8b2752-300x226.webp 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/b2449f43-0b5b-4514-a04d-156604420b11-6537e7c8b2752-80x60.webp 80w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/b2449f43-0b5b-4514-a04d-156604420b11-6537e7c8b2752-240x180.webp 240w\" sizes=\"(max-width: 400px) 100vw, 400px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"ai-chat-logs-research\">AI Chat Logs Research</h1>\n\n\n\n<p>AI Chat Log Research is a v-team collaboration among E+D Office of Applied Research, Microsoft Research, Bing Metrics and Analytics, and Turing to address practical challenges arising from the analysis of AI Chat logs.</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Large Language Model-based AI is transforming how users interact with assistive systems. Logs of user interactions from new chat and copilot AI systems provide more extensive signals of user satisfaction, success, and enjoyment than conventional search and recommendation logs due to the rich use of natural language in the interactions. Moreover, the LLMs themselves can be leveraged to extract the signals effectively from the natural language in the logs. This combination provides a unique opportunity to utilize the chat logs to quickly understand user intent, assess satisfaction, and characterize usage to drive continuous improvements in the chat and copilot systems over time.<br><br>aka.ms/chatlogs</p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>AI Chat Log Research is a v-team collaboration among E+D Office of Applied Research, Microsoft Research, Bing Metrics and Analytics, and Turing to address practical challenges arising from the analysis of AI Chat logs. Large Language Model-based AI is transforming how users interact with assistive systems. Logs of user interactions from new chat and copilot [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 978333,
        "date": "2023-10-23T01:48:37",
        "slug": "llmlingua",
        "title": "LLMLingua Series",
        "link": "https://www.microsoft.com/en-us/research/project/llmlingua/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-catalina-blue card-background--inset-right\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1792\" height=\"1024\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/LLMLingua_background.png\" class=\"attachment-full size-full\" alt=\"background of LLMLingua\" style=\"object-position: 33% 23%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/LLMLingua_background.png 1792w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/LLMLingua_background-300x171.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/LLMLingua_background-1024x585.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/LLMLingua_background-768x439.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/LLMLingua_background-1536x878.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/LLMLingua_background-240x137.png 240w\" sizes=\"(max-width: 1792px) 100vw, 1792px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"llmlingua\">LLMLingua</h1>\n\n\n\n<p>Effectively Deliver Information to LLMs via&nbsp;<strong>Prompt Compression</strong></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-119 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>LLMLingua</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"identify-and-remove-non-essential-tokens-in-prompts-using-perplexity-from-a-slm\">Identify and remove non-essential tokens in prompts using perplexity from a SLM</h3>\n\n\n\n<p><strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://llmlingua.com/llmlingua.html\">Read More<span class=\"sr-only\"> (opens in new tab)</span></a></strong></p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>LongLLMLingua</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"enhance-long-context-information-via-query-aware-compression-and-reorganization\">Enhance long-context information via query-aware compression and reorganization</h3>\n\n\n\n<p><strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://llmlingua.com/longllmlingua.html\">Read More<span class=\"sr-only\"> (opens in new tab)</span></a></strong></p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p>LLMLingua-2</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"utilize-data-distillation-to-learn-compression-targets-for-efficient-and-faithful-task-agnostic-compression\">Utilize data distillation to learn compression targets for efficient and faithful task-agnostic compression</h3>\n\n\n\n<p><strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://llmlingua.com/llmlingua2.html\">Read More<span class=\"sr-only\"> (opens in new tab)</span></a></strong></p>\n</div>\n</div>\n\n\n\n<p>Large language models (LLMs) have demonstrated remarkable capabilities and have been applied across various fields. Advancements in technologies such as Chain-of-Thought (CoT), In-Context Learning (ICL), and Retrieval-Augmented Generation (RAG) have led to increasingly lengthy prompts for LLMs, sometimes exceeding tens of thousands of tokens. Longer prompts, however, can result in 1) increased API response latency, 2) exceeded context window limits, 3) loss of contextual information, 4) expensive API bills, and 5) performance issues such as &#8220;lost in the middle.&#8221;</p>\n\n\n\n<p>Inspired by the concept of &#8220;LLMs as Compressors,&#8221; we designed a series of works that try to build a language for LLMs via prompt compression. This approach accelerates model inference, reduces costs, and improves downstream performance while revealing LLM context utilization and intelligence patterns. Our work achieved a <em>20x compression ratio</em> with minimal performance loss(<strong>LLMLingua</strong>), and <em>a 17.1% performance improvement with 4x compression</em> (<strong>LongLLMLingua</strong>). <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.05736\"><strong>LLMLingua-2</strong><span class=\"sr-only\"> (opens in new tab)</span></a>, a small-size yet powerful prompt compression method trained via data distillation from GPT-4 for token classification with a BERT-level encoder, excels in task-agnostic compression. It surpasses LLMLingua in handling out-of-domain data, offering 3x-6x faster performance.</p>\n\n\n\n<p>This page is for&nbsp;<strong>research demonstration purposes</strong>&nbsp;only. </p>\n\n\n\n<p>If you are interested in our ideas, please feel free to <strong>use LLMLingua</strong> and communicate with us.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"user-content-news\"><strong>News</strong></h3>\n\n\n\n<ul>\n<li><strong>\ud83e\udd9a</strong>&nbsp;&nbsp;We&#8217;re excited to announce the release of&nbsp;<strong>LLMLingua-2</strong>, boasting a 3x-6x speed improvement over LLMLingua! For more information, check out our&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2403.12968\">paper<span class=\"sr-only\"> (opens in new tab)</span></a></strong>, visit the&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://llmlingua.com/llmlingua2.html\">project page<span class=\"sr-only\"> (opens in new tab)</span></a></strong>, and explore our&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://huggingface.co/spaces/microsoft/LLMLingua-2\">demo<span class=\"sr-only\"> (opens in new tab)</span></a></strong>.</li>\n\n\n\n<li><strong>\ud83d\udc7e</strong>&nbsp;&nbsp;LLMLingua has been integrated into&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/langchain-ai/langchain/blob/master/docs/docs/integrations/retrievers/llmlingua.ipynb\">LangChain<span class=\"sr-only\"> (opens in new tab)</span></a></strong>&nbsp;and&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/LongLLMLingua.ipynb\">LlamaIndex<span class=\"sr-only\"> (opens in new tab)</span></a></strong>, two widely-used RAG frameworks.</li>\n\n\n\n<li><strong>\ud83e\udd33</strong>&nbsp;&nbsp;Talk slides are available in&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://drive.google.com/file/d/1fzK3wOvy2boF7XzaYuq2bQ3jFeP1WMk3/view?usp=sharing\">AI Time Jan, 24<span class=\"sr-only\"> (opens in new tab)</span></a></strong>.</li>\n\n\n\n<li><strong>\ud83d\udda5</strong>&nbsp;&nbsp;EMNLP&#8217;23 slides are available in&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://drive.google.com/file/d/1GxQLAEN8bBB2yiEdQdW4UKoJzZc0es9t/view\">Session 5<span class=\"sr-only\"> (opens in new tab)</span></a></strong>&nbsp;and&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://drive.google.com/file/d/1LJBUfJrKxbpdkwo13SgPOqugk-UjLVIF/view\">BoF-6<span class=\"sr-only\"> (opens in new tab)</span></a></strong>.</li>\n\n\n\n<li><strong>\ud83d\udcda</strong>&nbsp;&nbsp;Check out our new&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://medium.com/@iofu728/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7\">blog post<span class=\"sr-only\"> (opens in new tab)</span></a></strong>&nbsp;discussing RAG benefits and cost savings through prompt compression. See the script example&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/LLMLingua/blob/main/examples/Retrieval.ipynb\">here<span class=\"sr-only\"> (opens in new tab)</span></a></strong>.</li>\n\n\n\n<li><strong>\ud83d\udc68\u200d\ud83e\uddaf</strong>&nbsp;&nbsp;Explore our&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/LLMLingua/blob/main/examples\">&#8216;./examples&#8217;<span class=\"sr-only\"> (opens in new tab)</span></a></strong>&nbsp;directory for practical applications, including&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/LLMLingua/blob/main/examples/LLMLingua2.ipynb\">LLMLingua-2<span class=\"sr-only\"> (opens in new tab)</span></a></strong>,&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/LLMLingua/blob/main/examples/RAG.ipynb\">RAG<span class=\"sr-only\"> (opens in new tab)</span></a></strong>,&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/LLMLingua/blob/main/examples/OnlineMeeting.ipynb\">Online Meeting<span class=\"sr-only\"> (opens in new tab)</span></a></strong>,&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/LLMLingua/blob/main/examples/CoT.ipynb\">CoT<span class=\"sr-only\"> (opens in new tab)</span></a></strong>,&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/LLMLingua/blob/main/examples/Code.ipynb\">Code<span class=\"sr-only\"> (opens in new tab)</span></a></strong>, and&nbsp;<strong><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/LLMLingua/blob/main/examples/RAGLlamaIndex.ipynb\">RAG using LlamaIndex<span class=\"sr-only\"> (opens in new tab)</span></a></strong>.</li>\n</ul>\n\n\n\n<figure class=\"wp-block-video aligncenter\"><video autoplay controls preload=\"auto\" src=\"https://github.com/microsoft/LLMLingua/assets/30883354/eb0ea70d-6d4c-4aa7-8977-61f94bb87438\" style=\"width:600pt;text-align: center\"></video></figure>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" />\n\n\n\n<h3 class=\"wp-block-heading\" id=\"insights\"><strong>Insights</strong></h3>\n\n\n\n<ul>\n<li>Natural language is redundant, amount of information varies.</li>\n\n\n\n<li>LLMs can understand compressed prompt.</li>\n\n\n\n<li>There is a trade-off between language completeness and compression ratio.&nbsp;<strong>(LLMLingua)</strong></li>\n\n\n\n<li>GPT-4 can recover all the key information from a compressed prompt-emergent ability.&nbsp;<strong>(LLMLingua)</strong></li>\n\n\n\n<li>The density and position of key information in a prompt affect the performance of downstream tasks.&nbsp;<strong>(LongLLMLingua)</strong></li>\n\n\n\n<li>GPT-4 can perform high quality, extractive prompt compression using carefully designed instruction and chunking.&nbsp;<strong>(LLMLingua-2)</strong></li>\n\n\n\n<li>Prompt compression can be formulated as a token classification problem and accomplished by a Bert size model.&nbsp;<strong>(LLMLingua-2)</strong></li>\n\n\n\n<li>Prompt compression can be formulated as a token classification problem and accomplished by a Bert size model.&nbsp;<strong>(LLMLingua-2)</strong></li>\n</ul>\n\n\n\n<p>For more details, please refer to the project pages,&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://llmlingua.com/llmlingua.html\"><strong>LLMLingua</strong><span class=\"sr-only\"> (opens in new tab)</span></a>,&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://llmlingua.com/longllmlingua.html\"><strong>LongLLMLingua</strong><span class=\"sr-only\"> (opens in new tab)</span></a>, and&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://llmlingua.com/llmlingua2.html\"><strong>LLMLingua-2</strong><span class=\"sr-only\"> (opens in new tab)</span></a>.</p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" />\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"786\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/motivation-1024x786.png\" alt=\"the motivation of LLMLingua\" class=\"wp-image-1016511\" style=\"width:750px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/motivation-1024x786.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/motivation-300x230.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/motivation-768x589.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/motivation-1536x1179.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/motivation-2048x1572.png 2048w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/motivation-80x60.png 80w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/motivation-235x180.png 235w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"573\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-1024x573.png\" alt=\"LLMLingua onepage\" class=\"wp-image-1016517\" style=\"width:800px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-1024x573.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-300x168.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-768x430.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-1536x859.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2048x1146.png 2048w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-240x134.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"574\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-1024x574.png\" alt=\"LongLLMLingua onepage\" class=\"wp-image-1016526\" style=\"width:800px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-1024x574.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-300x168.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-768x430.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-1536x860.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-2048x1147.png 2048w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-240x134.png 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-640x360.png 640w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"578\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-1024x578.png\" alt=\"LLMLingua-2 onepage\" class=\"wp-image-1016529\" style=\"width:800px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-1024x578.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-768x434.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-1536x867.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-2048x1157.png 2048w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-240x136.png 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-640x360.png 640w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" />\n\n\n\n\n\n<p>Paper: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.05736\">https://arxiv.org/abs/2310.05736<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<p>Demo: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://huggingface.co/spaces/microsoft/LLMLingua\">https://huggingface.co/spaces/microsoft/LLMLingua<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<p>Project Page: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://llmlingua.com/llmlingua.html\">https://llmlingua.com/llmlingua.html<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<p>To accelerate model inference and reduce cost, we introduce LLMLingua, which employs a well-trained small language model after alignment, such as GPT2-small or LLaMA-7B, detects unimportant tokens in the prompt and enables inference with the compressed prompt in black-box LLMs, achieving up to 20x compression with minimal performance loss. It&#8217;s worth noting that token-level compressed prompts are a format that is difficult for humans to understand but can be well interpreted by LLMs.</p>\n\n\n\n<p>To evaluate the effectiveness of compressed prompts, especially the unique capabilities of LLMs, we conducted experiments in four different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23, which cover ICL, Reasoning, Summarization, and Conversation. The results show that our approach can effectively retain the original prompt&#8217;s capabilities, particularly in ICL and reasoning.&nbsp;</p>\n\n\n\n<p>Furthermore, we demonstrated the efficiency and practical acceleration of LLMLingua through latency tests and computational workload estimation.</p>\n\n\n\n<p></p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"573\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-1024x573.png\" alt=\"LLMLingua onepage\" class=\"wp-image-1016517\" style=\"width:800px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-1024x573.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-300x168.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-768x430.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-1536x859.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2048x1146.png 2048w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-240x134.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" />\n\n\n\n<h3 class=\"wp-block-heading\" id=\"insights-1\"><strong>Insights</strong></h3>\n\n\n\n<ul>\n<li>Natural language is redundant, amount of information varies.</li>\n\n\n\n<li>LLMs can understand compressed prompt.</li>\n\n\n\n<li>There is a trade-off between language completeness and compression ratio.&nbsp;<strong>(LLMLingua)</strong></li>\n\n\n\n<li>GPT-4 can recover all the key information from a compressed prompt-emergent ability.&nbsp;<strong>(LLMLingua)</strong><a id=\"_msocom_1\"></a></li>\n</ul>\n\n\n\n<p>For more details, please refer to the paper&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.05736\"><strong>LLMLingua</strong><span class=\"sr-only\"> (opens in new tab)</span></a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"why-llmlingua\"><strong>Why&nbsp;<em>LLMLingua</em>?</strong></h3>\n\n\n\n<p>Building on the intuition mentioned earlier, LLMLingua leverages small models&#8217; perplexity to measure the redundancy within a prompt. It has designed three modules, as illustrated above, to assign varying compression rates to different segments within the prompt. This approach takes into account the conditional probabilities between compressed tokens and other tokens to better establish a sensitive distribution. Moreover, to make small models more attuned to various black-box models, LLMLingua introduces an alignment mechanism that aligns small models more closely with the semantic distributions of LLMs.</p>\n\n\n\n<p>LLMLingua offers the following advantages:</p>\n\n\n\n<ul>\n<li><strong>It can be directly used for black-box LLMs and helps save computation and financial costs, up to 20x.</strong></li>\n\n\n\n<li><strong>It is a highly robust method that requires no training of the LLMs and is applicable to different LLMs, such as GPT-4, GPT-3.5-Turbo, Claude, Mistral, etc.</strong></li>\n\n\n\n<li><strong><strong>After compression, it allows the model to support longer context inputs.</strong></strong></li>\n\n\n\n<li><strong>LLMLingua effectively retains the capabilities of LLMs, including reasoning, in-context learning, etc.</strong></li>\n\n\n\n<li><strong><strong>LLMLingua effectively retains the capabilities of LLMs, including reasoning, in-context learning, etc.</strong></strong></li>\n\n\n\n<li><strong>Prompts compressed by LLMLingua can be effectively decompressed by GPT-4, retaining vital information.</strong></li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"bibtex-1\"><strong>BibTeX</strong></h3>\n\n\n\n<pre class=\"wp-block-code\"><code>@inproceedings{jiang-etal-2023-llmlingua,\n    title = \"{LLML}ingua: Compressing Prompts for Accelerated Inference of Large Language Models\",\n    author = \"Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu\",\n    booktitle = \"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2023\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.emnlp-main.825\",\n    doi = \"10.18653/v1/2023.emnlp-main.825\",\n    pages = \"13358--13376\",\n}</code></pre>\n\n\n\n\n\n<p>Paper: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.06839 \">https://arxiv.org/abs/2310.06839 <span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<p>Project Pape: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://llmlingua.com/longllmlingua.html\">https://llmlingua.com/longllmlingua.html<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<p>In long context scenarios, large language models face three main challenges: higher computational cost, performance reduction, and position bias. Research indicates that LLM performance hinges on the density and position of key information in the input prompt. Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs\u2019 perception of the key information to simultaneously address the three challenges. Our extensive evaluation across various long context scenarios demonstrates that LongLLMLingua not only enhances performance but also significantly reduces costs and latency. For instance, in the NaturalQuestions benchmark,&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.06839\"><strong>LongLLMLingua</strong><span class=\"sr-only\"> (opens in new tab)</span></a>&nbsp;boosts performance by up to&nbsp;<strong>21.4%</strong>&nbsp;with around&nbsp;<strong>4x fewer tokens</strong>&nbsp;in GPT-3.5-Turbo, leading to substantial cost savings. It achieves a&nbsp;<strong>94.0% cost reduction</strong>&nbsp;in the LooGLE benchmark. Moreover, when compressing prompts of about 10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency by 1.4x-2.6x.</p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" />\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"574\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-1024x574.png\" alt=\"LongLLMLingua onepage\" class=\"wp-image-1016526\" style=\"width:800px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-1024x574.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-300x168.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-768x430.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-1536x860.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-2048x1147.png 2048w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-240x134.png 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua-640x360.png 640w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"insights-2\"><strong>Insights</strong></h3>\n\n\n\n<ul>\n<li>Natural language is redundant, amount of information varies.</li>\n\n\n\n<li>LLMs can understand compressed prompt.</li>\n\n\n\n<li>There is a trade-off between language completeness and compression ratio.&nbsp;<strong>(LLMLingua)</strong></li>\n\n\n\n<li>GPT-4 can recover all the key information from a compressed prompt-emergent ability.&nbsp;<strong>(LLMLingua)</strong></li>\n\n\n\n<li>The density and position of key information in a prompt affect the performance of downstream tasks.&nbsp;<strong>(LongLLMLingua)</strong></li>\n</ul>\n\n\n\n<p>For more details, please refer to the paper&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2310.06839\"><strong>LongLLMLingua</strong><span class=\"sr-only\"> (opens in new tab)</span></a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"why-longllmlingua\"><strong>Why&nbsp;<em>LongLLMLingua</em>?</strong></h3>\n\n\n\n<p>In long context scenarios, the distribution of key information is generally very sparse. Previous work has found that the density and placement of relevant information significantly impact the performance of Large Language Models (LLMs), even for highly powerful models like GPT-4-Turbo. LongLLMLingua capitalizes on these distribution characteristics by employing prompt compression and reorganization. This strategy schedules and utilizes the limited but powerful context windows for LLMs more efficiently, effectively mitigating the &#8220;Lost in the middle&#8221; issue. As illustrated in the figure above, LongLLMLingua can achieve up to a 21.4% improvement on the NQ Multi-document QA task while using only 1/4 of the tokens.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"544\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_Motivation-1024x544.png\" alt=\"the motivation of LongLLMLingua\" class=\"wp-image-1016580\" style=\"width:800px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_Motivation-1024x544.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_Motivation-300x159.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_Motivation-768x408.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_Motivation-1536x816.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_Motivation-240x128.png 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_Motivation.png 1914w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<p><strong>Our main contributions are&nbsp;five-fold</strong>:</p>\n\n\n\n<ul>\n<li>We propose a&nbsp;<strong>question-aware coarse-to-fine compression method</strong>&nbsp;to improve the key information density in the prompt.</li>\n\n\n\n<li>We introduce a&nbsp;<strong>document reordering strategy</strong>&nbsp;to minimize position bias in LLMs.</li>\n\n\n\n<li>We establish&nbsp;<strong>dynamic compression ratios</strong>&nbsp;for precise control between coarse and fine compression levels</li>\n\n\n\n<li>We propose a&nbsp;<strong>post-compression subsequence recovery strategy</strong>&nbsp;to improve the integrity of the key information</li>\n\n\n\n<li>We evaluate LongLLMLingua across&nbsp;<strong>five benchmarks</strong>, i.e., NaturalQuestions, LongBench, ZeroSCROLLS , MuSicQue, and LooGLE, covering a variety of long context scenarios. Experimental results reveal that LongLLMLingua\u2019s compressed prompts outperform original prompts in terms of performance, cost efficiency, and system latency.</li>\n</ul>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"560\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_framework-1024x560.png\" alt=\"diagram\" class=\"wp-image-1016583\" style=\"width:800px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_framework-1024x560.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_framework-300x164.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_framework-768x420.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_framework-1536x840.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_framework-2048x1120.png 2048w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_framework-240x131.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"empirical-studies-of-question-aware-compression\"><strong>Empirical Studies of Question-aware Compression</strong></h3>\n\n\n\n<p>To test the effectiveness of our proposed question-aware coarse-grained and fine-grained compression method, we conducted an empirical study across two dimensions.<br>Firstly, we analyzed the effectiveness of the question-aware coarse-grained approach by comparing it with several state-of-the-art (SoTA) retrieval methods in real Retrieval-Augmented Generation (RAG) scenarios. We discovered that our method not only surpasses traditional retrieval methods such as BM25 and Gzip but also outperforms embedding methods like OpenAI embedding, Jina, and BGE, as well as various reranker methods, including Cohere reranker and BGE-Reranker.<br>Secondly, we assessed the effectiveness of the question-aware fine-grained approach by comparing perplexity and contrastive perplexity across various document context scenarios. It was observed that contrastive perplexity effectively captures key information in documents, while perplexity struggles to identify relevant information.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"500\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_empirical-1024x500.png\" alt=\"chart\" class=\"wp-image-1016586\" style=\"width:800px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_empirical-1024x500.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_empirical-300x147.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_empirical-768x375.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_empirical-1536x750.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_empirical-2048x1000.png 2048w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LongLLMLingua_empirical-240x117.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"bibtex\"><strong>BibTeX</strong></h3>\n\n\n\n<pre class=\"wp-block-code\"><code>@article{jiang-etal-2023-longllmlingua,\n    title = \"LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression\",\n    author = \"Huiqiang Jiang and Qianhui Wu and and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu\",\n    url = \"https://arxiv.org/abs/2310.06839\",\n    journal = \"ArXiv preprint\",\n    volume = \"abs/2310.06839\",\n    year = \"2023\",\n}</code></pre>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" />\n\n\n\n\n\n<p>Paper: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2403.12968\">https://arxiv.org/abs/2403.12968<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<p>Project Page: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://llmlingua.com/llmlingua2.html\">https://llmlingua.com/llmlingua2.html<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<p>Demo: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://huggingface.co/spaces/microsoft/llmlingua-2\">https://huggingface.co/spaces/microsoft/llmlingua-2<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"why-llmlingua-2\"><strong>Why&nbsp;<em>LLMLingua-2?</em></strong></h3>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"578\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-1024x578.png\" alt=\"LLMLingua-2 Onepage\" class=\"wp-image-1016529\" style=\"width:800px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-1024x578.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-768x434.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-1536x867.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-2048x1157.png 2048w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-240x136.png 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/LLMLingua-2-640x360.png 640w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<p>Challenges Encountered in Information Entropy Based Methods\u200b:</p>\n\n\n\n<ul>\n<li><strong>\ud83e\udd14</strong>&nbsp;&nbsp;Perplexity or information entropy may be suboptimal for prompt trimming:&nbsp;<strong>Not aligned with the prompt compression objective.</strong> </li>\n\n\n\n<li>\ud83e\udd16 How can we identify or build a suitable dataset to <strong>align the SLM</strong> <strong>towards effective prompt compression</strong>?</li>\n\n\n\n<li><strong>\u27a1\ufe0f</strong>&nbsp;&nbsp;Importance of tokens is context-dependent. Causal LMs&nbsp;<strong>only leverage unidirectional context</strong>, which may fail to capture all essential information within the context.</li>\n\n\n\n<li>\ud83d\udd04 How can we design a compression algorithm that effectively leverage the full bidirectional context?</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"why-data-distillation\"><strong>Why Data Distillation?</strong></h3>\n\n\n\n<p>Shortcomings of Existing Text Compression Datasets \u200b:</p>\n\n\n\n<ul>\n<li><strong>\ud83d\ude22</strong>&nbsp;&nbsp;Most text compression datasets are&nbsp;<strong>abstractive</strong>, which leads to&nbsp;<strong>slow autoregressive process</strong>&nbsp;and may&nbsp;<strong>produce hallucinated content</strong>.</li>\n\n\n\n<li><strong>\ud83e\udd37\u200d\u2642\ufe0f</strong>&nbsp;&nbsp;Extractive compression datasets such as&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://aclanthology.org/D13-1155/\"><strong>SentComp</strong><span class=\"sr-only\"> (opens in new tab)</span></a>&nbsp;and&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://aclanthology.org/2020.argmining-1.1/\"><strong>DebateSum</strong><span class=\"sr-only\"> (opens in new tab)</span></a>&nbsp;are mainly created for the summarization task and often lack detailed information. In the case of prompt compression, we should&nbsp;<strong>retain essential information</strong>&nbsp;as much as possible.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"bibtex-2\"><strong>BibTeX</strong></h3>\n\n\n\n<pre class=\"wp-block-code\"><code>@article{wu2024llmlingua2,\n    title = \"{LLML}ingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression\",\n    author = \"Zhuoshi Pan and Qianhui Wu and Huiqiang Jiang and Menglin Xia and Xufang Luo and Jue Zhang and Qingwei Lin and Victor Ruhle and Yuqing Yang and Chin-Yew Lin and H. Vicky Zhao and Lili Qiu and Dongmei Zhang\",\n    url = \"https://arxiv.org/abs/2403.12968\",\n    journal = \"ArXiv preprint\",\n    volume = \"abs/2403.12968\",\n    year = \"2024\",\n}</code></pre>\n\n\n\n\n\n<p></p>\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Effectively Deliver Information to LLMs via&nbsp;Prompt Compression LLMLingua Read More (opens in new tab) LongLLMLingua Read More (opens in new tab) LLMLingua-2 Read More (opens in new tab) Large language models (LLMs) have demonstrated remarkable capabilities and have been applied across various fields. Advancements in technologies such as Chain-of-Thought (CoT), In-Context Learning (ICL), and Retrieval-Augmented [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 969012,
        "date": "2023-10-18T03:33:20",
        "slug": "ccedit",
        "title": "CCEdit",
        "link": "https://www.microsoft.com/en-us/research/project/ccedit/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1371\" height=\"913\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/09/bg_img_video_editing.png\" class=\"attachment-full size-full\" alt=\"background pattern\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/09/bg_img_video_editing.png 1371w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/bg_img_video_editing-300x200.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/bg_img_video_editing-1024x682.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/bg_img_video_editing-768x511.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/bg_img_video_editing-240x160.png 240w\" sizes=\"(max-width: 1371px) 100vw, 1371px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"ccedit\">CCEdit</h1>\n\n\n\n<p>Creative and Controllable AI Video Editing</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p class=\"has-text-align-left\"><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.youtube.com/watch?v=UQw4jq-igN4\">Demo<span class=\"sr-only\"> (opens in new tab)</span></a>              <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2309.16496\">Paper<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<p>CCEdit is a comprehensive generative video editing framework meticulously designed to strike a harmonious balance between controllability and creativity while accommodating a wide range of editing requirements. CCEdit enables enhanced creative control through an innovative approach that decouples frame structure and appearance. We leverage the foundational ControlNet architecture to preserve structural integrity, while seamlessly integrating adaptable temporal modules compatible with state-of-the-art personalization techniques for text-to-image generation, such as DreamBooth and LoRA. Furthermore, we introduce reference-conditioned video editing, empowering users to exercise precise creative control over video editing through the more manageable process of editing key frames. </p>\n\n\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"951\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/teaser-652fb41e90aa3-1024x951.png\" alt=\"CCEdit is a comprehensive framework meticulously designed to strike a harmonious balance between controllability and creativity while accommodating a wide range of editing requirements.\" class=\"wp-image-977133\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/teaser-652fb41e90aa3-1024x951.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/teaser-652fb41e90aa3-300x279.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/teaser-652fb41e90aa3-768x713.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/teaser-652fb41e90aa3-194x180.png 194w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/teaser-652fb41e90aa3.png 1395w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Creative and Controllable AI Video Editing Demo (opens in new tab) Paper (opens in new tab) CCEdit is a comprehensive generative video editing framework meticulously designed to strike a harmonious balance between controllability and creativity while accommodating a wide range of editing requirements. CCEdit enables enhanced creative control through an innovative approach that decouples frame [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 967329,
        "date": "2023-10-16T02:14:29",
        "slug": "domain-specialization",
        "title": "Domain Specialization",
        "link": "https://www.microsoft.com/en-us/research/project/domain-specialization/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-catalina-blue card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"domain-specialization\">Domain Specialization</h1>\n\n\n\n<p>Scaling performance beyond Moore&#8217;s law</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Domain&nbsp;specialization&nbsp;is&nbsp;expected&nbsp;to&nbsp;play&nbsp;a&nbsp;big&nbsp;role&nbsp;in&nbsp;how&nbsp;computer&nbsp;systems&nbsp;evolve&nbsp;in&nbsp;future. With&nbsp;the&nbsp;end&nbsp;of&nbsp;Moore&#8217;s&nbsp;law,&nbsp;we&nbsp;are&nbsp;already&nbsp;seeing&nbsp;CPU,&nbsp;GPU&nbsp;and domain specific&nbsp;hardware&nbsp;evolving&nbsp;rapidly. The next decade&nbsp;is&nbsp;therefore&nbsp;expected&nbsp;to&nbsp;see&nbsp;big&nbsp;changes&nbsp;in&nbsp;how&nbsp;we&nbsp;develop,&nbsp;compile&nbsp;and&nbsp;run&nbsp;software. This project focuses on data systems, a class of systems where, as the data sizes grow, performance scaling is going to be of importance.<br><br>First,&nbsp;we&nbsp;believe&nbsp;that domain-specific&nbsp;compilers&nbsp;will&nbsp;play&nbsp;a&nbsp;crucial&nbsp;strategic&nbsp;role&nbsp;in&nbsp;helping&nbsp;software&nbsp;leverage&nbsp;the&nbsp;changing&nbsp;hardware&nbsp;landscape. Such compilers will be multi-layered and will progressively lower computation through multiple intermediate abstractions, performing domain specific optimizations at the higher layers and specializing code to the hardware in lower layers. We&nbsp;have&nbsp;been&nbsp;working&nbsp;on&nbsp;two&nbsp;such&nbsp;domain&nbsp;specific&nbsp;compilers&nbsp;in&nbsp;the&nbsp;data&nbsp;domain. Second, new hardware specific algorithms need to be designed to harness the hardware&#8217;s capabilities. To this end, we are designing new algorithms that can effectively harness the raw capabilities of a GPU irrespective of its architecture.</p>\n\n\n\n<p></p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized is-style-spectrum\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"589\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/image-652cf670dbed7-1024x589.png\" alt=\"diagram\" class=\"wp-image-976389\" style=\"width:700px;height:403px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/image-652cf670dbed7-1024x589.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/image-652cf670dbed7-300x173.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/image-652cf670dbed7-768x442.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/image-652cf670dbed7-240x138.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/image-652cf670dbed7.png 1128w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><strong>The landscape of domain specialization.</strong></figcaption></figure>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Scaling performance beyond Moore&#8217;s law Domain&nbsp;specialization&nbsp;is&nbsp;expected&nbsp;to&nbsp;play&nbsp;a&nbsp;big&nbsp;role&nbsp;in&nbsp;how&nbsp;computer&nbsp;systems&nbsp;evolve&nbsp;in&nbsp;future. With&nbsp;the&nbsp;end&nbsp;of&nbsp;Moore&#8217;s&nbsp;law,&nbsp;we&nbsp;are&nbsp;already&nbsp;seeing&nbsp;CPU,&nbsp;GPU&nbsp;and domain specific&nbsp;hardware&nbsp;evolving&nbsp;rapidly. The next decade&nbsp;is&nbsp;therefore&nbsp;expected&nbsp;to&nbsp;see&nbsp;big&nbsp;changes&nbsp;in&nbsp;how&nbsp;we&nbsp;develop,&nbsp;compile&nbsp;and&nbsp;run&nbsp;software. This project focuses on data systems, a class of systems where, as the data sizes grow, performance scaling is going to be of importance. First,&nbsp;we&nbsp;believe&nbsp;that domain-specific&nbsp;compilers&nbsp;will&nbsp;play&nbsp;a&nbsp;crucial&nbsp;strategic&nbsp;role&nbsp;in&nbsp;helping&nbsp;software&nbsp;leverage&nbsp;the&nbsp;changing&nbsp;hardware&nbsp;landscape. Such compilers will be multi-layered and will progressively lower computation through multiple intermediate abstractions, performing domain specific [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 973047,
        "date": "2023-10-06T15:16:20",
        "slug": "autogen",
        "title": "AutoGen",
        "link": "https://www.microsoft.com/en-us/research/project/autogen/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AutoGen_header_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"AutoGen header graphic showing relationship between User, Commander, Writer, and Safeguard\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AutoGen_header_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AutoGen_header_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AutoGen_header_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AutoGen_header_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AutoGen_header_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AutoGen_header_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/AutoGen_header_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"autogen\">AutoGen</h1>\n\n\n\n<p>Build multi-agent GenAI applications<br><a href=\"mailto:autogen@microsoft.com\">autogen@microsoft.com</a></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>AutoGen provides a multi-agent conversation framework as a high-level abstraction. It is an open-source library for enabling next-generation LLM applications with multi-agent collaborations, teachability and personalization. With this framework, users can build LLM workflows. The agent modularity and conversation-based programming simplifies development and enables reuse for developers. End-users benefit from multiple agents independently learning and collaborating on their behalf, enabling them to accomplish more with less work. Benefits of the multi agent approach with AutoGen include agents that can be backed by various LLM configurations; native support for a generic form of tool usage through code generation and execution; and, a special agent, the Human Proxy Agent that enables easy integration of human feedback and involvement at different levels.</p>\n\n\n\n<div class=\"wp-block-media-text has-video  has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile\" data-bi-an=\"media-text\"><figure class=\"wp-block-media-text__media video-wrapper\"><iframe class=\"media-text__video\" src=\"https://www.youtube-nocookie.com/embed/DXhqhpHWRuM?enablejsapi=1&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></figure><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<h3 class=\"wp-block-heading\" id=\"easily-build-llm-workflows\">Easily build LLM workflows</h3>\n\n\n\n<p>With AutoGen, building a complex multi-agent conversation system boils down to:</p>\n\n\n\n<ul>\n<li>Defining a set of agents with specialized capabilities and roles.</li>\n\n\n\n<li>Defining the interaction behavior between agents, i.e., what to reply when an agent receives messages from another agent.</li>\n</ul>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-fill-github\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://github.com/microsoft/autogen\" target=\"_blank\" rel=\"noreferrer noopener\" data-bi-cn=\"Easily build LLM workflows\">AutoGen</a></div>\n\n\n\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/publication/autogen-enabling-next-gen-llm-applications-via-multi-agent-conversation-framework/\" data-bi-cn=\"Easily build LLM workflows\">Read the paper</a></div>\n</div>\n</div></div>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"related-projects\">Related projects</h2>\n\n\n\n<p>AutoGen is an open-source, community-driven project under active development (as a spinoff from&nbsp;<a href=\"https://www.microsoft.com/en-us/research/project/flaml/\">FLAML</a>, a fast library for automated machine learning and tuning), which encourages contributions from individuals of all backgrounds. Many Microsoft Research collaborators have made great contributions to this project, including academic contributors like Pennsylvania State University and the University of Washington, and product teams like Microsoft Fabric and ML.NET. AutoGen aims to provide an effective and easy-to-use framework for developers to build next-generation applications, and already demonstrates promising opportunities to build creative applications and provide a large space for innovation.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/project/flaml/\">More about FLAML</a></div>\n</div>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Build multi-agent GenAI applicationsautogen@microsoft.com AutoGen provides a multi-agent conversation framework as a high-level abstraction. It is an open-source library for enabling next-generation LLM applications with multi-agent collaborations, teachability and personalization. With this framework, users can build LLM workflows. The agent modularity and conversation-based programming simplifies development and enables reuse for developers. End-users benefit from multiple [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 973743,
        "date": "2023-10-06T11:19:49",
        "slug": "llava-large-language-and-vision-assistant",
        "title": "LLaVA: Large Language and Vision Assistant",
        "link": "https://www.microsoft.com/en-us/research/project/llava-large-language-and-vision-assistant/",
        "content": "\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\"><section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 \">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h3 class=\"wp-block-heading\" id=\"building-next-gen-multimodal-foundation-models-for-general-purpose-assistants\">Building Next-Gen Multimodal Foundation Models for General-Purpose Assistants</h3>\n\n\n\n<p>LLaVA is an open-source project, collaborating with research community to advance the state-of-the-art in AI. LLaVA represents the first end-to-end trained large multimodal model (LMM) that achieves impressive chat capabilities mimicking spirits of the multimodal GPT-4. The LLaVA family continues growing to support more modalities, capabilities, applications and beyond.</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n</div>\n</div>\n\n\n\n\n\n<p>LLaVA represents a cost-efficient approach to building general-purpose multimodal assistant. It is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-122 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:100%\">\n<div class=\"wp-block-group is-horizontal is-content-justification-center is-nowrap is-layout-flex wp-container-core-group-is-layout-18 wp-block-group-is-layout-flex\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-120 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:100%\"></div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-121 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:100%\">\n<figure class=\"wp-block-image size-full is-resized is-style-spectrum\"><img loading=\"lazy\" decoding=\"async\" width=\"697\" height=\"252\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/mfm_evolution.jpeg\" alt=\"evolution\" class=\"wp-image-973812\" style=\"width:669px;height:undefinedpx\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/mfm_evolution.jpeg 697w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/mfm_evolution-300x108.jpeg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/mfm_evolution-240x87.jpeg 240w\" sizes=\"(max-width: 697px) 100vw, 697px\" /></figure>\n</div>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"recent-development\">Recent development</h3>\n\n\n\n<ul>\n<li><strong>LLaVA</strong>: The first open-source project to GPT-V alternative. [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://llava-vl.github.io/\" target=\"_blank\" rel=\"noreferrer noopener\">Project<span class=\"sr-only\"> (opens in new tab)</span></a>] [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2304.08485\" target=\"_blank\" rel=\"noreferrer noopener\">Paper<span class=\"sr-only\"> (opens in new tab)</span></a>] [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/haotian-liu/LLaVA\" target=\"_blank\" rel=\"noreferrer noopener\">Github<span class=\"sr-only\"> (opens in new tab)</span></a>] [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://llava.hliu.cc/\" target=\"_blank\" rel=\"noreferrer noopener\">Demo<span class=\"sr-only\"> (opens in new tab)</span></a>] [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K\" target=\"_blank\" rel=\"noreferrer noopener\">Data<span class=\"sr-only\"> (opens in new tab)</span></a>] [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0\" target=\"_blank\" rel=\"noreferrer noopener\">Model<span class=\"sr-only\"> (opens in new tab)</span></a>] [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2309.09958\" target=\"_blank\" rel=\"noreferrer noopener\">Scaling Note<span class=\"sr-only\"> (opens in new tab)</span></a>]</li>\n\n\n\n<li><strong>LLaVA-Med</strong>: The first multimodal assistant in the healthcare domain [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://aka.ms/llava-med\">Github<span class=\"sr-only\"> (opens in new tab)</span></a>] [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2306.00890\">Paper<span class=\"sr-only\"> (opens in new tab)</span></a>]</li>\n\n\n\n<li><strong>LLaVA-Interactive</strong>: An all-in-one demo to demonstrate the visual interaction/generation capabilities beyond language interaction alone, supported by <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/haotian-liu/LLaVA\">LLaVA<span class=\"sr-only\"> (opens in new tab)</span></a>, <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once\">SEEM<span class=\"sr-only\"> (opens in new tab)</span></a> and <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/gligen/GLIGEN\">GLIGEN<span class=\"sr-only\"> (opens in new tab)</span></a>.</li>\n\n\n\n<li><strong>Multimodal Foundation Models</strong>: A 118-page survey on the evolution, trends and our position of multimodal foundation models:&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2309.10020\" target=\"_blank\" rel=\"noreferrer noopener\">&#8220;Multimodal Foundation Models: From Specialists to General-Purpose Assistants&#8221;<span class=\"sr-only\"> (opens in new tab)</span></a>. This is based on <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://vlp-tutorial.github.io/2023\" target=\"_blank\" rel=\"noreferrer noopener\">CVPR 2023 Tutorial<span class=\"sr-only\"> (opens in new tab)</span></a>. [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2306.14895\" target=\"_blank\" rel=\"noreferrer noopener\">Note on Large Multimodal Models<span class=\"sr-only\"> (opens in new tab)</span></a>] [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">Slides<span class=\"sr-only\"> (opens in new tab)</span></a>] [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://youtu.be/mkI7EPD1vp8\" target=\"_blank\" rel=\"noreferrer noopener\">YouTube<span class=\"sr-only\"> (opens in new tab)</span></a>] [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.bilibili.com/video/BV1Ng4y1T7v3/\" target=\"_blank\" rel=\"noreferrer noopener\">Bilibili<span class=\"sr-only\"> (opens in new tab)</span></a>]</li>\n\n\n\n<li><strong>Instruction Tuning with GPT-4</strong>: the &#8220;first attempt&#8221; to use GPT-4 data for LLM self-instruct tuning. [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://instruction-tuning-with-gpt-4.github.io/\">Project<span class=\"sr-only\"> (opens in new tab)</span></a>] [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2304.03277\" target=\"_blank\" rel=\"noreferrer noopener\">Paper<span class=\"sr-only\"> (opens in new tab)</span></a>] [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM\" target=\"_blank\" rel=\"noreferrer noopener\">Github<span class=\"sr-only\"> (opens in new tab)</span></a>] [<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.linkedin.com/feed/update/urn:li:activity:7049992414995902464/\" target=\"_blank\" rel=\"noreferrer noopener\">My Learnings<span class=\"sr-only\"> (opens in new tab)</span></a>]</li>\n</ul>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-123 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:100%\"></div>\n</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>LLaVA is an open-source project, collaborating with research community to advance the state-of-the-art in AI. LLaVA represents the first end-to-end trained large multimodal model (LMM) that achieves impressive chat capabilities mimicking spirits of the multimodal GPT-4. The LLaVA family continues growing to support more modalities, capabilities, applications and beyond. LLaVA represents a cost-efficient approach to [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 971055,
        "date": "2023-10-04T10:18:10",
        "slug": "physics-of-agi",
        "title": "Physics of AGI",
        "link": "https://www.microsoft.com/en-us/research/project/physics-of-agi/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/10/Physics-of-AGI_header_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"Physics of AGI - abstract background with logo element\" style=\"object-position: 84% 50%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/10/Physics-of-AGI_header_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/10/Physics-of-AGI_header_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/10/Physics-of-AGI_header_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/10/Physics-of-AGI_header_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/10/Physics-of-AGI_header_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/10/Physics-of-AGI_header_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/10/Physics-of-AGI_header_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading has-text-align-left\" id=\"physics-of-agi\">Physics of AGI</h1>\n\n\n\n<p class=\"has-text-align-left\">Understanding and improving emergent intelligence in large language models</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<div class=\"wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile is-style-border\" style=\"grid-template-columns:30% auto\" data-bi-an=\"media-text\"><figure class=\"wp-block-media-text__media\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"1024\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/physics_agi.jpg\" alt=\"AI-generated image of a group of scientists standing at desks, reviewing a large whiteboard\" class=\"wp-image-972384 size-full\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/physics_agi.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/physics_agi-300x300.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/physics_agi-150x150.jpg 150w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/physics_agi-768x768.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/physics_agi-180x180.jpg 180w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/physics_agi-360x360.jpg 360w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<h2 class=\"wp-block-heading\" id=\"physics-of-agi-1\">Physics of AGI</h2>\n\n\n\n<p>We&#8217;re a group of scientists working at Microsoft Research, trying to understand how intelligence emerges in large language models (LLMs), and use this understanding to improve that intelligence.</p>\n</div></div>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Understanding and improving emergent intelligence in large language models We&#8217;re a group of scientists working at Microsoft Research, trying to understand how intelligence emerges in large language models (LLMs), and use this understanding to improve that intelligence. Opens in a new tab</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 968667,
        "date": "2023-10-04T00:44:27",
        "slug": "ai-infrastructure",
        "title": "AI Infrastructure",
        "link": "https://www.microsoft.com/en-us/research/project/ai-infrastructure/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-catalina-blue card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"ai-infrastructure\">AI Infrastructure</h1>\n\n\n\n<p>Towards efficient AI/ML deployment</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"431\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/09/ai-infra-banner-1024x431.png\" alt=\"Our research focuses on optimizing the training and inference of AI/ML workloads. We are also interested in exploring model and hardware innovation to drive down the cost of AI.\" class=\"wp-image-968682\" style=\"aspect-ratio:2.376181474480151;width:877px;height:auto\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/09/ai-infra-banner-1024x431.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/ai-infra-banner-300x126.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/ai-infra-banner-768x323.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/ai-infra-banner-1536x647.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/ai-infra-banner-2048x862.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/ai-infra-banner-665x280.png 665w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/ai-infra-banner-240x101.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<p></p>\n\n\n\n<p>The AI Infrastructure team at Microsoft Research India&nbsp;works on cutting-edge systems optimizations for improving the efficiency of a variety of AI/ML workloads, including an emerging class of workloads, namely, serving large language models (LLMs). AI/ML models are expensive to train and serve at scale and therefore, systems optimizations are crucial for unlocking the true potential of AI-powered applications. The key principle behind many of our projects is co-design of Systems and ML, i.e., we leverage deep domain knowledge of AI/ML workloads to design and improve the efficiency of these systems. Below is a summary of some of our current projects.&nbsp;</p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/publication/vattention-dynamic-memory-management-for-serving-llms-without-pagedattention/\"><strong>vAttention</strong><span class=\"sr-only\"> (opens in new tab)</span></a> is a dynamic KV cache memory allocator for serving Large Language Models. Compared to the popular PagedAttention model (pioneered by <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://dl.acm.org/doi/10.1145/3600006.3613165\">vLLM<span class=\"sr-only\"> (opens in new tab)</span></a>) that requires application-level changes, vAttention supports dynamic memory allocation transparently by leveraging low-level system support for demand paging. vAttention reduces software complexity while improving portability and performance. For example, vAttention unlocks the full potential of <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/Dao-AILab/flash-attention\" target=\"_blank\" rel=\"noreferrer noopener\">FlashAttention<span class=\"sr-only\"> (opens in new tab)</span></a>&#8216;s and <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/flashinfer-ai/flashinfer\" target=\"_blank\" rel=\"noreferrer noopener\">FlashInfer&#8217;s<span class=\"sr-only\"> (opens in new tab)</span></a> optimized kernels&nbsp;to significantly improve LLM serving&nbsp;throughput over vLLM.&nbsp;</p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/publication/vidur-a-large-scale-simulation-framework-for-llm-inference/\"><strong>VIDUR</strong><span class=\"sr-only\"> (opens in new tab)</span></a> is a high-fidelity and extensible simulation framework for LLM inference. It models the performance of LLM operators using a combination of experimental profiling and predictive modeling, and evaluates the end-to-end inference performance. It also includes a configuration search tool &#8211; Vidur-Search &#8211; that helps optimize LLM deployment. Vidur-Search automatically identifies the most cost-effective deployment configuration that meets application performance constraints. For example, Vidur-Search finds the best deployment configuration for LLaMA2-70B in one hour on a CPU machine, in contrast to a deployment-based exploration which would require 10s of thousands of GPU hours. Source available at <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/vidur\">https://github.com/microsoft/vidur<span class=\"sr-only\"> (opens in new tab)</span></a>.</p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/publication/just-in-time-checkpointing-low-cost-error-recovery-from-deep-learning-training-failures/\"><strong>Just-in-Time Checkpointing</strong></a> Training Deep Neural Networks (DNNs) requires frequent checkpointing to mitigate the effect of failures. However, for large models, periodic checkpointing incurs significant steady state overhead, and during recovery, a large number of GPUs need to redo work since the last checkpoint. We present a novel approach of just-in-time checkpointing, which enables recovery from failures with just a single minibatch iteration of work replayed by all GPUs. This approach reduces the cost of error recovery from several minutes to a few seconds per GPU, with nearly zero steady state overhead.</p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/publication/sarathi-efficient-llm-inference-by-piggybacking-decodes-with-chunked-prefills/\"><strong>SARATHI</strong></a> is a technique to improve the throughput of LLM inference. Time to serve an LLM inference is dominated by the memory bandwidth on GPUs due to the auto-regressive nature of transformer-based architectures that generate one token at a time. We show that the decode cost per-token can be as high as 200 times the prefill (the initial phase wherein all the tokens of a query are processed in parallel)\u00a0cost per token. Furthermore, the processing time of different batches can differ substantially due to varying number of tokens in different requests. This behavior introduces pipeline bubbles making it difficult to scale LLM inference on multiple nodes. \u00a0\u00a0</p>\n\n\n\n<p>SARATHI addresses these challenges with two novel techniques: (1) chunked-prefills, which splits a prefill request into equal sized chunks, and (2) decode-maximal batching, which constructs a batch using a single prefill chunk and populates the remaining slots with decodes. During inference, the prefill chunk saturates GPU compute, while the decode requests &#8216;piggyback&#8217; and cost up to an order of magnitude less compared to a decode-only batch. This approach allows constructing multiple decode-maximal batches from a single prefill request, maximizing coverage of decodes that can piggyback. Furthermore, the uniform&nbsp;work items&nbsp;of these batches ameliorate the imbalance between micro-batches, significantly reducing pipeline bubbles. Together, these optimizations substantially improve the decoding throughput of large language models across different models and GPUs.&nbsp;</p>\n\n\n\n<p><strong>CEISER</strong> takes an alternate approach to serving large classification models. State-of-the-art ML models for vision and language tasks promise ever-increasing accuracy, but deploying these large models for inference is often computationally expensive, with a prohibitive deployment cost. The usual mitigation is to distil the large model, i.e. decrease its size, and hence its computational cost for the specific task at hand, but this approach typically compromises inference accuracy. In CEISER (Cost Effective Inference SERving), we argue for an alternative to distilling large models: reducing inference cost, without compromising accuracy, by using ensembles of smaller models to achieve the desired overall accuracy. CEISER uses three techniques to achieve this. First, it introduces a novel ensemble selection algorithm that identifies the most cost-effective ensemble from the set of all possible ensembles for a given task using a bounded exploration strategy. Next, it determines the optimal compute-aware batch size that improves latency without compromising throughput. Finally, it performs input-aware scheduling of queries for tasks that are sensitive to the input distribution. Our experiments across different ML tasks and input loads on a physical cluster of 32 V100 GPUs and large-scale simulated clusters show that&nbsp;CEISER&nbsp;can reduce the monetary cost of inference serving by up to 50% compared to both the single largest model and prior work like Cocktail while providing the same accuracy as the best single model.&nbsp;</p>\n\n\n\n<p><strong><a href=\"https://www.microsoft.com/en-us/research/publication/renee-end-to-end-training-of-extreme-classification-models-2/\">Renee</a></strong> is an end-to-end system for training Extreme Multi-label Classification (XC) models. The goal of XC is to learn representations that enable mapping input texts to the most relevant subset of labels selected from an extremely large label set, potentially in hundreds of millions. Given the extreme scale, conventional wisdom believes it is infeasible to train an XC model in an end-to-end manner. Thus, for training efficiency, several modular and sampling-based approaches to XC training have been proposed in the literature. In Renee, we identify challenges in the end-to-end training of XC models and devise novel optimizations that improve training speed over an order of magnitude, making end-to-end XC model training practical. Furthermore, we show that our end-to-end trained model, Renee\u00b4 delivers state-of-the-art accuracy in a wide variety of XC benchmark datasets.&nbsp;</p>\n\n\n\n<p><strong>Systems+ML co-design: </strong>The massive scale of cloud infrastructure services enables, and often necessitates, vertical co-design of the infrastructure stack by cloud providers.&nbsp;Our approach to co-design extracts efficiency out of existing software infrastructure layers by making lightweight changes to generic software interfaces. We explore&nbsp;co-design in the context of several systems: <a href=\"https://www.microsoft.com/en-us/research/publication/singularity-planet-scale-preemptive-and-elastic-scheduling-of-ai-workloads/\">Singularity</a>, <a href=\"https://www.microsoft.com/en-us/research/publication/varuna-scalable-low-cost-training-of-massive-deep-learning-models/\">Varuna</a>, <a href=\"https://www.microsoft.com/en-us/research/publication/gandiva-introspective-cluster-scheduling-for-deep-learning/\">Gandiva</a> and <a href=\"https://www.microsoft.com/en-us/research/publication/quiver-an-informed-storage-cache-for-deep-learning/\">Quiver</a>. </p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/publication/singularity-planet-scale-preemptive-and-elastic-scheduling-of-ai-workloads/\">Singularity</a> is a globally distributed scheduling service for highly efficient and reliable execution of deep learning training and inference workloads. At the heart of Singularity is a novel, workload-aware scheduler that can transparently preempt and elastically scale deep learning workloads to drive high utilization without impacting their correctness or performance, across a global fleet of AI accelerators (e.g., GPUs, FPGAs).  </p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/publication/varuna-scalable-low-cost-training-of-massive-deep-learning-models/\">Varuna</a> is a system that enables training massive deep learning models on commodity networking. Varuna makes thrifty use of networking resources and automatically configures the user\u2019s training job to efficiently use any given set of resources. Therefore, Varuna is able to leverage \u201clow-priority&#8221; VMs that cost about 5x cheaper than dedicated GPUs, thus significantly reducing the cost of training massive models.<a href=\"http:\" target=\"_blank\" rel=\"noreferrer noopener\"> </a></p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/project/gandiva-scheduler-for-dnns/\" target=\"_blank\" rel=\"noreferrer noopener\">Gandiva</a>&nbsp;is a co-designed cluster scheduler for running deep learning training jobs. Gandiva leverages the predictability of DNN training jobs to make the scheduler \u201ccontinuous\u201d and \u201cintrospective\u201d. </p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/publication/quiver-an-informed-storage-cache-for-deep-learning/\" target=\"_blank\" rel=\"noreferrer noopener\">Quiver</a>&nbsp;is an informed storage cache for deep learning jobs in a cluster of GPUs. Quiver employs domain-specific intelligence within the caching layer, to achieve much higher efficiency compared to a generic storage cache.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Towards efficient AI/ML deployment The AI Infrastructure team at Microsoft Research India&nbsp;works on cutting-edge systems optimizations for improving the efficiency of a variety of AI/ML workloads, including an emerging class of workloads, namely, serving large language models (LLMs). AI/ML models are expensive to train and serve at scale and therefore, systems optimizations are crucial for [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 971832,
        "date": "2023-10-02T08:22:51",
        "slug": "oppertune",
        "title": "OPPerTune",
        "link": "https://www.microsoft.com/en-us/research/project/oppertune/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-catalina-blue card-background--inset-right\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading is-style-default\" id=\"oppertune\">OPPerTune</h1>\n\n\n\n<p>Post-Deployment Configuration Tuning of Services Made Easy</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Real-world application deployments have hundreds of inter-dependent configuration parameters, many of which significantly influence performance and efficiency. With today&#8217;s complex and dynamic services, operators need to continuously monitor and set the right configuration values (<em>configuration tuning</em>) well after a service is widely deployed. This is challenging since experimenting with different configurations post-deployment may reduce application performance or cause disruptions.</p>\n\n\n\n<p>While state-of-the-art ML approaches do help to automate configuration tuning, they do not fully address the multiple challenges in end-to-end configuration tuning of deployed applications.</p>\n\n\n\n<p>We present OPPerTune, a service that enables configuration tuning of applications in deployment at Microsoft. OPPerTune reduces application interruptions while maximizing the performance of deployed applications as and when the workload or the underlying infrastructure changes. It automates three essential processes that facilitate post-deployment configuration tuning:</p>\n\n\n\n<ol>\n<li>Determining which configurations to tune</li>\n\n\n\n<li>Automatically managing the scope at which to tune the configurations, and</li>\n\n\n\n<li>Using a novel reinforcement learning algorithm to simultaneously and quickly tune numerical and categorical configurations, thereby keeping the overhead of configuration tuning low.</li>\n</ol>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized is-style-default\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"526\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/Oppertune-architecture-1024x526.png\" alt=\"diagram, schematic\" class=\"wp-image-971901\" style=\"width:685px;height:352px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/Oppertune-architecture-1024x526.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/Oppertune-architecture-300x154.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/Oppertune-architecture-768x395.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/Oppertune-architecture-1536x789.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/Oppertune-architecture-2048x1052.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/Oppertune-architecture-240x123.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\">OPPerTune service architecture &#8211; Applications create tuning instances on the server to tune various configuration parameters.<br>Autoscoper helps automatically create, manage, and scope tuning instances based on the application\u2019s dynamic context.<br>Selector helps pick the most promising configuration parameters to tune.</figcaption></figure>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Post-Deployment Configuration Tuning of Services Made Easy Real-world application deployments have hundreds of inter-dependent configuration parameters, many of which significantly influence performance and efficiency. With today&#8217;s complex and dynamic services, operators need to continuously monitor and set the right configuration values (configuration tuning) well after a service is widely deployed. This is challenging since experimenting [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 971610,
        "date": "2023-09-28T22:51:42",
        "slug": "pwr",
        "title": "Programming with Representations (PwR)",
        "link": "https://www.microsoft.com/en-us/research/project/pwr/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"2560\" height=\"1707\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/09/CLO20b_Madeleine_Evan_office_001-scaled.jpg\" class=\"attachment-full size-full\" alt=\"A professional software development team\" style=\"object-position: 41% 62%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/09/CLO20b_Madeleine_Evan_office_001-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/CLO20b_Madeleine_Evan_office_001-300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/CLO20b_Madeleine_Evan_office_001-1024x683.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/CLO20b_Madeleine_Evan_office_001-768x512.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/CLO20b_Madeleine_Evan_office_001-1536x1024.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/CLO20b_Madeleine_Evan_office_001-2048x1365.jpg 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/CLO20b_Madeleine_Evan_office_001-240x160.jpg 240w\" sizes=\"(max-width: 2560px) 100vw, 2560px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"programming-with-representations-pwr\">Programming with Representations (PwR)</h1>\n\n\n\n<p>PwR aims to redefine software methodology for the generative AI era, enabling professional software teams to process a high volume of changes, faster and better.</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>The increasing capabilities of generative AI, especially in coding-related tasks, offers us the opportunity to reimagine software development in a world where AI and humans work closely together. This project studies the topic of <em>conversational programming</em> where a developer uses AI to write software using natural language instructions. While natural language can facilitate AI-developer interaction, it also introduces the potential for misinterpreting tasks. Existing solutions address this gap by prompting AI to communicate its understanding in a structured natural-language document. This document can then be inspected, edited, and approved by the developer. While effective, the developer still needs to vet the resulting AI-generated code for safety and reliability, requiring both domain and coding expertise. Our goal is to decouple this requirement, paving the way for numerous organizations and individuals, including those without coding expertise, to develop software.</p>\n\n\n\n<p>Programming with Representations (PwR, pronounced \u201cpower\u201d) is a software development approach that relies on a domain-specific language (DSL), or representation, defined by a developer specializing in a specific domain. This representation includes built-in guardrails that are automatically implemented throughout the software development process. Once a representation is defined for a domain, PwR enables any developer interested in that domain to translate their intentions using natural language into a program in that representation.</p>\n\n\n\n<p> </p>\n\n\n\n<p></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Programming with Representation (PwR) aims to redefine software methodology for the generative AI era, enabling professional software teams to process a high volume of changes and requirements, faster and better.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 788159,
        "date": "2023-09-25T21:53:00",
        "slug": "agent-ai",
        "title": "Agent AI",
        "link": "https://www.microsoft.com/en-us/research/project/agent-ai/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 align-self-center\">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading is-style-default\" id=\"agent-ai\">Agent AI</h1>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>      Agent-based multimodal AI systems are becoming a ubiquitous presence in our everyday lives. A promising direction for making these systems more interactive is to embody them as agents within specific environments. The grounding of large foundation models to act as agents within specific environments can provide a way of incorporating visual and contextual information into an embodied system. For example, a system that can perceive user actions, human behavior, environment objects, audio expressions, and the collective sentiment of a scene can be used to inform and direct agent responses within the given environment. Emergent Agent AI as an interactive system that can perceive visual stimuli, language inputs, or other environmentally-grounded data and can produce meaningful actions, manipulation, navigation, gesture, etc. In particular, we focus on improving upon agents based on next action predication by incorporating external knowledge, multimodality, and human feedback obtained by the interactive agent. We argue that by developing agentic AI systems in grounded environments, we will also minimize the hallucinations of large foundation models and their ability to generate incorrect outputs. To accelerate research on embodied agent intelligence, we propose a new project on <strong>General Embodied Agent AI</strong>, which focuses on the broader embodied and agentic aspects of multimodal interactions.</p>\n\n\n\n<figure class=\"wp-block-image alignright size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"1015\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Agentcate-1024x1015.png\" alt=\"Agent\" class=\"wp-image-1009659\" style=\"width:305px;height:auto\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Agentcate-1024x1015.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Agentcate-300x297.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Agentcate-150x150.png 150w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Agentcate-768x761.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Agentcate-182x180.png 182w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/Agentcate.png 1459w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<p></p>\n\n\n\n<p>The related papers are published: </p>\n\n\n\n<p>1) <a href=\"https://www.microsoft.com/en-us/research/publication/embodied-agent-ai/\">Agent AI Towards a Holistic Intelligence</a></p>\n\n\n\n<p>2) <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/pdf/2402.05929.pdf\">Agent foundation model<span class=\"sr-only\"> (opens in new tab)</span></a> for embodied interaction in Robot, Gaming, and Healthcare;  </p>\n\n\n\n<p>3) <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/pdf/2309.09971.pdf\">Multi-agent for Gaming<span class=\"sr-only\"> (opens in new tab)</span></a> (GPT-4) in simulation and real infrastructure;</p>\n\n\n\n<p>4) <a href=\"https://www.microsoft.com/en-us/research/publication/reinforced-cross-modal-matching-and-self-supervised-imitation-learning-forvision-language-navigation/\">Navigation Agent for Robotics;</a></p>\n\n\n\n<p>5) <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2311.12015\">GPT4V agent for Robotics<span class=\"sr-only\"> (opens in new tab)</span></a> </p>\n\n\n\n<p>6) <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2401.03568\">Agent AI<span class=\"sr-only\"> (opens in new tab)</span></a> Survey and GPT 4V for Robotics, Gaming, and Healthcare.</p>\n\n\n\n<p><strong><em>Community building: </em></strong></p>\n\n\n\n<p>In addition, we will organize <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://multimodalagentai.github.io/\">CVPR2024 Tutorial on Generalist Agent AI<span class=\"sr-only\"> (opens in new tab)</span></a>, and will release two embodied new datasets &#8211; <em>CuisineWorld</em>  and <em>VideoAnalytica</em> &#8211; with a set of baseline models, encouraging researchers across the world to develop new models and systems, and explore ways to evaluate and improve upon performance in our agent-based multimodal leaderboard.</p>\n\n\n\n<p>To push the frontier of this important area, this project aims at bringing researchers and practitioners in the relevant embodied agent AI together, to share ideas and insights. This is an emerging research area that poses new challenges for embodied AI systems and there is still significant room for improvement. A deeper understanding between audio, vision and language has also started to play a key role in human-machine interaction systems. Our project will greatly advance large foundation model technologies including cross-modality understanding and agnostic reality integration, generic agent information and human-aesthetic evaluation.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"716\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/12/EmergentAgentAI-657efcb33d5b4-1024x716.png\" alt=\"Agent AI\" class=\"wp-image-993330\" style=\"width:831px;height:auto\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/12/EmergentAgentAI-657efcb33d5b4-1024x716.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/EmergentAgentAI-657efcb33d5b4-300x210.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/EmergentAgentAI-657efcb33d5b4-768x537.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/EmergentAgentAI-657efcb33d5b4-1536x1074.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/EmergentAgentAI-657efcb33d5b4-2048x1432.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/EmergentAgentAI-657efcb33d5b4-240x168.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\"><em>Agent AI is emerging as a promising route for early progress on the path to Artificial General Intelligence (AGI). The Agent AI training process has been shown to demonstrate an ability for multi-modal understanding in the physical world, and provides a framework for reality-agnostic training by leveraging generative AI alongside multiple independent sources of data. Large foundation models trained for agent and action-related tasks can be applied to physical and virtual/simulated worlds when trained with cross-reality training data. We present the general overview of an Agent AI system that can perceive and act in many different domains and applications, possibly serving as a route towards AGI using an agent paradigm.</em><br> <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/pdf/2401.03568.pdf\">2401.03568.pdf (arxiv.org)<span class=\"sr-only\"> (opens in new tab)</span></a></figcaption></figure>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Agent-based multimodal AI systems are becoming a ubiquitous presence in our everyday lives. A promising direction for making these systems more interactive is to embody them as agents within specific environments. The grounding of large foundation models to act as agents within specific environments can provide a way of incorporating visual and contextual information into [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 967350,
        "date": "2023-09-12T02:52:36",
        "slug": "967350",
        "title": "AI-Driven Software Engineering",
        "link": "https://www.microsoft.com/en-us/research/project/967350/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"ai-driven-software-engineering\">AI-Driven Software Engineering</h1>\n\n\n\n<p></p>\n\n\n\n<p>Using AI to assist every developer build better software faster</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Generative AI is transforming the way software is built. We conduct research at the forefront of this transformation. We design ML models, algorithms and platforms to improve developer productivity and software reliability. Check out the publications tab to learn more.</p>\n\n\n\n\n\n<p></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Using AI to assist every developer build better software faster Generative AI is transforming the way software is built. We conduct research at the forefront of this transformation. We design ML models, algorithms and platforms to improve developer productivity and software reliability. Check out the publications tab to learn more. Opens in a new tab</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 959313,
        "date": "2023-08-13T16:18:27",
        "slug": "speechx",
        "title": "SpeechX",
        "link": "https://www.microsoft.com/en-us/research/project/speechx/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1970\" height=\"1522\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/iStock-1056703886.jpg\" class=\"attachment-full size-full\" alt=\"waveform\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/iStock-1056703886.jpg 1970w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/iStock-1056703886-300x232.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/iStock-1056703886-1024x791.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/iStock-1056703886-768x593.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/iStock-1056703886-1536x1187.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/iStock-1056703886-233x180.jpg 233w\" sizes=\"(max-width: 1970px) 100vw, 1970px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"speechx\">SpeechX</h1>\n\n\n\n<p>Neural Codec Language Model as a Versatile Speech Transformer</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>SpeechX is a versatile speech generation model leveraging audio and text prompts, which can deal with both clean and noisy speech inputs and perform zero-shot TTS and various tasks involving transforming the input speech. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting. This enables unified treatment of various tasks in an extensible manner, providing a consistent way of leveraging text input for speech enhancement and transformation. &nbsp;The current model, trained on 60K hours of speech audio, can perform zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing, where the spoken content can be altered while preserving the speaker and background sounds.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button\" id=\"https://www.microsoft.com/en-us/research/publication/speechx-neural-codec-language-model-as-a-versatile-speech-transformer/\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://arxiv.org/abs/2308.06873\" target=\"_blank\" rel=\"https://www.microsoft.com/en-us/research/publication/speechx-neural-codec-language-model-as-a-versatile-speech-transformer/ noopener\">Read the paper</a></div>\n</div>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center\" id=\"approach-1\">Model overview</h2>\n\n\n\n<p>SpeechX is a neural codec language model based on audio and text prompts and incorporates special tokens for task-dependent prompting, which helps the model to determine what a desired output is. The current model was trained on 60K hours of data from LibriLight.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"888\" height=\"606\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/model1.jpg\" alt=\"SpeechX Model\" class=\"wp-image-960708\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/model1.jpg 888w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/model1-300x205.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/model1-768x524.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/model1-240x164.jpg 240w\" sizes=\"(max-width: 888px) 100vw, 888px\" /></figure>\n\n\n\n<div style=\"height:100px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center\" id=\"multiple-tasks-with-one-model\">Multiple tasks with one model</h2>\n\n\n\n<p>SpeechX deals with various input-output transformation relationships by employing a generic language modeling architecture using acoustic and textual tokens.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"384\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/speechx_tasks-64d9b2efb98da-1024x384.png\" alt=\"Multiple tasks with one model\" class=\"wp-image-962094\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/speechx_tasks-64d9b2efb98da-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/speechx_tasks-64d9b2efb98da-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/speechx_tasks-64d9b2efb98da-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/speechx_tasks-64d9b2efb98da-1536x577.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/speechx_tasks-64d9b2efb98da-2048x769.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/speechx_tasks-64d9b2efb98da-1920x720.png 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/speechx_tasks-64d9b2efb98da-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/speechx_tasks-64d9b2efb98da-240x90.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:100px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center\" id=\"applications-demo\">Applications / demo</h2>\n\n\n\n<p>Below, we included audio samples demonstrating how SpeechX performs in various speech-processing tasks. The audio files were normalized in amplitude and resampled at 16 kHz for listening. The speech samples and transcripts were taken from LibriSpeech test-clean. The speech samples below are provided for the sole purpose of illustrating SpeechX.</p>\n\n\n\n<div style=\"height:48px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<title>Speech Generation Tasks</title>\n\n\n<div style=\"width: 100%;margin: 0 auto\">\n\n    <!-- Zero-shot TTS -->\n    <div style=\"margin-bottom: 50px\">\n        <h5>Zero-shot TTS (Text To Speech)</h5>\n        <p>SpeechX synthesizes speech in the style specified by an audio prompt</p>\n        <div style=\"border-bottom: 2px solid black;margin-bottom: 2px\"></div>\n        <div style=\"background-color: #E6E6FA;padding: 20px;border-radius: 5px;max-width: 80%;margin: 20px auto\">\n\n            <table style=\"width: 100%;border-collapse: collapse;border: 1px solid;border: none\">\n                <thead>\n                    <tr style=\"border-bottom: 2px solid black\">\n                        <th style=\"text-align: center;padding: 8px;width:25%\">Text</th>\n                        <th style=\"text-align: center;padding: 8px;width:25%\">Prompt (speaker)</th>\n                        <th style=\"text-align: center;padding: 8px;width:25%\">SpeechX output</th>\n                        <th style=\"text-align: center;padding: 8px;width:25%\">Ground truth</th>\n                    </tr>\n                </thead>\n                <tbody>\n                    \n                    <!--2-->\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">miss de graf said kenneth noticing the boy&#8217;s face\n                            critically as he stood where the light from the passage fell upon it</td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tts_sample6_6829-68769-0018_enroll_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tts_sample6_6829-68769-0018_predicted_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tts_sample6_6829-68769-0018_clean_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n<!-- 1-->\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">the paris plant like that at the crystal palace was a\n                            temporary exhibit</td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tts_sample3_2300-131720-0000_enroll_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tts_sample3_2300-131720-0000_predicted_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tts_sample3_2300-131720-0000_clean_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <!-- 3-->\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">that summer&#8217;s emigration however being mainly from the\n                            free states greatly changed the relative strength of the two parties</td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tts_sample2_7729-102255-0002_enroll_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tts_sample2_7729-102255-0002_predicted_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tts_sample2_7729-102255-0002_clean_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">it is my heart hung in the sky and no clouds ever\n                            float between the grave flowers and my heart on high</td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tts_sample1_8555-292519-0007_enroll_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tts_sample1_8555-292519-0007_predicted_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tts_sample1_8555-292519-0007_clean_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n\n\n\n                    <!-- ... Add more rows as necessary ... -->\n                </tbody>\n            </table>\n        </div>\n    </div>\n\n    <!-- Clean speech editing -->\n    <div style=\"margin-bottom: 50px\">\n        <h5>Spoken content editing</h5>\n        <p>SpeechX helps correct misspoken words.</p>\n        <div style=\"border-bottom: 2px solid black;margin-bottom: 2px\"></div>\n        <div style=\"background-color: #E6E6FA;padding: 20px;border-radius: 5px;max-width: 80%;margin: 20px auto\">\n\n            <table style=\"width: 100%;border-collapse: collapse\">\n                <thead>\n                    <tr style=\"border-bottom: 2px solid black\">\n                        <th style=\"text-align: center;padding: 8px;width:25%\">Original Text</th>\n                        <th style=\"text-align: center;padding: 8px;width:25%\">Edited Text</th>\n                        <th style=\"text-align: center;padding: 8px;width:25%\">Original speech</th>\n                        <th style=\"text-align: center;padding: 8px;width:25%\">SpeechX output</th>\n                    </tr>\n                </thead>\n                <tbody>\n                      <!--3-->\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">\n\n                            cotton is a wonderful thing is it not boys she said rather primly\n\n                        </td>\n                        <td style=\"text-align: left;padding: 8px\">\n\n                            cotton is a wonderful thing <b> with its soft and breathable texture </b> she said rather primly\n\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/1995-1826-0019-pre-edit_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/1995-1826-0019-post-edit_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    \n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">\n\n                            gold is the most common metal in the land of oz and is used for many purposes because it is soft and pliable\n\n                        </td>\n                        <td style=\"text-align: left;padding: 8px\"> <b> in the land of oz gold is the prevalent metal and is utilized for various reasons </b> because it is soft and pliable.\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/1284-1181-0004-pre-edit_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/1284-1181-0004-post-edit_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n<!--1-->\n                     <tr>\n                        <td style=\"text-align: left;padding: 8px\">\n\n                            its jaw is enormous and according to naturalists it is armed with no less than one hundred\n                            and eighty two teeth\n\n                        </td>\n                        <td style=\"text-align: left;padding: 8px\">its jaw is enormous and according to naturalists it is\n                            armed with no <b> more than five hundred and thirty three </b> teeth\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_clean_editing_sample4_260-123286-0028-pre-edit_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_clean_editing_sample4_260-123286-0028-post-edit_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                   \n                   <!--4-->\n                   <tr>\n                    <td style=\"text-align: left;padding: 8px\">shame on you citizens cried he i blush for my fellows\n                        of nottingham</td>\n                    <td style=\"text-align: left;padding: 8px\"><b>citizens you should be ashamed</b> cried he i blush\n                        for my fellows of nottingham</td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_clean_editing_sample1_61-70968-0020-pre-edit_normalized.wav\"></audio>\n                    </td>\n                    <td style=\"text-align: center;padding: 8px\">\n                        <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_clean_editing_sample1_61-70968-0020-post-edit_normalized.wav\"></audio>\n                    </td>\n                </tr>\n                    <!-- ... Add more rows as necessary ... -->\n                </tbody>\n            </table>\n        </div>\n    </div>\n    <!-- Noisy speech editing -->\n    <div style=\"margin-bottom: 50px\">\n        <h5>Background-preserving spoken content editing</h5>\n        <p>SpeechX naturally corrects misspoken words by preserving original ambience</p>\n        <div style=\"border-bottom: 2px solid black;margin-bottom: 2px\"></div>\n        <div style=\"background-color: #E6E6FA;padding: 20px;border-radius: 5px;max-width: 80%;margin: 20px auto\">\n\n            <table style=\"width: 100%;border-collapse: collapse\">\n                <thead>\n                    <tr style=\"border-bottom: 2px solid black\">\n                        <th style=\"text-align: center;padding: 8px;width:25%\">Original Text</th>\n                        <th style=\"text-align: center;padding: 8px;width:25%\">Edited Text</th>\n                        <th style=\"text-align: center;padding: 8px;width:25%\">Original speech</th>\n                        <th style=\"text-align: center;padding: 8px;width:25%\">SpeechX output</th>\n                    </tr>\n                </thead>\n                <tbody>\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">we will go out together to the bower there is a way\n                            down to the court from my window</td>\n                        <td style=\"text-align: left;padding: 8px\">we will go out together to the bower there is <b>a\n                                pathway down to the courtyard</b></td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_noisy_editing_sample1_61-70970-0016-pre-edit_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_noisy_editing_sample1_61-70970-0016-post-edit_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">she has been dead these twenty years</td>\n                        <td style=\"text-align: left;padding: 8px\">she has been dead <b>thirty nine</b> years</td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_noisy_editing_sample2_121-127105-0009-pre-edit_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_noisy_editing_sample2_121-127105-0009-post-edit_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">\n\n                            its origin was small a germ an insignificant seed hardly to be thought of as likely to\n                            arouse opposition\n\n\n\n                        </td>\n                        <td style=\"text-align: left;padding: 8px\">\n\n                            its origin was small a germ an insignificant <b> seed always expected to provoke strong </b>\n                            opposition\n\n\n\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_noisy_editing_sample3_4077-13751-0001-pre-edit_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_noisy_editing_sample3_4077-13751-0001-post-edit_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">\n\n                            he was a fanatic on formality and he only addressed me in the third person to the point\n                            where it got tiresome\n\n                        </td>\n                        <td style=\"text-align: left;padding: 8px\">\n\n                            he was <b>obsessive about protocol and always spoke to </b> me in the third person to the\n                            point where it got tiresome\n\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_noisy_editing_sample4_8463-294828-0013-pre-edit_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_noisy_editing_sample4_8463-294828-0013-post-edit_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <!-- ... Add more rows as necessary ... -->\n                </tbody>\n            </table>\n        </div>\n    </div>\n    <!-- Noise suppression -->\n    <div style=\"margin-bottom: 50px\">\n        <h5>Noise suppression</h5>\n        <p>SpeechX removes unwanted background sounds that have been mixed into your recordings. Text input is optional,\n            but it helps.</p>\n        <div style=\"border-bottom: 2px solid black;margin-bottom: 2px\"></div>\n        <div style=\"background-color: #E6E6FA;padding: 20px;border-radius: 5px;max-width: 80%;margin: 20px auto\">\n\n            <table style=\"width: 100%;border-collapse: collapse\">\n                <thead>\n                    <tr style=\"border-bottom: 2px solid black\">\n                        <th style=\"text-align: center;padding: 8px;width:25%\">Text</th>\n                        <th style=\"text-align: center;padding: 8px;width:25%\">Noisy speech</th>\n                        <th style=\"text-align: center;padding: 8px;width:25%\">SpeechX output</th>\n                        <th style=\"text-align: center;padding: 8px;width:25%\">Ground truth</th>\n                    </tr>\n                </thead>\n                <tbody>\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">secure as he thought in the careful administration of\n                            justice in that city and the character of its well disposed inhabitants the good hidalgo was\n                            far from thinking that any disaster could befal his family</td>\n                        <td style=\"text-align: center;text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_fg_sample8_5639-40744-0001_noisy_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_fg_sample8_5639-40744-0001_prediction_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_fg_sample8_5639-40744-0001_clean_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">one day when the boy was sent by his grandfather with\n                            a message to a relation he passed along a street in which there was a great concourse of\n                            horsemen</td>\n                        <td style=\"text-align: center;text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_fg_sample9_5639-40744-0024_noisy_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_fg_sample9_5639-40744-0024_decompressed_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_fg_sample9_5639-40744-0024_clean_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">the ideas also remain but they have become types in\n                            nature forms of men animals birds fishes</td>\n                        <td style=\"text-align: center;text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_fg_sample3_2961-960-0014_noisy_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_fg_sample3_2961-960-0014_prediction_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_fg_sample3_2961-960-0014_clean_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">the salient features of this development of domestic\n                            service have already been indicated</td>\n                        <td style=\"text-align: center;text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_fg_sample6_3570-5696-0004_noisy_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_fg_sample6_3570-5696-0004_prediction_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_fg_sample6_3570-5696-0004_clean_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                </tbody>\n            </table>\n        </div>\n    </div>\n\n    <!-- Target speaker extraction -->\n    <div style=\"margin-bottom: 50px\">\n        <h5>Target speaker extraction</h5>\n        <p>SpeechX zeros in on one person in a mixture of voices</p>\n        <div style=\"border-bottom: 2px solid black;margin-bottom: 2px\"></div>\n        <div style=\"background-color: #E6E6FA;padding: 20px;border-radius: 5px;max-width: 80%;margin: 20px auto\">\n\n            <table style=\"width: 100%;border-collapse: collapse\">\n                <thead>\n                    <tr style=\"border-bottom: 2px solid black\">\n                        <th style=\"text-align: center;padding: 8px;width:40%\">Text</th>\n                        <th style=\"text-align: center;padding: 8px;width:15%\">Prompt (speaker)</th>\n                        <th style=\"text-align: center;padding: 8px;width:15%\">Mixed speech</th>\n                        <th style=\"text-align: center;padding: 8px;width:15%\">SpeechX output</th>\n                        <th style=\"text-align: center;padding: 8px;width:15%\">Ground truth</th>\n                    </tr>\n                </thead>\n                <tbody>\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">i allude to the goddess</br></td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample3_7127-75947-0005_enroll_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample3_7127-75947-0005_mixed_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample3_7127-75947-0005_predicted_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample3_7127-75947-0005_clean_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">at that moment the gentleman entered bearing a huge\n                            object concealed by a piece of green felt</td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample2_4992-41806-0012_enroll_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample2_4992-41806-0012_mixed_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample2_4992-41806-0012_predicted_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample2_4992-41806-0012_clean_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">I knew nothing of the doctrine of faith because we\n                            were taught sophistry instead of certainty and nobody understood spiritual boasting</td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample1_2830-3980-0019_enroll_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample1_2830-3980-0019_mixed_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample1_2830-3980-0019_predicted_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample1_2830-3980-0019_clean_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n\n                    <tr>\n                        <td style=\"text-align: left;padding: 8px\">it is my heart hung in the sky and no clouds ever\n                            float between the grave flowers and my heart on high</td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample4_8555-292519-0007_enroll_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample4_8555-292519-0007_mixed_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample4_8555-292519-0007_predicted_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_tse_sample4_8555-292519-0007_clean_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <!-- ... more rows ... -->\n                </tbody>\n            </table>\n        </div>\n    </div>\n\n    <!-- Speech removal -->\n    <div style=\"margin-bottom: 50px\">\n        <h5>Speech removal</h5>\n        <p>SpeechX can naturally erase human voices for audio redaction</p>\n        <div style=\"border-bottom: 2px solid black;margin-bottom: 2px\"></div>\n        <div style=\"background-color: #E6E6FA;padding: 20px;border-radius: 5px;max-width: 80%;margin: 20px auto\">\n\n            <table style=\"width: 100%;border-collapse: collapse\">\n                <thead>\n                    <tr style=\"border-bottom: 2px solid black\">\n                        <th style=\"text-align: center;padding: 8px;width:33.33%\">Noisy speech</th>\n                        <th style=\"text-align: center;padding: 8px;width:33.33%\">SpeechX output</th>\n                        <th style=\"text-align: center;padding: 8px;width:33.33%\">Ground truth</th>\n                    </tr>\n                </thead>\n                <tbody>\n                    <tr>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_bg_sample1_61-70968-0049_noisy_normalized_final.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_bg_sample1_61-70968-0049_prediction_normalized_final.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_bg_sample1_61-70968-0049_noise_ref_normalized_final.wav\"></audio>\n                        </td>\n                    </tr>\n                    <tr>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_bg_sample2_61-70968-0005_noisy_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_bg_sample2_61-70968-0005_prediction_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_bg_sample2_61-70968-0005_noise_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <tr>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_bg_sample3_4992-41806-0003_noisy_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_bg_sample3_4992-41806-0003_prediction_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_bg_sample3_4992-41806-0003_noise_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <tr>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_bg_sample4_121-127105-0036_noisy_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_bg_sample4_121-127105-0036_prediction_normalized.wav\"></audio>\n                        </td>\n                        <td style=\"text-align: center;padding: 8px\">\n                            <audio controls style=\"width: 150px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/task_ns_bg_sample4_121-127105-0036_noise_ref_normalized.wav\"></audio>\n                        </td>\n                    </tr>\n                    <!-- ... more rows ... -->\n                </tbody>\n            </table>\n        </div>\n    </div>\n\n\n\n\n</div>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h2 class=\"wp-block-heading has-text-align-center\" id=\"ethics-statement\">Ethics statement</h2>\n\n\n\n<p>SpeechX could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While SpeechX can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Neural Codec Language Model as a Versatile Speech Transformer SpeechX is a versatile speech generation model leveraging audio and text prompts, which can deal with both clean and noisy speech inputs and perform zero-shot TTS and various tasks involving transforming the input speech. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting. [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 960144,
        "date": "2023-08-13T04:05:49",
        "slug": "dragnuwa",
        "title": "DragNUWA",
        "link": "https://www.microsoft.com/en-us/research/project/dragnuwa/",
        "content": "\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\"><section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"2560\" height=\"1600\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/hills-2836301.jpg\" class=\"attachment-full size-full\" alt=\"DragNUWA background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/hills-2836301.jpg 2560w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/hills-2836301-300x188.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/hills-2836301-1024x640.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/hills-2836301-768x480.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/hills-2836301-1536x960.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/hills-2836301-2048x1280.jpg 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/hills-2836301-240x150.jpg 240w\" sizes=\"(max-width: 2560px) 100vw, 2560px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"dragnuwa\">DragNUWA</h1>\n\n\n\n<p>DragNUWA is a video generation model that utilizes text, images, and trajectory as three essential control factors to facilitate highly controllable video generation.</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n</div>\n\n\n\n\n\n<p><strong>DragNUWA </strong>is a video generation model that utilizes<strong> text, images, and trajectory</strong> as three essential control factors to facilitate <strong>highly controllable video generation</strong> from semantic, spatial, and temporal aspects. Distinct from existing research, DragNUWA enables users to manipulate backgrounds or objects within images directly, and the model seamlessly translates these actions into camera movements or object motions, generating the corresponding video.</p>\n\n\n\n<div class=\"annotations \" data-bi-aN=\"citation\">\n\t<ul class=\"annotations__list card depth-16 bg-body p-4 \">\n\t\t<li class=\"annotations__list-item\">\n\t\t\t\t\t\t<span class=\"annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small\">Publication</span>\n\t\t\t<a href=\"https://www.microsoft.com/en-us/research/publication/dragnuwa-fine-grained-control-in-video-generation-by-integrating-text-image-and-trajectory/\" target=\"_self\" class=\"annotations__link font-weight-semibold text-decoration-none\" data-bi-type=\"annotated-link\" aria-label=\"DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory\" data-bi-aN=\"citation\" data-bi-cN=\"DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory\">\n\t\t\t\tDragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory&nbsp;<span class=\"glyph-append glyph-append-chevron-right glyph-append-xsmall\"></span>\n\t\t\t</a>\n\t\t\t\t\t</li>\n\t</ul>\n</div>\n\n\n\n<p>Click the <strong>top-left &#8220;play&#8221; button</strong> to observe how DragNUWA manipulates the same image to create videos with desired camera movements and object motions.</p>\n\n\n\n<p><img loading=\"lazy\" decoding=\"async\" width=\"2057\" height=\"720\" class=\"wp-image-961641\" style=\"width: 2000px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Fig1-64d79c3b13269.gif\" alt=\"DragNUWA-Fig1\"></p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<p><img loading=\"lazy\" decoding=\"async\" width=\"1366\" height=\"720\" class=\"wp-image-961977\" style=\"width: 2000px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Fig2-64d8b745bdc9c.gif\" alt=\"DragNUWA-Fig1\"></p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>DragNUWA is a video generation model that utilizes text, images, and trajectory as three essential control factors to facilitate highly controllable video generation. DragNUWA is a video generation model that utilizes text, images, and trajectory as three essential control factors to facilitate highly controllable video generation from semantic, spatial, and temporal aspects. Distinct from existing [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 958836,
        "date": "2023-08-11T06:56:17",
        "slug": "hyway",
        "title": "HyWay",
        "link": "https://www.microsoft.com/en-us/research/project/hyway/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay_banner_v1.jpg\" class=\"attachment-full size-full\" alt=\"a group of people sitting at a desk looking at a laptop\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay_banner_v1.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay_banner_v1-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay_banner_v1-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay_banner_v1-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay_banner_v1-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay_banner_v1-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay_banner_v1-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading h2\" id=\"hyway-hybrid-hallway\">HyWay: Hybrid Hallway</h1>\n\n\n\n<p>A platform to enable mingling between in-person (physical) and remote (virtual) users. </p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<h3 class=\"wp-block-heading\" id=\"context\">Context</h3>\n\n\n\n<p>As normalcy has returned after the COVID-19 pandemic, offices have reopened, and events have returned to a physical format. However, the convenience and cost savings of remote work have not been forgotten. So, we have entered the era of hybrid work and events, with many employers allowing employees the flexibility of mixing remote work with in-person work, and events such as conferences routinely allowing remote attendance alongside in-person participation.</p>\n\n\n\n<p>Mainstream tools such as Teams and Zoom are designed for planned and structured meetings, as are newer tools such as the ones noted below. What\u2019s missing is the support for unplanned, unstructured, and in-the-moment interactions, which we term \u201cmingling\u201d.</p>\n\n\n\n<p>During the pandemic, tools such as Discord, Gather, and SpatialChat became popular for virtual gathering experiences. What\u2019s missing in these virtual-only tools, however, is the support for hybrid interactions due to the inherent limitations of their design.</p>\n\n\n\n<p>We believe that <em>hybrid </em>interaction is <em>more than being just about meetings</em>. This motivates our work on HyWay.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"overview\">Overview</h3>\n\n\n\n<p>We have developed HyWay, short for &#8220;Hybrid Hallway&#8221;, to enable mingling between in-person (physical) and remote (virtual) users in semi-structured and unstructured settings. Examples of semi-structured settings include tradeshows and birds-of-a-feather mixer events, where participants congregate around topics or content but retain the ability of move around flexibly. Examples of unstructured settings include the office watercooler and the conference hallway during breaks, which tend to be even more fluid in terms of user participation and mobility than semi-structured settings.</p>\n\n\n\n<p>We take inspiration from the physical setting, where users are comfortable mingling because the interactions tend to be surprise-free and fluid. The goal of HyWay is to recreate this as best of possible in hybrid settings where some users are remote. Ambient <strong><em>awareness</em> </strong>ensures that users know of the presence of others around, not just those they may be talking to but also others in the vicinity. They are often also able to overhear nearby conversation(s). If someone they see or something they overhear catches their interest, users have the <strong><em>agency</em> </strong>to move over and join that conversation. The <strong><em>agility</em> </strong>manifests itself in multiple ways in the design of HyWay: in our use of commodity hardware which facilitates scaling and deployment; not requiring the in-person users to install an app or even carry a device, which helps with frictionless bootstrapping to attain critical mass; and employing a deploy-learn-refine flywheel to improve the system continually.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"design\">Design</h3>\n\n\n\n<p>In HyWay, remote users navigate a map-based representation of the physical space, bump into in-person users who are mingling in the hallway and engage with them via large HyWay-enabled displays (\u201cphysical zones\u201d; see the left image below) that are placed in the physical spaces. Their ability to explore and discover is enabled by the map-based interface, which displays face bubbles for both in-person and remote users (see the right image below). </p>\n\n\n\n<p>The design of HyWay doesn&#8217;t impose much at all on the in-person users. Very little changes for them \u2014 they engage with each other just as in a purely-physical setting, except when they are in the vicinity of a display, when they can additionally engage with remote users.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-127 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:100%\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-126 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:100%\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-125 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:100%\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-124 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:100%\">\n<figure class=\"wp-block-gallery has-nested-images columns-default is-cropped wp-block-gallery-1 is-layout-flex wp-block-gallery-is-layout-flex\">\n<figure class=\"wp-block-image size-full\"><a data-bi-bhvr=\"14\"  data-bi-cn=\"HyWay physical zone\" href=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay-PhysicalKiosk-64d62d1a1d31f.jpg\"><img loading=\"lazy\" decoding=\"async\" width=\"605\" height=\"327\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay-PhysicalKiosk-64d62d1a1d31f.jpg\" alt=\"HyWay physical zone\" class=\"wp-image-960399\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay-PhysicalKiosk-64d62d1a1d31f.jpg 605w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay-PhysicalKiosk-64d62d1a1d31f-300x162.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay-PhysicalKiosk-64d62d1a1d31f-240x130.jpg 240w\" sizes=\"(max-width: 605px) 100vw, 605px\" /></a><figcaption class=\"wp-element-caption\"><strong>HyWay Physical Zone</strong></figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-full\"><a data-bi-bhvr=\"14\"  data-bi-cn=\"HyWay map\" href=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay-Map-64d62d1a4c26a.jpg\"><img loading=\"lazy\" decoding=\"async\" width=\"1175\" height=\"684\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay-Map-64d62d1a4c26a.jpg\" alt=\"HyWay map\" class=\"wp-image-960402\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay-Map-64d62d1a4c26a.jpg 1175w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay-Map-64d62d1a4c26a-300x175.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay-Map-64d62d1a4c26a-1024x596.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay-Map-64d62d1a4c26a-768x447.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay-Map-64d62d1a4c26a-480x280.jpg 480w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/HyWay-Map-64d62d1a4c26a-240x140.jpg 240w\" sizes=\"(max-width: 1175px) 100vw, 1175px\" /></a><figcaption class=\"wp-element-caption\"><strong>HyWay Map</strong></figcaption></figure>\n</figure>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n\n\n\n<p>Bridging the awareness gap between the in-person and remote users is key to enabling natural, surprise-free mingling. To this end, HyWay employs 3 design elements: porosity, reciprocity, and map-based interface. (1) <strong><em>Porosity</em> </strong>ensures that, unlike in Teams or Zoom calls, users engaged in a conversation can overhear nearby conversations, and reciprocally be overheard by others in the vicinity. (2) <strong><em>Reciprocity</em> </strong>ensures that users \u2014 both in-person and remote \u2014 can see and hear others only if they are themselves seen and heard. Unlike in structured settings, where it might be fine for some or even many of the participants to have their cameras turned off or mics muted, we believe that in an unstructured setting, it is hard for users to mingle when they cannot see or hear others. (3) <em><strong>Map-based interface</strong> </em>uses a skeuomorphic representation of the physical space, with face bubbles representing both in-person and remote users (see the right image above). This allows users to be aware of others around, including those who might be within earshot or who might be approaching.</p>\n\n\n\n<p>Together, the above design elements ensure that users are not locked into airtight conversations. Instead, users (both in-person and remote) are fully aware of the presence of other nearby in-person and remote users, both through aural &nbsp;(overhearing) and visual (map) cues. This naturally provides agency. Just as in-person users in the hallway are at liberty to move about, so can remote users move from one conversation to another using the map-based interface. The fact that the map corresponds to the space where the in-person is situated (and is not merely a synthetic map as in tools such as Gather) allows for natural hybrid interactions, such as an in-person user moving from a conversation in one zone to another, and a remote user following along, or vice versa.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"user-feedback\">User Feedback</h3>\n\n\n\n<p>We have deployed HyWay at several events at Microsoft, including poster sessions, meetup/mixer events, and social gatherings. The feedback from these have been very encouraging, with users and event organizers alike having good things to say. Here is a sampling of the feedback received (bolding added):</p>\n\n\n\n<ul>\n<li>&#8220;<em>great job of </em><strong><em>bridging the physical and the virtual</em></strong>&#8220;</li>\n\n\n\n<li>&#8220;<em>Loved the </em><strong><em>informality; porosity </em></strong><em>made it feel much more real</em>&#8220;</li>\n\n\n\n<li>&#8220;<strong><em>much more like in-person meetings </em></strong><em>versus just showing up on a Teams call</em>&#8220;</li>\n\n\n\n<li>&#8220;<em>the </em><strong><em>HyWay system was great </em></strong><em>\u2026 it was the </em><strong><em>best hybrid experience </em></strong><em>they had during the pandemic</em>&#8220;</li>\n</ul>\n\n\n\n<p></p>\n\n\n\n<p>Please see the <a href=\"https://www.microsoft.com/en-us/research/publication/hyway-enabling-mingling-in-the-hybrid-world/\">paper on HyWay at ACM UbiComp 2023</a> for more details on the user studies.</p>\n\n\n\n<p></p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"notable-visitors-on-hyway\">Notable Visitors on HyWay</h3>\n\n\n\n<p>We have had the opportunity to showcase HyWay in action to some notable visitors from within and outside Microsoft, who praised the experience.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-128 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a data-bi-bhvr=\"14\"  data-bi-cn=\"Trevor Noah and others standing in a room receiving instruction on how to use HyWay\" href=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/TrevorNoah-Sep2022_1920x930.jpg\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"496\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/TrevorNoah-Sep2022_1920x930-1024x496.jpg\" alt=\"Trevor Noah and others standing in a room receiving instruction on how to use HyWay\" class=\"wp-image-963246\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/TrevorNoah-Sep2022_1920x930-1024x496.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/TrevorNoah-Sep2022_1920x930-300x145.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/TrevorNoah-Sep2022_1920x930-768x372.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/TrevorNoah-Sep2022_1920x930-1536x744.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/TrevorNoah-Sep2022_1920x930-240x116.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/TrevorNoah-Sep2022_1920x930.jpg 1920w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\"><strong>Trevor Noah (Sep 2022)</strong></figcaption></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a data-bi-bhvr=\"14\"  data-bi-cn=\"Satya Nadella using HyWay\" href=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/SatyaNadella-Jan2023_1920x930.jpg\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"496\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/SatyaNadella-Jan2023_1920x930-1024x496.jpg\" alt=\"Satya Nadella using HyWay\" class=\"wp-image-963243\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/SatyaNadella-Jan2023_1920x930-1024x496.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/SatyaNadella-Jan2023_1920x930-300x145.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/SatyaNadella-Jan2023_1920x930-768x372.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/SatyaNadella-Jan2023_1920x930-1536x744.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/SatyaNadella-Jan2023_1920x930-240x116.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/SatyaNadella-Jan2023_1920x930.jpg 1920w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\"><strong>Satya Nadella (Jan 2023)</strong></figcaption></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full\"><a data-bi-bhvr=\"14\"  data-bi-cn=\"Demo of HyWay to Bill Gates\" href=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/BillGates-Mar2023-64d63484c9a92.jpg\"><img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"930\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/BillGates-Mar2023-64d63484c9a92.jpg\" alt=\"Demo of HyWay to Bill Gates\" class=\"wp-image-960462\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/08/BillGates-Mar2023-64d63484c9a92.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/BillGates-Mar2023-64d63484c9a92-300x145.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/BillGates-Mar2023-64d63484c9a92-1024x496.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/BillGates-Mar2023-64d63484c9a92-768x372.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/BillGates-Mar2023-64d63484c9a92-1536x744.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/BillGates-Mar2023-64d63484c9a92-240x116.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" /></a><figcaption class=\"wp-element-caption\"><strong>Bill Gates (Mar 2023)</strong></figcaption></figure>\n</div>\n</div>\n\n\n\n\n\n<p>This page provides an overview of the HyWay experience through a set of video clips from a deployment at a poster event at Microsoft Research India in February 2023.&nbsp;</p>\n\n\n\n<p>A key aspect of attending such an event in person is being able to experience the setting in its fullness \u2014 hearing the buzz of background conversations and seeing who is around.&nbsp;This can be clearly seen in the following video clip of an in-person user walking through the event floor:&nbsp;</p>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"HyWay: Physical Walk (MSR India - TAB Feb 2023)\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/jVvEylK57Go?feature=oembed&rel=0\" frameborder=\"0\" allowfullscreen></iframe>\n</div></figure>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p>HyWay seeks to approximate this experience in a hybrid setting. Here\u2019s a video clip of a remote user \u201cwalking through\u201d the same event floor using the \u201cmap view\u201d of HyWay. This user can hear the buzz of the floor, with voices fading in/out as they move about. The user can also see who is around via the face bubbles on the map \u2014 red halos for in-person users and green for remote users.&nbsp;</p>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"HyWay: Virtual Walk (MSR India - TAB Feb 2023)\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/obCxrhHltG8?feature=oembed&rel=0\" frameborder=\"0\" allowfullscreen></iframe>\n</div></figure>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p>Not only can the remote user walk around, but they can also engage in conversation with in-person and remote users by \u201cstepping in\u201d to a conversation zone and entering the \u201ccall view\u201d. Such zones could either be physical (i.e., equipped with an A/V setup) or virtual.</p>\n\n\n\n<p>The following clip shows the remote user entering a physical zone, where there is already an ongoing conversation. Notice the \u201cneighbourhood view\u201d near the top-right of the call view, showing the face bubbles of nearby remote users. This provides the remote user awareness of their immediate surroundings, thereby avoiding surprises, even when the user isn\u2019t in the map view.&nbsp;</p>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"HyWay: Conversation Zone (MSR India - TAB Feb 2023)\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/i6IFaXXpJyE?feature=oembed&rel=0\" frameborder=\"0\" allowfullscreen></iframe>\n</div></figure>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p>The awareness resulting from the map view and the neighbourhood view in HyWay enables users to mingle across the physical-virtual divide in interesting ways. First up, here\u2019s an example of a remote user \u201cfollowing\u201d another remote user as they move from one physical conversation zone to another.&nbsp;</p>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"HyWay: Virtual User Tailing Virtual User (MSR India - TAB Feb 2023)\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/V4KQUT39i94?feature=oembed&rel=0\" frameborder=\"0\" allowfullscreen></iframe>\n</div></figure>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p>The map also enables a remote user to find and \u201cwalk up to\u201d an in-person user. In the following clip, the remote user (Professor Sunita Sarawagi, IIT Bombay) spotted a familiar face (Venkat Padmanabhan, who was present in person) on the map and walked over to have a conversation.&nbsp;</p>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"HyWay: Virtual User Seeking Out Physical User (MSR India - TAB Feb 2023)\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/Vrw3gEQNnEQ?feature=oembed&rel=0\" frameborder=\"0\" allowfullscreen></iframe>\n</div></figure>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<p>The correspondence between the map seen by remote users and the physical space where the in-person users are situated also enables the reverse \u2014 an in-person user spotting a remote user on the map and (physically) walking over to the physical zone that is nearest to the remote user to have a chat. (not shown)&nbsp;</p>\n\n\n\n<p>Finally, our decision in HyWay to have a map that corresponds to the physical setting, unlike the synthetic maps used in other map-based systems such as Gather and SpatialChat, has paid off in unexpected ways too. Here\u2019s the comment from a remote user (Professor Tom Dietterich, Oregon State University), who noted that having \u201cvisited\u201d remotely on HyWay during a previous event gave him immediate familiarity with the physical setting when he visited in person a year later.</p>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"HyWay: User Comment on Map Experience (MSR India - TAB Feb 2023)\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/iH1opZh7Ug8?feature=oembed&rel=0\" frameborder=\"0\" allowfullscreen></iframe>\n</div></figure>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n\n<div class=\"wp-block-msr-podcast-container\">\n\t<iframe loading=\"lazy\" src=\"https://player.blubrry.com/?podcast_id=116507230&modern=1\" class=\"podcast-player\" frameborder=\"0\" height=\"164px\" width=\"100%\" scrolling=\"no\" title=\"Podcast Player\"></iframe>\n</div>\n\n\n\n<p>The COVID pandemic forced most of us into a new paradigm of work from home and a number of tools to cater to remote work became popular. However, the post pandemic environment has seen interesting scenarios with some people preferring to continue to work from home, some people preferring to return full time to work and a number of people adopting something in between. This hybrid work environment exists today in the workplace as well as in other scenarios such as events. While tools such as Microsoft Teams do extremely well in supporting scheduled and agenda driven work meetings, there is need for a tool that supports a mix of virtual and in-person gatherings in an informal or semi-structured work environment, such as in hallways or at water coolers. In this edition of the podcast, I speak to <a href=\"https://www.microsoft.com/en-us/research/people/padmanab/\">Venkat Padmanabhan</a>, Deputy MD (Deputy Managing Director) of MSR India, and <a href=\"https://www.microsoft.com/en-us/research/people/ajayma/\">Ajay Manchepalli</a>, Principal Research Program Manager, about a project called HyWay. HyWay\u2019s a system to support unstructured and semi structured hybrid and informal interactions between groups of in-person and remote participants.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/lab/microsoft-research-india/articles/podcast-hyway-enabling-mingling-in-the-hybrid-world/\">Show notes page</a></div>\n</div>\n\n\n\n<div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n\n\n<h2 class=\"wp-block-heading\" id=\"job-title-full-stack-engineer-contingent-position\">Job Title: Full Stack Engineer (Contingent Position)</h2>\n\n\n\n<p><br>We are seeking a highly skilled Full Stack Engineer with at least a year of experience (3+ years preferred) to join our team working on HyWay, an exciting new platform for hybrid mingling developed at Microsoft Research, which has seen significant interest at Microsoft. This is a contingent position for a period of one year, with the possibility of an extension. Please visit <a href=\"https://www.microsoft.com/en-us/research/project/hyway/\">https://www.microsoft.com/en-us/research/project/hyway/</a> for more information.</p>\n\n\n\n<p>The ideal candidate would be proficient in React and Redux and would have experience working with video conferencing/webRTC applications. Familiarity with HTML5 game development is a plus.<br></p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"responsibilities\">Responsibilities:</h3>\n\n\n\n<ul>\n<li>Develop and maintain the HyWay system using React, Redux, and other modern web technologies.</li>\n\n\n\n<li>Collaborate with cross-functional teams to design, develop, and implement new features.</li>\n\n\n\n<li>Write clean, maintainable, and scalable code.</li>\n\n\n\n<li>Work with product managers and designers to ensure the best user experience on HyWay.</li>\n\n\n\n<li>Participate in code reviews and share knowledge with the team.</li>\n\n\n\n<li>Continuously improve the codebase and development process for scalability, robustness, and ease of deployment.</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"requirements\">Requirements:</h3>\n\n\n\n<ul>\n<li>Bachelor\u2019s or Master\u2019s degree in computer science or a related field, with at least a year of experience (3+ years preferred) as a Full Stack Engineer.</li>\n\n\n\n<li>Proficiency in React, Redux, and other modern web technologies.</li>\n\n\n\n<li>Experience working with video conferencing/webRTC apps.</li>\n\n\n\n<li>Strong problem-solving skills and ability to work independently.</li>\n\n\n\n<li>Good communication skills and ability to work in a team environment.</li>\n</ul>\n\n\n\n<p>This is a unique opportunity to work on an innovative research system and help scale it to serve users the world over, all while being part of the intellectually stimulating environment at Microsoft Research. If you are passionate about web development and enjoy working in a dynamic and fast-paced environment, we would love to hear from you!</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"how-to-apply\">How to apply:</h3>\n\n\n\n<p>Please send email to <a href=\"mailto:hywayjobs@microsoft.com\">hywayjobs@microsoft.com</a> with the subject line containing \u201cFull Stack Engineer (Contingent Position)\u201d, the body containing a short blurb (less than 50 words) regarding yourself and your skills & experience, and your current resume as a PDF attachment. Please do NOT email the project members directly.</p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>A platform to enable mingling between in-person (physical) and remote (virtual) users. As normalcy has returned after the COVID-19 pandemic, offices have reopened, and events have returned to a physical format. However, the convenience and cost savings of remote work have not been forgotten. So, we have entered the era of hybrid work and events, [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 955254,
        "date": "2023-07-20T03:38:31",
        "slug": "craft",
        "title": "CRAFT: Creative Robotics and Future Technologies",
        "link": "https://www.microsoft.com/en-us/research/project/craft/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"1080\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Project-Silica-Robot-Picking-1920x1080-1.jpg\" class=\"attachment-full size-full\" alt=\"Project Silica robot picking\" style=\"object-position: 50% 50%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Project-Silica-Robot-Picking-1920x1080-1.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Project-Silica-Robot-Picking-1920x1080-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Project-Silica-Robot-Picking-1920x1080-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Project-Silica-Robot-Picking-1920x1080-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Project-Silica-Robot-Picking-1920x1080-1-1536x864.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Project-Silica-Robot-Picking-1920x1080-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Project-Silica-Robot-Picking-1920x1080-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Project-Silica-Robot-Picking-1920x1080-1-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Project-Silica-Robot-Picking-1920x1080-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Project-Silica-Robot-Picking-1920x1080-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Project-Silica-Robot-Picking-1920x1080-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Project-Silica-Robot-Picking-1920x1080-1-1280x720.jpg 1280w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"craftcreative-robotics-and-future-technologies\">CRAFT</h1>\n\n\n\n<h1 class=\"wp-block-heading\" id=\"craftcreative-robotics-and-future-technologies\">Creative Robotics and Future Technologies</h1>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Microsoft&#8217;s data-centre footprint is expanding rapidly to meet the growing global demand for compute and storage. Our team is exploring the potential of automation and robotics to enhance the sustainability, efficiency, and reliability of the data center operations.</p>\n\n\n\n<p>The team is comprised of experts in mechanical design, electronics, computer systems, machine learning and robotics, who work together to turn our ideas into reality. We don&#8217;t just brainstorm solutions &#8211; we bring them to life! Thanks to our state-of-the-art prototyping workshops, we are able to quickly and effectively test and implement our ideas, making a real impact on the world. By leveraging our diverse skill sets and cutting-edge technology, we co-design the hardware and software and create innovative solutions in the area of data center robotics.</p>\n\n\n\n<p>Our latest project involved developing a library system for <a href=\"https://www.microsoft.com/en-us/research/project/project-silica/\" target=\"_blank\" rel=\"noreferrer noopener\">Project Silica</a>. This project consists of a mesh network of autonomous robots designed to quickly and reliably handle and transfer glass media between systems responsible for reading, storing, and writing data. This innovation will enable customers to store long-term information, such as digital movie sources, for thousands of years. The Silica technology has been designed and built from the media up for long-term, sustainable storage. Our challenge was to create a storage system that was reliable, cost-effective, and adaptable to changing workloads over time. The Silica library has been designed based on the following core principles:</p>\n\n\n\n<ul>\n<li><strong>Sustainability.</strong> With passive shelving, the physical infrastructure requires minimal maintenance over the lifetime of the media, as generations of robots, readers, and writers continue to evolve.</li>\n\n\n\n<li><strong>Availability.</strong> By limiting the impact of a failed robot, the design ensures high availability throughout the system, making the Silica library a reliable and resilient solution.</li>\n\n\n\n<li><strong>Flexibility.</strong> With every media accessible by multiple robots, the system can adapt quickly and easily to changing workloads, further ensuring high availability in the event of robot failures.</li>\n\n\n\n<li><strong>Scalability.</strong> The system&#8217;s aggregate throughput, storage capacity, and number of robots can all be scaled independently to meet the changing demands of the workload, making Silica a future-proof technology.</li>\n\n\n\n<li><strong>Simplicity.</strong> The robot motion is achieved using a minimal number of inexpensive components, resulting in a reliable, easy-to-service, and cost-efficient solution without sacrificing performance.</li>\n</ul>\n\n\n\n<p>Our team is now exploring the potential benefits of automation for deployment and maintenance tasks in the data center. If you would like to learn more about our ongoing projects in this area, please don&#8217;t hesitate to contact us.</p>\n\n\n\n<div style=\"height:40px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"data-centre-robotics-gallery-1\">Gallery</h2>\n\n\n\n<p>Check out some of our cool robotics:</p>\n\n\n\n<div style=\"height:24px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<div style=\"height:32px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<figure class=\"wp-block-video\"><video controls poster=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/07/robot-driving-poster-1920x1080-1.jpg\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Shuttles-Driving-LR.mp4\"></video></figure>\n\n\n\n<div style=\"height:32px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<figure class=\"wp-block-video\"><video controls poster=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/07/robot-picking-poster-1920x1080-1.jpg\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Shuttle-Picking-LR.mp4\"></video></figure>\n\n\n\n<div style=\"height:32px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<figure class=\"wp-block-video\"><video controls poster=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/07/robot-climbing-poster-1920x1080-1.jpg\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Shuttle-Climbing-LR.mp4\"></video></figure>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Microsoft&#8217;s data-centre footprint is expanding rapidly to meet the growing global demand for compute and storage. Our team is exploring the potential of automation and robotics to enhance the sustainability, efficiency, and reliability of the data center operations. The team is comprised of experts in mechanical design, electronics, computer systems, machine learning and robotics, who [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 956286,
        "date": "2023-07-19T21:08:24",
        "slug": "copilots-for-specialized-domains",
        "title": "Copilots for Specialized Domains",
        "link": "https://www.microsoft.com/en-us/research/project/copilots-for-specialized-domains/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"nextgen-communications-copilot\">NextGen Communications Copilot</h1>\n\n\n\n<p>Adapting foundation models to specialized domains like 5G to enable a conversational artificial intelligence tool for information synthesis of wireless communication specifications.</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Existing approaches to understanding, developing and researching modern wireless communication technologies involves time-intensive and arduous process of sifting through numerous webpages and technical specification documents, gathering the required information and synthesizing it. This paper presents NextGen Communications Copilot, a conversational artificial intelligence tool for information synthesis of wireless communication specifications. The system builds on top of recent advancements in foundation models and consists of three key additional components: a domain-specific database, a context extractor, and a feedback mechanism. The system appends user queries with concise and query-dependant contextual information extracted from a database of wireless technical specifications and incorporates tools for expert feedback and data contributions. On evaluation using a benchmark dataset of queries and reference responses created by subject matter experts, the system demonstrated more relevant and accurate answers with an average BLEU score and BERTScore F1-measure of 0.37 and 0.79 respectively compared to the corresponding values of 0.07 and 0.59 achieved by state-of-the-art tools like ChatGPT.</p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Adapting foundation models to specialized domains like 5G to enable a conversational artificial intelligence tool for information synthesis of wireless communication specifications. Existing approaches to understanding, developing and researching modern wireless communication technologies involves time-intensive and arduous process of sifting through numerous webpages and technical specification documents, gathering the required information and synthesizing it. This [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 953901,
        "date": "2023-07-10T12:10:56",
        "slug": "project-rumi",
        "title": "Project Rumi",
        "link": "https://www.microsoft.com/en-us/research/project/project-rumi/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/07/project-rumi-header_color_1920x720.png\" class=\"attachment-full size-full\" alt=\"Project Rumi header - multimodal diagram on gradient background\" style=\"object-position: 51% 44%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/07/project-rumi-header_color_1920x720.png 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/project-rumi-header_color_1920x720-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/project-rumi-header_color_1920x720-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/project-rumi-header_color_1920x720-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/project-rumi-header_color_1920x720-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/project-rumi-header_color_1920x720-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/project-rumi-header_color_1920x720-240x90.png 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"project-rumi\">Project Rumi</h1>\n\n\n\n<p>Multimodal paralinguistic prompting for large language models</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<h2 class=\"wp-block-heading\" id=\"multimodal-paralinguistic-prompting-for-large-language-models\">Multimodal paralinguistic prompting for large language models</h2>\n\n\n\n<p>Large language models (LLMs) are powerful neural networks that generate natural language with demonstrated value across a wide range of domains. They are revolutionizing many aspects of human society and culture including introducing a \u201cwhole new interaction model between humans and computers, turning natural language into the most powerful productivity tool on the planet.\u201d<sup>1</sup></p>\n\n\n\n<p>However, LLMs also have limitations: they may not always understand the context and nuances of a conversation. Their performance also depends on the quality and specificity of the user\u2019s input, or prompt. The data that the user inputs into the LLM is a lexical entry, which does not comprehensively represent the nuances of human-to-human interaction; it is in fact missing all the <strong>paralinguistic</strong> information (intonation, gestures, facial expressions, and everything besides the actual words) that contribute to the meaning and intentions of the speaker. This can lead to misinterpretation, misunderstanding, or inappropriate responses from the LLM. Project Rumi incorporates paralinguistic input into prompt-based interactions with LLMs with the objective of improving the quality of communication. Providing this context is critical to enhancing LLMs capabilities in this \u201cAI as a copilot\u201d era.</p>\n\n\n\n<p>Our current system leverages separately trained vision and audio-based models to detect and analyze non-verbal cues extracted from data streams. The models assess sentiment from cognitive and physiological data in real time, generating appropriate paralinguistic tokens to augment standard lexical prompt input to existing LLMs such as GPT4. &nbsp;This multimodal, muti-step architecture integrates seamlessly with all pretrained text-based LLMs to provide additional information on the user&#8217;s sentiment and intention that is not captured by text-based models, augmenting the prompt with the richness and subtlety of human communication to bring human-AI interaction to a new level.</p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full\"><img loading=\"lazy\" decoding=\"async\" width=\"1400\" height=\"1033\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Rumi-system-diagram_UPDATED_July31_1400.png\" alt=\"Diagram illustrating user interaction with application. Inputs user\u2019s sensor data such as video and audio and uses processed sensor data to generate paralinguistic classification. This classification augments the input to the LLM, generating an augmented output to application.\" class=\"wp-image-958200\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Rumi-system-diagram_UPDATED_July31_1400.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Rumi-system-diagram_UPDATED_July31_1400-300x221.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Rumi-system-diagram_UPDATED_July31_1400-1024x756.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Rumi-system-diagram_UPDATED_July31_1400-768x567.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Rumi-system-diagram_UPDATED_July31_1400-80x60.png 80w, https://www.microsoft.com/en-us/research/uploads/prod/2023/07/Rumi-system-diagram_UPDATED_July31_1400-240x177.png 240w\" sizes=\"(max-width: 1400px) 100vw, 1400px\" /><figcaption class=\"wp-element-caption\">Diagram illustrating user interaction with application. Inputs user\u2019s sensor data such as video and audio and uses processed sensor data to generate paralinguistic classification. This classification augments the input to the LLM, generating an augmented output to application.</figcaption></figure>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<p>Future explorations include improving performance of existing models and incorporating additional signals like HRV (heart rate variability) derived from standard video, and cognitive and ambient sensing. Conveying unspoken meaning and intention is an essential component in the next generation of AI interaction.</p>\n\n\n\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"Multimodal Paralinguistic Prompting for Large Language Models (Project Rumi) Demo\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/hKnOsfKrKbE?feature=oembed&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</div></figure>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/worklab/ai-a-whole-new-way-of-working\" target=\"_blank\" rel=\"noreferrer noopener\">Explore AI: A whole new way of working</a></div>\n</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Project Rumi incorporates paralinguistic input into prompt-based interactions with LLMs with the objective of improving the quality of communication. Providing this context is critical to enhancing LLMs capabilities in this \u201cAI as a copilot\u201d era.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 951570,
        "date": "2023-06-21T23:06:15",
        "slug": "foundation-of-agi",
        "title": "Foundation of AGI",
        "link": "https://www.microsoft.com/en-us/research/project/foundation-of-agi/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"foundation-of-agi\">Foundation of AGI</h1>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>We focus on the fundamental research of Foundation of AGI as part of our mission-focused research on advancing AGI for humanity. Our research involves pushing #TheBigConvergence of foundation models across tasks, languages, and modalities, developing new neural architectures for A(G)I and new learning paradigms for autonomous agents, as well as understanding the principles and boundary of (artificial general) intelligence. More information at <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://aka.ms/GeneralAI\" target=\"_blank\" rel=\"noreferrer noopener\">https://aka.ms/GeneralAI<span class=\"sr-only\"> (opens in new tab)</span></a>.</p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>We focus on the fundamental research of Foundation of AGI as part of our mission-focused research on advancing AGI for humanity. Our research involves pushing #TheBigConvergence of foundation models across tasks, languages, and modalities, developing new neural architectures for A(G)I and new learning paradigms for autonomous agents, as well as understanding the principles and boundary [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 950052,
        "date": "2023-06-16T19:33:56",
        "slug": "project-vellm",
        "title": "Project VeLLM",
        "link": "https://www.microsoft.com/en-us/research/project/project-vellm/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"project-vellm\">Project VeLLM</h1>\n\n\n\n<p>uniVersal Empowerment with LLMs</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>The technology landscape is being rapidly transformed by Large Language Models (LLMs), allowing users to address real-world applications in various domains. However, a digital divide exists that may exclude large populations from benefiting and contributing to this technological revolution due to factors such as language, income, digital awareness, and access to information. To address this issue, Project VeLLM (UniVersal&nbsp;Empowerment with Large Language Models) is focused on developing a principled approach to enable inclusive applications of LLMs for all languages and cultures worldwide. This interdisciplinary research project is being conducted at Microsoft Research India in collaboration with partners across Microsoft. In Project VeLLM, we are working on the following fundamental research problems that are currently barriers for making LLMs inclusive to everyone:</p>\n\n\n\n<ol>\n<li>Multilingual Language Models</li>\n\n\n\n<li>Responsible AI and safety across languages and cultures</li>\n\n\n\n<li>Multi-modal models</li>\n\n\n\n<li>Knowledge representation and grounding</li>\n\n\n\n<li>Cost and optimization</li>\n</ol>\n\n\n\n<p><strong>Multilingual Language Models:</strong> Our work focuses on the evaluation and improvement of LLMs on non-English languages. Towards this, we carried out a<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/pdf/2303.12528.pdf\"> comprehensive evaluation of GPT models<span class=\"sr-only\"> (opens in new tab)</span></a> (EMNLP 2023) and other LLMs on the MEGA benchmark that comprises of 16 datasets covering over 70 languages. Our current focus in this direction is on scaling up multilingual evaluation, including the use of <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2309.07462\">LLM-based evaluators in the multilingual setting<span class=\"sr-only\"> (opens in new tab)</span></a> with humans in the loop. </p>\n\n\n\n<p><strong>Responsible AI and safety across languages and cultures</strong>: Our focus in this direction is on defining and reducing bias in LLMs in non-English languages and cultures. Our survey (EACL 2023) describes the <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://aclanthology.org/2023.findings-eacl.157/\">challenges in scaling fairness in languages beyond English<span class=\"sr-only\"> (opens in new tab)</span></a> and our current work includes parameter efficient techniques to reduce bias in models across various dimensions of bias.</p>\n\n\n\n<p></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>uniVersal Empowerment with LLMs The technology landscape is being rapidly transformed by Large Language Models (LLMs), allowing users to address real-world applications in various domains. However, a digital divide exists that may exclude large populations from benefiting and contributing to this technological revolution due to factors such as language, income, digital awareness, and access to [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 947154,
        "date": "2023-06-09T01:13:41",
        "slug": "vall-e-x",
        "title": "VALL-E",
        "link": "https://www.microsoft.com/en-us/research/project/vall-e-x/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-catalina-blue card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"2000\" height=\"601\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/06/brain-neuroscience-workshop-2023-kv.jpg\" class=\"attachment-full size-full\" alt=\"background pattern\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/06/brain-neuroscience-workshop-2023-kv.jpg 2000w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/brain-neuroscience-workshop-2023-kv-300x90.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/brain-neuroscience-workshop-2023-kv-1024x308.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/brain-neuroscience-workshop-2023-kv-768x231.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/brain-neuroscience-workshop-2023-kv-1536x462.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/brain-neuroscience-workshop-2023-kv-240x72.jpg 240w\" sizes=\"(max-width: 2000px) 100vw, 2000px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"vall-e\">VALL-E</h1>\n\n\n\n<p>A neural codec language model for speech synthesis</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to&nbsp;synthesize <strong>high-quality</strong> personalized speech with<strong> </strong>only a<strong> 3-second </strong>enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of <strong>speech naturalness and speaker similarity</strong>. In addition, VALL-E could preserve the speaker&#8217;s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating <strong>cross-lingual </strong>zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the<strong> robustness </strong>of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: <strong>human parity </strong>in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field.</p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"model-versions\">Model versions</h2>\n\n\n\n<table style=\"margin-left: auto;margin-right: auto\">\n<tr>\n<td style=\"width: 52%\">\n<figure class=\"wp-block-image size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" class=\"wp-image-943299\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Overview-1024x552.jpg\" alt=\"VALL-E model overview diagram\" width=\"640\" height=\"345\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Overview-1024x552.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Overview-300x162.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Overview-768x414.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Overview-240x129.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Overview.jpg 1047w\" sizes=\"(max-width: 640px) 100vw, 640px\" /></figure>\n</td>\n<td>\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" class=\"wp-image-944352\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vallex_framework.jpg\" alt=\"VALL-E X model overview diagram\" width=\"640\" height=\"345\" /></figure>\n</td>\n</tr>\n<tr>\n<td style=\"width: 52%\">\n<div class=\"wp-block-button\"><a class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/project/vall-e-x/vall-e/\">See VALL-E samples</a></div>\n</td>\n<td>\n<div class=\"wp-block-button\"><a class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/project/vall-e-x/vall-e-x/\">See VALL-E X samples</a></div>\n</td>\n</tr>\n</table>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-129 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\"></div>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\"></div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\"></div>\n</div>\n</div>\n\n\n\n<table style=\"margin-left: auto;margin-right: auto\">\n<tr>\n<td style=\"width: 52%\">\n<figure class=\"wp-block-image size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" class=\"wp-image-943299\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/valler-768x338.jpg\" alt=\"VALL-E model overview diagram\" width=\"640\" height=\"345\"></figure>\n</td>\n<td>\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" class=\"wp-image-944352\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/VALLE2.jpg\" alt=\"VALL-E X model overview diagram\" width=\"640\" height=\"345\" /></figure>\n</td>\n</tr>\n<tr>\n<td style=\"width: 52%\">\n<div class=\"wp-block-button\"><a class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/project/vall-e-x/vall-e-r/\">See VALL-E R samples</a></div>\n</td>\n<td>\n<div class=\"wp-block-button\"><a class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/project/vall-e-x/vall-e-2/\">See VALL-E 2 samples</a></div>\n</td>\n</tr>\n</table>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\"></div>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"ethics-statement\">Ethics statement</h2>\n\n\n\n<p>VALL-E  could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E  can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E  is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal.</p>\n\n\n\n\n\n<p>VALL-E is a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. VALL-E emerges in-context learning capabilities and can be used to <strong>synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker</strong> as an acoustic prompt. Experiment results show that VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find VALL-E could preserve the speaker\u2019s emotion and acoustic environment of the acoustic prompt in synthesis.</p>\n\n\n\n<p>This page is for&nbsp;<strong>research demonstration purposes</strong>&nbsp;only.</p>\n\n\n\n<div class=\"wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  has-media-on-the-right is-stacked-on-mobile is-style-border\" data-bi-an=\"media-text\"><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<h2 class=\"wp-block-heading\" id=\"model-overview-2\">Model Overview</h2>\n\n\n\n<p>Unlike the previous pipeline (e.g., phoneme \u2192 mel-spectrogram \u2192 waveform), the pipeline of VALL-E is phoneme \u2192 discrete code \u2192 waveform. VALL-E generates the discrete audio codec codes based on phoneme and acoustic code prompts, corresponding to the target content and the speaker\u2019s voice. VALL-E directly enables various speech synthesis applications, such as zero-shot TTS, speech editing, and content creation combined with other generative AI models like GPT.</p>\n</div><figure class=\"wp-block-media-text__media\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"552\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Overview-1024x552.jpg\" alt=\"VALL-E model overview diagram\" class=\"wp-image-943299 size-full\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Overview-1024x552.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Overview-300x162.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Overview-768x414.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Overview-240x129.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Overview.jpg 1047w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure></div>\n\n\n\n<h2 class=\"wp-block-heading\">Zero-shot TTS for LibriSpeech and VCTK dataset\u00a0</h2>\n\n\n\n\n\n<table class=\"table table-hover pt-2\" style=\"height: 0px;border-spacing: inherit;border-collapse: collapse\">\n<thead>\n<tr style=\"height: 69px\">\n<th style=\"text-align: center;height: 69px\">Text</th>\n<th style=\"text-align: center;height: 69px\">Speaker Prompt</th>\n<th style=\"text-align: center;height: 69px\">Ground Truth</th>\n<th style=\"text-align: center;height: 69px\">Baseline</th>\n<th style=\"text-align: center;height: 69px\">VALL-E</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 600px;height: 0px\">They moved thereafter cautiously about the hut groping before and about them to find something to show that Warrenton had fulfilled his mission.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_61-70970-0024_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_61-70970-0024_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_61-70970-0024_yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_61-70970-0024_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">And lay me down in thy cold bed and leave my shining lot.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_908-157963-0027_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_908-157963-0027_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_908-157963-0027_yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_908-157963-0027_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">Number ten, fresh nelly is waiting on you, good night husband.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1089-134686-0004_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1089-134686-0004_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1089-134686-0004_yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1089-134686-0004_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">Yea, his honourable worship is within, but he hath a godly minister or two with him, and likewise a leech.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1221-135767-0014_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1221-135767-0014_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1221-135767-0014_yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1221-135767-0014_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">Instead of shoes, the old man wore boots with turnover tops, and his blue coat had wide cuffs of gold braid.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1284-1180-0002_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1284-1180-0002_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1284-1180-0002_yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1284-1180-0002_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">The army found the people in poverty and left them in comparative wealth.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_4077-13754-0000_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_4077-13754-0000_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_4077-13754-0000_yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_4077-13754-0000_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">Thus did this humane and right minded father comfort his unhappy daughter, and her mother embracing her again, did all she could to soothe her feelings.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_5639-40744-0020_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_5639-40744-0020_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_5639-40744-0020_yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_5639-40744-0020_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">He was in deep converse with the clerk and entered the hall holding him by the arm.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_61-70970-0007_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_61-70970-0007_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_61-70970-0007_yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_61-70970-0007_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n\n\n<p></p>\n\n\n\n<table class=\"table table-hover pt-2\" style=\"height: 0px;border-spacing: inherit;border-collapse: collapse\">\n<thead>\n<tr style=\"height: 69px\">\n<th style=\"text-align: center;height: 69px\">Text</th>\n<th style=\"text-align: center;height: 69px\">Speaker Prompt</th>\n<th style=\"text-align: center;height: 69px\">Ground Truth</th>\n<th style=\"text-align: center;height: 69px\">Baseline</th>\n<th style=\"text-align: center;height: 69px\">VALL-E</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 600px;height: 0px\">We have to reduce the number of plastic bags.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p254_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p254_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p254_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p254_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">So what is the campaign about?</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p263_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p263_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p263_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p263_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">My life has changed a lot.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p273_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p273_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p273_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p273_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">Nothing is yet confirmed.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p255_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p255_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p255_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p255_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">I could hardly move for the next couple of days.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p262_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p262_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p262_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p262_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">His son has been travelling with the Tartan Army for years.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p256_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p256_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p256_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p256_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">Her husband was very concerned that it might be fatal.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p248_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p248_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p248_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p248_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">We\u2019ve made a couple of albums.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p233_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p233_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p233_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_p233_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<p></p>\n\n\n\n<hr class=\"wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots\" />\n\n\n\n<h2 class=\"wp-block-heading\">Synthesis of diversity</h2>\n\n\n\n\n\n<p></p>\n\n\n\n<table class=\"table table-hover pt-2\" style=\"height: 0px;border-spacing: inherit;border-collapse: collapse\">\n<thead>\n<tr style=\"height: 69px\">\n<th style=\"text-align: center;height: 69px\">Text</th>\n<th style=\"text-align: center;height: 69px\">Speaker Prompt</th>\n<th style=\"text-align: center;height: 69px\">VALL-E Sample1</th>\n<th style=\"text-align: center;height: 69px\">VALL-E Sample2</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">Because we do not need it.</td>\n<td style=\"text-align: center;height: 0px;width: 342px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_diversity_p226_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px;width: 342px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_diversity_p226_s1.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px;width: 344px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_diversity_p226_s2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">I must do something about it.</td>\n<td style=\"text-align: center;height: 0px;width: 342px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_diversity_p230_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px;width: 342px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_diversity_p230_s1.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px;width: 344px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_diversity_p230_s2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">He has not been named.</td>\n<td style=\"text-align: center;height: 0px;width: 342px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_diversity_p246_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px;width: 342px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_diversity_p246_s1.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px;width: 344px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vctk_diversity_p246_s2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">Number ten, fresh nelly is waiting on you, good night husband.</td>\n<td style=\"text-align: center;height: 0px;width: 342px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_diversity_sample2_prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px;width: 342px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_diversity_sample2_0.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px;width: 344px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_diversity_sample2_1.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<p></p>\n\n\n\n<hr class=\"wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots\" />\n\n\n\n<h2 class=\"wp-block-heading\">Acoustic environment maintenance</h2>\n\n\n\n\n\n<p></p>\n\n\n\n<table class=\"table table-hover pt-2\" style=\"height: 10px;border-spacing: inherit;border-collapse: collapse\">\n<thead>\n<tr style=\"height: 0px\">\n<th style=\"text-align: center;height: 0px\">Text</th>\n<th style=\"text-align: center;height: 0px\">Speaker Prompt</th>\n<th style=\"text-align: center;height: 0px\">Ground Truth</th>\n<th style=\"text-align: center;height: 0px\">VALL-E</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"height: 10px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 10px\">I think it\u2019s like you know um more convenient too.</td>\n<td style=\"text-align: center;height: 10px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/fisher_1_pt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 10px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/fisher_1_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 10px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/fisher_1_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">Um we have to pay have this security fee just in case she would damage something but um.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/fisher_2_pt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/fisher_2_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/fisher_2_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">Everything is run by computer but you got to know how to think before you can do a computer.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/fisher_3_pt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/fisher_3_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/fisher_3_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">As friends thing I definitely I\u2019ve got more male friends.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/fisher_4_pt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/fisher_4_gt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/fisher_4_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<p></p>\n\n\n\n<hr class=\"wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots\" />\n\n\n\n<h2 class=\"wp-block-heading\">Speaker emotion maintenance</h2>\n\n\n\n\n\n<p></p>\n\n\n\n<table class=\"table table-hover pt-2\" style=\"height: 30px;border-spacing: inherit;border-collapse: collapse\">\n<thead>\n<tr style=\"height: 10px\">\n<th style=\"text-align: center;height: 10px\">Text</th>\n<th style=\"text-align: center;height: 10px\">Emotion</th>\n<th style=\"text-align: center;height: 10px\">Speaker Prompt</th>\n<th style=\"text-align: center;height: 10px\">VALL-E</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 20px\" rowspan=\"5\">We have to reduce the number of plastic bags.</td>\n<td style=\"text-align: center;vertical-align: middle;width: 220px;height: 0px\">Anger</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emov_db_anger_pt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emov_db_anger_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 10px\">\n<td style=\"text-align: center;vertical-align: middle;width: 220px;height: 10px\">Sleepy</td>\n<td style=\"text-align: center;height: 10px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emov_db_sleepiness_pt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 10px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emov_db_sleepiness_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: center;vertical-align: middle;width: 220px;height: 0px\">Neutral</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emov_db_neutral_pt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emov_db_neutral_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 10px\">\n<td style=\"text-align: center;vertical-align: middle;width: 220px;height: 10px\">Amused</td>\n<td style=\"text-align: center;height: 10px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emov_db_amused_pt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 10px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emov_db_amused_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: center;vertical-align: middle;width: 220px;height: 0px\">Disgusted</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emov_db_disgust_pt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emov_db_disgust_ours.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<hr class=\"wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots\" />\n\n\n\n<h2 class=\"wp-block-heading\">More samples</h2>\n\n\n\n\n\n<p></p>\n\n\n\n<table class=\"table table-hover pt-2\" style=\"height: 30px;border-spacing: inherit;border-collapse: collapse\">\n<thead>\n<tr style=\"height: 0px\">\n<th style=\"text-align: center;height: 10px\">Text</th>\n<th style=\"text-align: center;height: 10px\">Speaker Prompt</th>\n<th style=\"text-align: center;height: 10px\">VALL-E</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">The others resented postponement, but it was just his scruples that charmed me.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_121-127105-0006_prompt1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_121-127105-0006_prompt2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_121-127105-0006_prompt3.wav\"></audio></audio></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_121-127105-0006_ours1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_121-127105-0006_ours2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_121-127105-0006_ours3.wav\"></audio></audio></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 10px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 10px\">Notwithstanding the high resolution of hawkeye, he fully comprehended all the difficulties and danger he was about to incur.</td>\n<td style=\"text-align: center;height: 10px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1320-122617-0000_prompt1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1320-122617-0000_prompt2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1320-122617-0000_prompt3.wav\"></audio></audio></audio></figure>\n</td>\n<td style=\"text-align: center;height: 10px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1320-122617-0000_ours1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1320-122617-0000_ours2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_1320-122617-0000_ours3.wav\"></audio></audio></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 10px\">We were more interested in the technical condition of the station than in the commercial part.</td>\n<td style=\"text-align: center;height: 10px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2300-131720-0040_prompt1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2300-131720-0040_prompt2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2300-131720-0040_prompt3.wav\"></audio></audio></audio></figure>\n</td>\n<td style=\"text-align: center;height: 10px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2300-131720-0040_ours1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2300-131720-0040_ours2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2300-131720-0040_ours3.wav\"></audio></audio></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">Paul takes pride in his ministry not to his own praise but to the praise of god.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2830-3980-0008_prompt1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2830-3980-0008_prompt2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2830-3980-0008_prompt3.wav\"></audio></audio></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2830-3980-0008_ours1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2830-3980-0008_ours2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2830-3980-0008_ours3.wav\"></audio></audio></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">The ideas also remain but they have become types in nature forms of men animals birds fishes.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2961-960-0014_prompt1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2961-960-0014_prompt2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2961-960-0014_prompt3.wav\"></audio></audio></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2961-960-0014_ours1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2961-960-0014_ours2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_2961-960-0014_ours3.wav\"></audio></audio></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">Other circumstances permitting that instinct disposes men to look with favor upon productive efficiency and on whatever is of human use.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_3570-5696-0002_prompt1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_3570-5696-0002_prompt2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_3570-5696-0002_prompt3.wav\"></audio></audio></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_3570-5696-0002_ours1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_3570-5696-0002_ours2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_3570-5696-0002_ours3.wav\"></audio></audio></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">But suppose you said I\u2019m fond of writing, my people always say my letters home are good enough for punch.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_7176-92135-0005_prompt1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_7176-92135-0005_prompt2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_7176-92135-0005_prompt3.wav\"></audio></audio></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_7176-92135-0005_ours1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_7176-92135-0005_ours2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_7176-92135-0005_ours3.wav\"></audio></audio></audio></figure>\n</td>\n</tr>\n<tr style=\"height: 0px\">\n<td style=\"text-align: left;vertical-align: middle;width: 500px;height: 0px\">He summoned half a dozen citizens to join his posse who followed obeyed and assisted him.</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_7729-102255-0030_prompt1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_7729-102255-0030_prompt2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_7729-102255-0030_prompt3.wav\"></audio></audio></audio></figure>\n</td>\n<td style=\"text-align: center;height: 0px\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_7729-102255-0030_ours1.wav\" controls=\"controls\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_7729-102255-0030_ours2.wav\"><audio controls=\"controls\" style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/librispeech_7729-102255-0030_ours3.wav\"></audio></audio></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<hr class=\"wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots\" />\n\n\n\n\n\n<p>We extend VALL-E to a  cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis, and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can <strong>generate high-quality speech in the target language via just one speech utterance in the source language as a prompt</strong> while preserving the unseen speaker&#8217;s voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates foreign accent problems, which can be controlled by a language ID.</p>\n\n\n\n<p>This page is for&nbsp;<strong>research demonstration purposes</strong>&nbsp;only.</p>\n\n\n\n<div class=\"wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  has-media-on-the-right is-stacked-on-mobile is-style-border\" data-bi-an=\"media-text\"><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<h2 class=\"wp-block-heading\" id=\"model-overview-2\">Model Overview</h2>\n\n\n\n<p>VALL-E X can synthesize personalized speech in another language for a monolingual speaker. Taking the phoneme sequences derived from the source and target text, and the source acoustic tokens derived from an audio codec model as prompts, VALL-E X is able to produce the acoustic tokens in the target language, which can be then decompressed to the target speech waveform. Thanks to its powerful in-context learning capabilities, VALL-E X does not require cross-lingual speech data of the same speakers for training and can perform various zero-shot cross-lingual speech generation tasks, such as cross-lingual text-to-speech synthesis and speech-to-speech translation.</p>\n</div><figure class=\"wp-block-media-text__media\"><img loading=\"lazy\" decoding=\"async\" width=\"768\" height=\"384\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vallex_framework.jpg\" alt=\"VALL-E X model overview diagram\" class=\"wp-image-944352 size-full\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vallex_framework.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vallex_framework-300x150.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/vallex_framework-240x120.jpg 240w\" sizes=\"(max-width: 768px) 100vw, 768px\" /></figure></div>\n\n\n\n<div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"xtts-samples\">Zero-shot cross-lingual text to speech</h2>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">English Text</th>\n<th style=\"text-align: center\">Chinese Speaker Prompt</th>\n<th style=\"text-align: center\">Baseline</th>\n<th style=\"text-align: center\">VALL-E X</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">Look a little closer while our guide lets the light of his lamp fall upon the black wall at your side.</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_aishell2libri_518_ch_prompts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_aishell2libri_518_en_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_aishell2libri_518_en_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">He honours whatever he recognizes in himself, such morality equals self-glorification.</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_emime2libri_729_ch_prompts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_emime2libri_729_en_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_emime2libri_729_en_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">One dark night at the head of a score of his tribe, he fell upon Wabigoon&#8217;s camp, his object being the abduction of the princess.</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_emime2libri_905_ch_prompts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_emime2libri_905_en_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_emime2libri_905_en_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">There could be little art in this last and final round of fencing.</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_aishell2libri_34_ch_prompts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_aishell2libri_34_en_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_aishell2libri_34_en_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">It&#8217;s the first time Hilda has been to our house and Tom introduces her around.</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_aishell2libri_205_ch_prompts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_aishell2libri_205_en_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_aishell2libri_205_en_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">It was youth and poverty and proximity and everything was young and kindly.</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_emime2libri_60_ch_prompts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_emime2libri_60_en_baseline.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_emime2libri_60_en_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">Chinese Text</th>\n<th style=\"text-align: center\">English Speaker Prompt</th>\n<th style=\"text-align: center\">VALL-E X</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 500px\">\u575a\u6301\u623f\u5730\u4ea7\u8c03\u63a7\u653f\u7b56\u4e0d\u52a8\u6447\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_ref_output_238_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_output_238_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 500px\">\u503c\u5f97\u5173\u6ce8\u7684\u662f\u4ece\u4e8c\u96f6\u4e00\u96f6\u5e74\u5230\u4e8c\u96f6\u4e00\u56db\u5e74\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_ref_output_576_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_output_576_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 500px\">\u4e24\u5343\u516d\u767e\u56db\u5341\u516b\u4e07\u4e8c\u5343\u4e94\u767e\u56db\u5341\u516d\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_ref_output_590_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_output_590_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 500px\">\u6c47\u805a\u90e8\u5206\u5168\u7403\u9886\u5148\u54c1\u724c\u7684\u4e0b\u4e00\u4ee3\u6280\u672f\u521b\u65b0\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_ref_output_591_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_output_591_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 500px\">\u5546\u54c1\u623f\u9500\u552e\u60c5\u51b5\u4e5f\u4f20\u9012\u51fa\u4e86\u66f4\u591a\u6696\u610f\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_ref_output_610_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_output_610_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 500px\">\u6700\u4f4e\u9996\u4ed8\u6b3e\u6bd4\u4f8b\u4e3a\u767e\u5206\u4e4b\u4e00\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_ref_output_1243_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/xtts_output_1243_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<hr class=\"wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots\" />\n\n\n\n<h2 class=\"wp-block-heading\" id=\"s2st-samples\">Zero-shot speech-to-speech translation</h2>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">Chinese Speech</th>\n<th style=\"text-align: center\">English Ground Truth</th>\n<th style=\"text-align: center\">Baseline</th>\n<th style=\"text-align: center\">VALL-E X Trans</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_source_zh_50.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_target_en_50.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_baseline_baseline_50.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_ours_vallex_50.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_source_zh_135.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_target_en_135.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_baseline_baseline_135.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_ours_vallex_135.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_source_zh_234.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_target_en_234.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_baseline_baseline_234.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_ours_vallex_234.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_source_zh_150.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_target_en_150.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_baseline_baseline_150.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_ours_vallex_150.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_source_zh_168.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_target_en_168.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_baseline_baseline_168.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_ours_vallex_168.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_source_zh_9.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_target_en_9.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_baseline_baseline_9.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_zh-en-translation_ours_vallex_9.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">English Speech</th>\n<th style=\"text-align: center\">Chinese Ground Truth</th>\n<th style=\"text-align: center\">VALL-E X Trans</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_source_en_204.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_target_zh_204.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_ours_vallex_204.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_source_en_98.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_target_zh_98.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_ours_vallex_98.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_source_en_137.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_target_zh_137.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_ours_vallex_137.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_source_en_209.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_target_zh_209.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_ours_vallex_209.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_source_en_148.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_target_zh_148.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_ours_vallex_148.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_source_en_168.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_target_zh_168.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_en-zh-translation_ours_vallex_168.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">Chinese Text</th>\n<th style=\"text-align: center\">Chinese Speech</th>\n<th style=\"text-align: center\">Baseline</th>\n<th style=\"text-align: center\">VALL-E X Trans</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 500px\">\u6211\u4eec\u5c31\u5750\u5728\u4ed6\u7684\u4f4d\u4e8e\u534a\u5c71\u5761\u7684\u529e\u516c\u5ba4\u91cc\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_source_output_103_reference.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_baseline_SSB07170458.104.yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_ours_aishell_zh2en_103_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 500px\">\u662f\u5df4\u897f\u5973\u9009\u624b\u5728\u70ed\u8eab\u6cf3\u9053\u4e2d\u8fdd\u89c4\u4f7f\u7528\u811a\u6251\u5f71\u54cd\u4ed6\u4eba\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_source_output_140_reference.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_baseline_SSB17450167.141.yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_ours_aishell_zh2en_140_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 500px\">\u66f4\u5728\u4e8e\u8fd9\u9879\u8fd0\u52a8\u672c\u8eab\u5177\u6709\u7740\u6781\u5176\u4e30\u5bcc\u7684\u7cbe\u795e\u5185\u6db5\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_source_output_151_reference.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_baseline_SSB08870390.152.yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_ours_aishell_zh2en_151_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 500px\">\u7f8e\u56fd\u5e76\u6ca1\u6709\u7edf\u4e00\u7684\u5168\u56fd\u9ad8\u4e2d\u8054\u8d5b\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_source_output_3_reference.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_baseline_SSB06930182.4.yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_ours_aishell_zh2en_3_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 500px\">\u672c\u5e02\u8fd8\u8981\u6253\u9020\u6148\u5584\u6350\u8d60\u4e8b\u4e1a\u7684\u9633\u5149\u5de5\u7a0b\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_source_output_12_reference.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_baseline_SSB11870416.13.yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_ours_aishell_zh2en_12_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 500px\">\u8fbe\u831c\u516e\u4f1a\u79f0\u547c\u6211\u4e3a\u74e6\u9f50\u91cc\u5148\u751f\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_source_output_59_reference.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_baseline_SSB07170279.60.yourtts.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-aishell3-zhen_ours_aishell_zh2en_59_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">English Text</th>\n<th style=\"text-align: center\">English Speech</th>\n<th style=\"text-align: center\">VALL-E X Trans</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">His instant of panic was followed by a small sharp blow high on his chest.</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-libridev-enzh_output_22_reference.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-libridev-enzh_output_22_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">The last two days of the voyage Bartley found almost intolerable.</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-libridev-enzh_output_61_reference.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-libridev-enzh_output_61_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">She merely brushed his cheek with her lips and put a hand lightly and joyously on either shoulder.</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-libridev-enzh_output_65_reference.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-libridev-enzh_output_65_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">But in this awful moment of the danger of the church. their vow was superseded by a more sublime and indispensable duty.</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-libridev-enzh_output_105_reference.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-libridev-enzh_output_105_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">We&#8217;ve lost the key of the cellar and there&#8217;s nothing out except water and i don&#8217;t think you&#8217;d care for that.</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-libridev-enzh_output_425_reference.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-libridev-enzh_output_425_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">He had been late he had offered no excuse no explanation.</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-libridev-enzh_output_1058_reference.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/s2st_s2st-libridev-enzh_output_1058_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<hr class=\"wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots\" />\n\n\n\n<h2 class=\"wp-block-heading\" id=\"lid-samples\">Foreign accent control</h2>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">English Speech (Prompt)</th>\n<th style=\"text-align: center\">Chinese Speech (Ground Truth)</th>\n<th style=\"text-align: center\">VALL-E X with English LID</th>\n<th style=\"text-align: center\">VALL-E X with Chinese LID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_emime_en_43_truth.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_emime_zh_43_truth.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_emime_en2zh_43_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_emime_en2zh_43_ar01lid1.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_MF1_ENG_0002_1.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_MF1_MAN_0002_1.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_output_with_EN_LID_MF1_ENG_0002_1.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_output_with_ZH_LID_MF1_ENG_0002_1.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_emime_en_252_truth.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_emime_zh_252_truth.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_emime_en2zh_252_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_emime_en2zh_252_ar01lid1.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_emime_en_266_truth.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_emime_zh_266_truth.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_emime_en2zh_266_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_enzh_emime_en2zh_266_ar01lid1.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">Chinese Speech (Prompt)</th>\n<th style=\"text-align: center\">English Speech (Ground Truth)</th>\n<th style=\"text-align: center\">VALL-E X with English LID</th>\n<th style=\"text-align: center\">VALL-E X with Chinese LID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_236_truth_zh.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_236_truth_en.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_236_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_236_ar01lid1.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_155_truth_zh.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_155_truth_en.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_155_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_155_ar01lid1.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_123_truth_zh.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_123_truth_en.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_123_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_123_ar01lid1.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_262_truth_zh.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_262_truth_en.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_262_ar01lid2.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/LID_zhen_emime_zh2en_262_ar01lid1.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<hr class=\"wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots\" />\n\n\n\n<h2 class=\"wp-block-heading\" id=\"emotion-samples\">Voice emotion maintenance</h2>\n\n\n\n\n\n<p>VALL-E X Trans can synthesize personalized target speech while maintaining the emotion in the source speech. The source audio are sampled from the Emotional Voices Database EmoV-DB.</p>\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">Emotion</th>\n<th style=\"text-align: center\">English Speech</th>\n<th style=\"text-align: center\">VALL-E X Trans</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;vertical-align: middle;width: 500px\">Neutral</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emotion_output_4162_neutral_85-112_0097.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emotion_output_4162_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;vertical-align: middle;width: 500px\">Amused</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emotion_output_3872_amused_85-112_0110.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emotion_output_3872_28_2_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;vertical-align: middle;width: 500px\">Sleepiness</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emotion_output_1455_sleepiness_253-280_0253.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emotion_output_1455_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;vertical-align: middle;width: 500px\">Anger</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emotion_output_5249_anger_367-392_0368.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emotion_output_5249_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;vertical-align: middle;width: 500px\">Disgust</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emotion_output_5712_disgust_336-364_0364.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/emotion_output_5712_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<hr class=\"wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots\" />\n\n\n\n<h2 class=\"wp-block-heading\" id=\"codeswitch-samples\">Code-switch speech synthesis</h2>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">Code-Switch Text</th>\n<th style=\"text-align: center\">Prompts</th>\n<th style=\"text-align: center\">VALL-E X</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;vertical-align: middle;width: 500px\">\u64ad\u653e\u6b4c\u66f2 BEST FRIEND\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/codeswitch_12_MF3_MAN_0010_0.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/codeswitch_output_12_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;vertical-align: middle;width: 500px\">\u6211\u60f3\u53bbtravel\u4e00\u4e0b\uff0c\u653e\u677e\u4e00\u4e0b\u81ea\u5df1\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/codeswitch_25_MM5_MAN_0003_0.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/codeswitch_output_25_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;vertical-align: middle;width: 500px\">\u4ed6\u662f\u4e00\u4e2afunny\u7684\u4eba\uff0c\u603b\u662f\u8bb2\u7b11\u8bdd\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/codeswitch_26_MM5_MAN_0024_0.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/codeswitch_output_26_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;vertical-align: middle;width: 500px\">\u8fd9\u4e2arestaurant\u5f88\u6709\u540d\uff0c\u5f88\u591a\u4eba\u90fd\u6765\u5403\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/codeswitch_17_MM1_MAN_0003_0.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/codeswitch_output_17_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;vertical-align: middle;width: 500px\">\u8fd9\u4e2apizza\u5f88\u597d\u5403\uff0c\u4f60\u8981\u4e0d\u8981try\u4e00\u4e0b\u3002</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/codeswitch_32_MM7_MAN_0019_0.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/codeswitch_output_32_decompressed.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n</tbody>\n</table>\n\n\n\n\n\n<hr class=\"wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots\" />\n\n\n\n\n\n<p>With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a merge codec approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth in experimental results. In addition, it requires fewer autoregressive steps during inference, resulting in over 60% time savings in inference time.</p>\n\n\n\n<p>This page is for&nbsp;<strong>research demonstration purposes</strong>&nbsp;only.</p>\n\n\n\n<div class=\"wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  has-media-on-the-right is-stacked-on-mobile is-style-border\" data-bi-an=\"media-text\"><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<h2 class=\"wp-block-heading\" id=\"model-overview-2\">Model Overview</h2>\n\n\n\n<p>The overview of VALL-E R, a robust and efficient neural codec language model for zero-shot TTS. It incorporates phoneme information (green) when predict audio codec (blue), which can enhance the connection between phoneme and audio to improve the robustness of decoder-only transformer TTS model. Note that VALL-E R achieves faster inference speeds by adopting compact codec codes, derived from the proposed merge codec method, within its autoregressive model.</p>\n</div><figure class=\"wp-block-media-text__media\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"451\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/valler-1024x451.jpg\" alt=\"diagram\" class=\"wp-image-1027320 size-full\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/valler-1024x451.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/valler-300x132.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/valler-768x338.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/valler-240x106.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/valler.jpg 1500w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure></div>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"xtts-samples\">Audio Samples</h2>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">Text</th>\n<th style=\"text-align: center\">Prompt</th>\n<th style=\"text-align: center\">VALL-E</th>\n<th style=\"text-align: center\">VALL-E R</th>\n<th style=\"text-align: center\">Ground Truth</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">THEN DEAR SAID MISSUS WHITNEY YOU MUST BE KINDER TO HER THAN EVER THINK WHAT IT WOULD BE FOR ONE OF YOU TO BE AWAY FROM HOME EVEN AMONG FRIENDS</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/237-126133-0002.encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/237-126133-0002.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/237-126133-0002.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/237-126133-0002.gt_.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">CRIED ALICE AGAIN FOR THIS TIME THE MOUSE WAS BRISTLING ALL OVER AND SHE FELT CERTAIN IT MUST BE REALLY OFFENDED</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/260-123440-0019.encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/260-123440-0019.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/260-123440-0019.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/260-123440-0019.gt_.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">THE CHAOS IN WHICH HIS ARDOUR EXTINGUISHED ITSELF WAS A COLD INDIFFERENT KNOWLEDGE OF HIMSELF</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/1089-134686-0008.encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/1089-134686-0008.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/1089-134686-0008.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/1089-134686-0008.gt_.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">YOU WERE QUITE RIGHT TO SAY NO AMBROSE BEGAN NEVER SMOKE WITH JOHN JAGO HIS CIGARS WILL POISON YOU</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/5142-36377-0023.encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/5142-36377-0023.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/5142-36377-0023.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/5142-36377-0023.gt_.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">MOTHER CAREY POURED COFFEE NANCY CHOCOLATE AND THE OTHERS HELPED SERVE THE SANDWICHES AND CAKE DOUGHNUTS AND TARTS</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/4992-41806-0011.encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/4992-41806-0011.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/4992-41806-0011.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/4992-41806-0011.gt_.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n</tbody>\n</table>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">Text</th>\n<th style=\"text-align: center\">Prompt</th>\n<th style=\"text-align: center\">VALL-E</th>\n<th style=\"text-align: center\">VALL-E R</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">HE SAT DOWN WEAK BEWILDERED AND ONE THOUGHT WAS UPPERMOST ZORA</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/1995-1837-0019.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/1995-1837-0019.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/1995-1837-0019.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">EDISON HELD THAT THE ELECTRICITY SOLD MUST BE MEASURED JUST LIKE GAS OR WATER AND HE PROCEEDED TO DEVELOP A METER</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/2300-131720-0027.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/2300-131720-0027.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/2300-131720-0027.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">THE LODGE IN WHICH UNCAS WAS CONFINED WAS IN THE VERY CENTER OF THE VILLAGE AND IN A SITUATION PERHAPS MORE DIFFICULT THAN ANY OTHER TO APPROACH OR LEAVE WITHOUT OBSERVATION</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/1320-122617-0011.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/1320-122617-0011.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/1320-122617-0011.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">AND THEN HE TOLD ALL ABOUT HIS YOUTH AND THE LITTLE MICE HAD NEVER HEARD THE LIKE BEFORE AND THEY LISTENED AND SAID</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/672-122797-0055.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/672-122797-0055.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/672-122797-0055.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">THE QUESTION IS WHICH OF THE TWO METHODS WILL MOST EFFECTIVELY REACH THE PERSONS WHOSE CONVICTIONS IT IS DESIRED TO AFFECT</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/3570-5695-0008.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/3570-5695-0008.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/3570-5695-0008.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n\n</tbody>\n</table>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">Text</th>\n<th style=\"text-align: center\">Prompt</th>\n<th style=\"text-align: center\">Prosody Reference</th>\n<th style=\"text-align: center\">VALL-E R</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">IF IT ONLY WERE NOT SO DARK HERE AND SO TERRIBLY LONELY</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/672-122797-0048.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/672-122797-0048.prosody.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/672-122797-0048.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">I PRAY FOR YOU BUT THAT&#8217;S NOT THE SAME AS IF YOU PRAYED YOURSELF</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/237-134500-0040.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/237-134500-0040.prosody.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/237-134500-0040.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">ALL MY DANGER AND SUFFERINGS WERE NEEDED TO STRIKE A SPARK OF HUMAN FEELING OUT OF HIM BUT NOW THAT I AM WELL HIS NATURE HAS RESUMED ITS SWAY</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/260-123286-0002.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/260-123286-0002.prosody.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/260-123286-0002.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">AND THE WHOLE NIGHT THE TREE STOOD STILL AND IN DEEP THOUGHT</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/672-122797-0040.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/672-122797-0040.prosody.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/672-122797-0040.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">AGAINST THESE BOASTING FALSE APOSTLES PAUL BOLDLY DEFENDS HIS APOSTOLIC AUTHORITY AND MINISTRY</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/2830-3980-0006.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/2830-3980-0006.prosody.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/2830-3980-0006.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n\n</tbody>\n</table>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">Text</th>\n<th style=\"text-align: center\">Prompt</th>\n<th style=\"text-align: center\">VALL-E</th>\n<th style=\"text-align: center\">VALL-E R</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">As the cosmic cosmic cosmic cosmic cosmic cosmic dance of the stars unfolds in in in in in in silence, revealing the mystical mysteries of the celestial celestial celestial celestial celestial celestial realm</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/61.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/61.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/61.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">Beneath the moonlit night, the solitary wolf\u2019s haunting howl howl howl howl howl echoed through the ancient forest, embodying the primal spirit of the wilderness</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/64.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/64.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/64.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">The relentless relentless relentless relentless relentless relentless pursuit of perfection in in in in in in in in in craftsmanship led the artisan to create an exquisite masterpiece admired for its meticulous meticulous meticulous meticulous meticulous meticulous details</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/81.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/81.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/81.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">As the quantum physicist delved into the quantum realm, the enigmatic entanglement of particles perplexed even the most astute astute astute astute astute astute minds</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/62.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/62.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/62.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">Adventurous ants anxiously ate apples, adventurous adventurous apples</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/31.prompt.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/31.valle_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/31.valler.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n\n</tbody>\n</table>\n\n\n\n\n\n<table class=\"table table-hover pt-2\">\n<thead>\n<tr>\n<th style=\"text-align: center\">Text</th>\n<th style=\"text-align: center\">Ground Truth</th>\n<th style=\"text-align: center\">Merge Codec</th>\n<th style=\"text-align: center\">Encodec</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">AND ALL HIS BROTHERS AND SISTERS STOOD ROUND AND LISTENED WITH THEIR MOUTHS OPEN</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/7021-85628-0022.gt_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/7021-85628-0022.merge_encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/7021-85628-0022.encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">THEN AS IF SATISFIED OF THEIR SAFETY THE SCOUT LEFT HIS POSITION AND SLOWLY ENTERED THE PLACE</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/1320-122617-0017.gt_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/1320-122617-0017.merge_encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/1320-122617-0017.encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">HOTEL A PLACE WHERE A GUEST OFTEN GIVES UP GOOD DOLLARS FOR POOR QUARTERS</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/121-121726-0009.gt_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/121-121726-0009.merge_encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/121-121726-0009.encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">THIS WAS WHAT DID THE MISCHIEF SO FAR AS THE RUNNING AWAY WAS CONCERNED</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/8463-287645-0000.gt_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/8463-287645-0000.merge_encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/8463-287645-0000.encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n<tr>\n<td style=\"text-align: left;vertical-align: middle;width: 600px\">IT WAS IN A CORNER THAT HE LAY AMONG WEEDS AND NETTLES</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/672-122797-0069.gt_.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/672-122797-0069.merge_encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n<td style=\"text-align: center\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/672-122797-0069.encodec.wav\" controls=\"controls\"></audio></figure>\n</td>\n</tr>\n\n\n</tbody>\n</table>\n\n\n\n\n\n\n\n<p>VALL-E 2 is the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), <em>achieving human parity for the first time</em>. Building upon the foundation laid by its predecessor, VALL-E, the new iteration introduces two significant enhancements to elevate its performance: <strong>Repetition Aware Sampling</strong> refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue encountered in VALL-E. <strong>Grouped Code Modeling</strong> organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments, conducted on the LibriSpeech and VCTK datasets, have shown that VALL-E 2 surpasses previous zero-shot TTS systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases.</p>\n\n\n\n<p>This page is for <strong>research demonstration purposes</strong> only.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-130 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:100%\">\n<table style=\"margin-left: auto;margin-right: auto\">\n<tr>\n<td style=\"width: 47%\" align=\"center\">\n<figure class=\"wp-block-image size-large is-resized\"><img decoding=\"async\" class=\"wp-image-943299\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/VALLE2-1024x496.jpg\" alt=\"raphical user interface, application, Word\"></figure>\n</td>\n<td style=\"width:6%\" />\n<td align=\"center\">\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img decoding=\"async\" class=\"wp-image-944352\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/Overview-1024x579.png\" alt=\"graphical user interface, application, Word\" /></figure>\n</td>\n</tr>\n<tr>\n<td>\n<p> Grouped code modeling not only accelerates inference by reducing the sequence length but also improves performance by mitigating the long context modeling problem. Based on the token repetition in the decoding history, repetition aware sampling enhances the stability of the decoding process and circumvents the infinite loop issue encountered in VALL-E.</p>\n</td>\n<td />\n<td>\n<p> VALL-E 2 achieves human parity zero-shot TTS performance for the first time. Robustness, naturalness and similarity scores are relative numbers calculated based on the results reported in the original papers, irrespective of differences in model architecture and training data.</p>\n</td>\n</tr>\n</table>\n</div>\n</div>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"hard-examples\">Hard Examples</h2>\n\n\n\n\n\n<table class=\"table table-hover pt-2\" style=\"height: 0px;border-spacing: inherit;border-collapse: collapse\">\n\t\t\t<thead>\n\t\t\t<tr>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">Text</th>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">Speaker Prompt </th>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">VALL-E </th>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">VALL-E 2</th>\n\t\t\t</tr>\n\t\t\t</thead>\n\t\t\t<tbody>\n\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: left;vertical-align:middle;width: 500px\">F one F two F four F eight H sixteen H thirty two H sixty four</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_0_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_0_valle.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_0_valle.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_0_valle2.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_0_valle2.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: left;vertical-align:middle;width: 500px\">Clever cats carefully crafted colorful collages creating cheerful compositions</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_7_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_7_valle.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_7_valle.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_7_valle2.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_7_valle2.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: left;vertical-align:middle;width: 500px\">Curious koalas curiously climbed curious curious climbers</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_40_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_40_valle.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_40_valle.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_40_valle2.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_40_valle2.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: left;vertical-align:middle;width: 500px\">Sad snakes sadly sighed sad sad sighs</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_42_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_42_valle.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_42_valle.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_42_valle2.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_42_valle2.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: left;vertical-align:middle;width: 500px\">Joyful jaguars joyfully jumped joyful joyful jumps</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_46_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_46_valle.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_46_valle.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_46_valle2.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_46_valle2.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: left;vertical-align:middle;width: 500px\">Noisy newts nonsensically nibbled noisy noisy nibbles</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_48_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_48_valle.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_48_valle.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_48_valle2.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_48_valle2.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: left;vertical-align:middle;width: 500px\">Crafting a symphony of flavors the skilled chef orchestrated a culinary masterpiece that left an indelible mark mark mark mark mark on the palates of the discerning diners</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_67_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_67_valle.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_67_valle.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_67_valle2.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_67_valle2.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: left;vertical-align:middle;width: 500px\">The future belongs to belongs to belongs to belongs to belongs to those who believe in the beauty of the beauty of the beauty of the beauty of the beauty of their dreams</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_89_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_89_valle.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_89_valle.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_89_valle2.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_hard_samples_89_valle2.1.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t</tbody>\n\t\t\t</table>\n\n\n\n\n\n<h2 class=\"wp-block-heading\" id=\"librispeech-samples\">LibriSpeech Samples</h2>\n\n\n\n\n\n<table class=\"table table-hover pt-2\" style=\"height: 0px;border-spacing: inherit;border-collapse: collapse\">\n\t\t\t<thead>\n\t\t\t<tr>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">Text</th>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">Speaker Prompt (Prefix/Ref)</th>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">VALL-E </th>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">VALL-E 2 <br> (GroupSize=1)</th>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">VALL-E 2 <br> (GroupSize=2)</th>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">VALL-E 2 <br> (GroupSize=4)</th>\n\t\t\t</tr>\n\t\t\t</thead>\n\t\t\t<tbody>\n\t\t\t<tr>\n\t\t\t<td rowspan=\"2\" style=\"text-align: left;vertical-align:middle;width: 500px\">They moved thereafter cautiously about the hut groping before and about them to find something to show that Warrenton had fulfilled his mission</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_809_conti_infer_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure></td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_809_conti_infer_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_809_conti_infer_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_809_conti_infer_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_809_conti_infer_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_809_cross_infer_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure></td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_809_cross_infer_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_809_cross_infer_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_809_cross_infer_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_809_cross_infer_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td rowspan=\"2\" style=\"text-align: left;vertical-align:middle;width: 500px\">And lay me down in thy cold bed and leave my shining lot</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1216_conti_infer_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure></td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1216_conti_infer_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1216_conti_infer_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1216_conti_infer_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1216_conti_infer_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1216_cross_infer_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure></td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1216_cross_infer_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1216_cross_infer_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1216_cross_infer_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1216_cross_infer_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td rowspan=\"2\" style=\"text-align: left;vertical-align:middle;width: 500px\">Number ten fresh nelly is waiting on you good night husband</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1_conti_infer_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure></td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1_conti_infer_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1_conti_infer_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1_conti_infer_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1_conti_infer_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1_cross_infer_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure></td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1_cross_infer_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1_cross_infer_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1_cross_infer_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_1_cross_infer_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td rowspan=\"2\" style=\"text-align: left;vertical-align:middle;width: 500px\">Yea his honourable worship is within but he hath a godly minister or two with him and likewise a leech</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_74_conti_infer_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure></td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_74_conti_infer_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_74_conti_infer_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_74_conti_infer_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_74_conti_infer_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_74_cross_infer_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure></td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_74_cross_infer_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_74_cross_infer_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_74_cross_infer_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_74_cross_infer_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t\n\t\t\t<tr>\n\t\t\t<td rowspan=\"2\" style=\"text-align: left;vertical-align:middle;width: 500px\">Instead of shoes the old man wore boots with turnover tops and his blue coat had wide cuffs of gold braid</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_78_conti_infer_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure></td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_78_conti_infer_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_78_conti_infer_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_78_conti_infer_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_78_conti_infer_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_78_cross_infer_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure></td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_78_cross_infer_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_78_cross_infer_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_78_cross_infer_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_librispeech_78_cross_infer_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t\n\t\t\t</tbody>\n\t\t\t</table>\n\n\n\n\n\n<h2 class=\"wp-block-heading\" id=\"vctk-samples\">VCTK Samples</h2>\n\n\n\n\n\n<table class=\"table table-hover pt-2\" style=\"height: 0px;border-spacing: inherit;border-collapse: collapse\">\n\t\t\t<thead>\n\t\t\t<tr>\n\t\t\t\t<th style=\"text-align: center;vertical-align:middle\">Text</th>\n\t\t\t\t<th style=\"text-align: center;vertical-align:middle\">Speaker Prompt (3s/5s/10s)</th>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">VALL-E </th>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">VALL-E 2 <br>  (GroupSize=1)</th>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">VALL-E 2 <br>  (GroupSize=2)</th>\n\t\t\t<th style=\"text-align: center;vertical-align:middle\">VALL-E 2 <br>  (GroupSize=4)</th>\n\t\t\t</tr>\n\t\t\t</thead>\n\t\t\t<tbody>\n\t\t\t<tr>\n\t\t\t<td rowspan=\"3\" style=\"text-align: left;vertical-align:middle;width: 500px\">We have to reduce the number of plastic bags</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_3s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_3s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_3s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_3s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_3s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_5s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_5s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_5s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_5s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_5s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_10s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_10s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_10s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_10s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_27_10s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td rowspan=\"3\" style=\"text-align: left;vertical-align:middle;width: 500px\">So what is the campaign about</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_3s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_3s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_3s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_3s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_3s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_5s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_5s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_5s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_5s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_5s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_10s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_10s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_10s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_10s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_36_10s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td rowspan=\"3\" style=\"text-align: left;vertical-align:middle;width: 500px\">My life has changed a lot</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_3s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_3s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_3s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_3s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_3s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_5s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_5s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_5s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_5s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_5s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_10s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_10s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_10s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_10s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_46_10s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td rowspan=\"3\" style=\"text-align: left;vertical-align:middle;width: 500px\">Nothing is yet confirmed</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_3s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_3s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_3s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_3s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_3s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_5s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_5s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_5s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_5s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_5s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_10s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_10s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_10s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_10s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_28_10s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td rowspan=\"3\" style=\"text-align: left;vertical-align:middle;width: 500px\">I could hardly move for the next couple of days</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_3s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_3s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_3s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_3s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_3s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_5s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_5s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_5s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_5s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_5s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_10s_prompt.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_10s_valle.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_10s_valle2_shift1.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_10s_valle2_shift2.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t<td style=\"text-align: center;vertical-align:middle\">\n\t\t\t\t<figure><audio style=\"width: 140px\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/06/valle2_vctk_35_10s_valle2_shift4.vocos_.0.wav\" controls=\"controls\" preload=\"none\"></audio></figure>\n\t\t\t</td>\n\t\t\t</tr>\n\t\t\t\n\t\t\t<tr>\n\t\t\t</tr></tbody>\n\t\t\t</table>\n\n\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>A neural codec language model for speech synthesis We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 932478,
        "date": "2023-06-06T10:00:55",
        "slug": "asl-citizen",
        "title": "ASL Citizen",
        "link": "https://www.microsoft.com/en-us/research/project/asl-citizen/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"3840\" height=\"1920\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/06/ASLCitizen_grid_blurred.png\" class=\"attachment-full size-full\" alt=\"A grid of screenshots from the ASL Citizen dataset, showing different people performing different signs in American Sign Language.\" style=\"object-position: 44% 41%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/06/ASLCitizen_grid_blurred.png 3840w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/ASLCitizen_grid_blurred-300x150.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/ASLCitizen_grid_blurred-1024x512.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/ASLCitizen_grid_blurred-768x384.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/ASLCitizen_grid_blurred-1536x768.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/ASLCitizen_grid_blurred-2048x1024.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/ASLCitizen_grid_blurred-240x120.png 240w\" sizes=\"(max-width: 3840px) 100vw, 3840px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"asl-citizen\">ASL Citizen</h1>\n\n\n\n<p>A Community-sourced Dataset for Advancing Isolated Sign Language Recognition</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"Overview\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/EohCVAZ1UwE?feature=oembed&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</div></figure>\n\n\n\n<p>Signed languages are the primary languages of about <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"http://wfdeaf.org/our-work/\" target=\"_blank\" rel=\"noreferrer noopener\">70 million D/deaf people worldwide<span class=\"sr-only\"> (opens in new tab)</span></a>. Despite their importance, existing information and communication technologies are primarily designed for written or spoken language.&nbsp;Though automated solutions&nbsp;might help address such&nbsp;accessibility gaps, the state of sign language modeling is far behind that of spoken language modeling, largely due to lack of appropriate training data. Prior datasets suffer from a combination of small size, limited diversity, lack of real-world recording settings, poor labels, and lack of consent for video use.&nbsp;</p>\n\n\n\n<p>To help advance the state of sign language modeling, we created ASL Citizen &#8212; the first crowdsourced isolated sign language dataset, containing about 84k&nbsp;videos of 2.7k&nbsp;distinct signs from American Sign Language (ASL). This dataset is the largest-to-date Isolated Sign Language Recognition (ISLR) dataset. In addition to its size, unlike prior datasets, it contains everyday signers in everyday recording scenarios, and was collected with consent from each contributor under IRB approval. Deaf research team members were involved throughout.</p>\n\n\n\n<p>This dataset is released alongside&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2304.05934\" target=\"_blank\" rel=\"noreferrer noopener\">our paper<span class=\"sr-only\"> (opens in new tab)</span></a> reframing ISLR as a dictionary retrieval task and establishing state-of-the-art baselines. In dictionary retrieval, someone sees or thinks of a sign that they would like to look up; they try to repeat the sign in front of an everyday (RGB) camera; and then an ISLR algorithm returns a list of signs that are closest to the demonstrated sign.&nbsp;The results list may be accompanied by sign definitions in text or sign language video. This framing grounds ISLR research in a meaningful, real-world application. Our baselines leverage existing appearance and pose-based techniques, and with our dataset improve the state-of-the-art in ISLR from about 32% to 62% accuracy.</p>\n\n\n\n<p>This project was conducted at Microsoft Research with collaborators at multiple organizations.</p>\n\n\n\n<ul>\n<li>Microsoft: Danielle Bragg (PI), Mary Bellard, Hal Daum\u00e9 III, Alex Lu, Vanessa Milan, Fyodor Minakov, Paul Oka, Philip Rosenfield, Chinmay Singh, William Thies</li>\n\n\n\n<li>Boston University: Lauren Berger, Naomi Caselli, Miriam Goldberg, Hannah Goldblatt, Kriston Pumphrey</li>\n\n\n\n<li>University of Washington: Aashaka Desai, Richard Ladner</li>\n\n\n\n<li>Rochester Institute of Technology: Abraham Glasser</li>\n</ul>\n\n\n\n<p><strong>Dataset License:</strong> Please see the supporting tab. If you are interested in commercial use, please contact&nbsp;<a href=\"mailto:ASL_Citizen@microsoft.com\" target=\"_blank\" rel=\"noreferrer noopener\">ASL_Citizen@microsoft.com</a>.&nbsp;</p>\n\n\n\n<p><strong>Dataset Download:</strong> </p>\n\n\n\n<p>To download via web interface, please visit: <a href=\"https://www.microsoft.com/en-us/download/details.aspx?id=105253\" target=\"_blank\" rel=\"noreferrer noopener\">Download ASL Citizen from Official Microsoft Download Center</a></p>\n\n\n\n<p>To download via command line, please execute: wget https://download.microsoft.com/download/b/8/8/b88c0bae-e6c1-43e1-8726-98cf5af36ca4/ASL_Citizen.zip</p>\n\n\n\n<p><strong>Open-source Repo:</strong> <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/ASL-citizen-code\" target=\"_blank\" rel=\"noreferrer noopener\">https://github.com/microsoft/ASL-citizen-code<span class=\"sr-only\"> (opens in new tab)</span></a> </p>\n\n\n\n<p><strong>Citation:</strong> If you use this dataset in your work, please cite <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2304.05934\" target=\"_blank\" rel=\"noreferrer noopener\">our paper<span class=\"sr-only\"> (opens in new tab)</span></a>.</p>\n\n\n\n<pre class=\"wp-block-preformatted\">@article{desai2023asl,\n  title={ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition},\n  author={Desai, Aashaka and Berger, Lauren and Minakov, Fyodor O and Milan, Vanessa and Singh, Chinmay and Pumphrey, Kriston and Ladner, Richard E and Daum{\\'e} III, Hal and Lu, Alex X and Caselli, Naomi and Bragg, Danielle},\n  journal={arXiv preprint arXiv:2304.05934},\n  year={2023}\n}</pre>\n\n\n\n<p><strong>Acknowledgements:</strong> We are deeply grateful to all community members who participated in this dataset project.</p>\n\n\n\n\n\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"Deaf Culture\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/9B3-lABCvgY?feature=oembed&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</div></figure>\n\n\n\n<p>Deaf and hard-of-hearing (DHH) individuals communicate in many ways, including&nbsp;through sign language. Along with being an accessible modality for DHH individuals, signed languages are culturally significant, forming a cornerstone of shared experience, cultural identity, and institutions for Deaf communities worldwide. &nbsp;</p>\n\n\n\n<p>American Sign Language (ASL) is the primary sign language used in North America (and several other parts of the world). Like other signed languages, signs in ASL are composed of phonological elements including handshapes, hand and body movements, hand location, and facial expressions. Vocabulary size is large, and complex rules govern how these signs are put together to make sentence. These rules for combining words, phonology, and morphology into sentences are rich, and substantially different from English. Sign execution also varies among signers, across contexts, and through regions and dialects. ASL is a complete, natural language in its own right.</p>\n\n\n\n<p>Signed languages play a central role in Deaf cultures and identity. Groups of people who primarily communicate in a signed language form distinct cultures as sociolinguistic minorities within the broader hearing majority. Within these communities, Deafness is a proud cultural identity. Despite the richness of signed languages and Deaf cultures, Deaf communities have a history of marginalization and oppression by the hearing majority. Many harmful misconceptions and biases exist within hearing communities about signed languages and Deaf people. For example, education systems have suppressed the use of signed languages, to the detriment of many deaf students.</p>\n\n\n\n<p>Given this context, it is particularly important that sign language technologies are developed in partnership with Deaf communities and with an understanding of Deaf culture and signed languages. To this end, we involved Deaf collaborators in key roles at every step of this project, including conception, recruitment, participation, analysis, and dissemination. We encourage those using this dataset to educate themselves on Deaf culture and American Sign Language in order to conduct research and build systems that are useful to Deaf community members while minimizing harms.</p>\n\n\n\n<p>As an entry point to more information on Deaf cultures and sign languages,&nbsp;please check out the following resources.</p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://nationaldeafcenter.org/resource-items/deaf-community-introduction/\" target=\"_blank\" rel=\"noreferrer noopener\">The Deaf Community: An Introduction &#8211; National Deaf Center<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.nad.org/resources/american-sign-language/community-and-culture-frequently-asked-questions/\" target=\"_blank\" rel=\"noreferrer noopener\">Community and Culture Frequently Asked Questions &#8211; National Association of the Deaf<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://wfdeaf.org/\">Home Page &#8211; World Federation of the Deaf<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.museumofdeaf.org/\">Home Page &#8211; Museum of Deaf History, Arts & Culture<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n\n\n\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"Dataset Description\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/2I7EU6uHmrI?feature=oembed&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</div></figure>\n\n\n\n<p>ASL Citizen is the first crowdsourced isolated sign language video dataset. The dataset contains about 84k video recordings of 2.7k isolated signs from American Sign Language (ASL), and is about four times larger than prior single-sign datasets. Our videos were recorded by 52 Deaf or hard of hearing signers through our novel sign language crowdsourcing platform, first proposed in our <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://dl.acm.org/doi/abs/10.1145/3555627\" target=\"_blank\" rel=\"noreferrer noopener\">prior work<span class=\"sr-only\"> (opens in new tab)</span></a> exploring crowdsourcing for sign language video collection, and enhanced for scalability. Because the data is crowdsourced, it contains examples of everyday signers in everyday environments, providing more representative data for data-driven machine learning models intended to generalize in real-world settings, such as Isolated Sign Language Recognition (ISLR) models deployed for applications like sign language dictionary retrieval.</p>\n\n\n\n<p>On the platform, signers contributed videos for two simultaneous purposes: 1) to help build a community-sourced dictionary, and 2) to contribute to a dataset for research purposes. This dual-purpose design enables the creation of a direct community resource, while also enabling longer-term research. On the website, participants were prompted by a series of videos of isolated signs (as demonstrated by a highly proficient ASL model, who we refer to as the \u201cseed signer\u201d), and recorded their own version of each. This setup allowed us to capture a range of backgrounds and lighting conditions, similar to those present in everyday dictionary lookup queries. Our task design also enabled us to automatically label each video with the prompt sign, largely eliminating post-hoc labelling challenges that have limited large sign language data collections in the past. Finally, this setup enabled participants to engage in a full consent process, unlike past scraped sign language video collections. Table 1 below provides a comparison to past datasets.&nbsp;</p>\n\n\n\n<p>The sign vocabulary we use is taken from <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://asl-lex.org/download.html\" target=\"_blank\" rel=\"noreferrer noopener\">ASL-LEX<span class=\"sr-only\"> (opens in new tab)</span></a>, which also provides detailed linguistic analysis of each sign. The signs in our corpus map onto the signs in their linguistic corpus, using the unique identifier token (referred to as the sign &#8220;Code&#8221; in ASL-LEX resources).</p>\n\n\n\n<figure class=\"wp-block-image size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"268\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table1-1024x268.png\" alt=\"Table comparing prior ISLR datasets for ASL compared to ASL Citizen. Columns -- Dataset, Vocab size, Videos, Videos/sign, Signers, Collection, Consent. Rows -- RWTH BOSTN-50: 50, 483, 9.7, 3Deaf, Lab, Check; Purdue RVL-SLL: 39, 546, 14.0, 14 Deaf, Lab, Check. Boston ASLLVD; 2,742, 9,794, 3.6, 6 Deaf, Lab, Check; WLASL-2000: 2,000, 21,083, 10.5, 119 Unknown, Scraped, X; ASL Citizen: 2,731, 83,399, 30.5, 52 Deaf/HH, Crowd, Check.\" class=\"wp-image-945624\" style=\"width:768px;height:201px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table1-1024x268.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table1-300x79.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table1-768x201.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table1-1536x402.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table1-2048x536.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table1-240x63.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\">Table 1: Properties of existing ISLR datasets for ASL compared to our new dataset (<strong>ASL Citizen</strong>, last row).</figcaption></figure>\n\n\n\n<p>We release our dataset along with training, validation, and test splits (shown in Table 2 below). Each dataset participant has been assigned to one split. By creating signer-independent splits, we provide a setup for methods development that more aligns with real-world recognition applications, where users querying a model are unlikely to be previously seen in training data. The distribution of signers across splits was chosen to balance female-male gender ratio. We also provide generalized user demographics and gloss metadata to support further analysis. Please note that the seed signer is included in the dataset as P52.</p>\n\n\n\n<figure class=\"wp-block-image size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"213\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table2-1024x213.png\" alt=\"Table showing statistics for ASL Citizen dataset splits. Columns -- Train, Val, Test. Rows -- Users: 35, 6, 11; Videos: 40,154, 10,304, 32,941; User distribution: 60% F, 83% F, 55% F; Video distribution: 54% F, 71 %F, 55% F.\" class=\"wp-image-945627\" style=\"width:768px;height:160px\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table2-1024x213.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table2-300x62.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table2-768x160.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table2-1536x319.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table2-240x50.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/dataset-table2.png 1776w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\">Table 2: Description of our splits: train, validation (val), and test. Splits are user-independent and designed to have roughly comparable gender breakdowns.</figcaption></figure>\n\n\n\n<p>The video counts displayed in publications and on this page may differ slightly. This is due to small dataset updates that occurred during preparation and analysis (e.g. to remove a small number of videos that displayed an error message during webcam recording). All collection and release procedures were reviewed and approved by our ethics review board and IRB.</p>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><td>Version</td><td># Videos</td><td># Signs</td><td># Signers</td><td>Date, Publication(s)</td></tr><tr><td>Version 0.9</td><td>83,912</td><td>2,731</td><td>52</td><td>April 2023, arXiv initial publication</td></tr><tr><td>Version 1.0</td><td>83,399</td><td>2,731</td><td>52</td><td>June 2023</td></tr></tbody></table><figcaption class=\"wp-element-caption\">Table 3: List of dataset versions.</figcaption></figure>\n\n\n\n<p>For additional details on the dataset, please see the datasheet in the supporting tab, and check out our publications.</p>\n\n\n\n\n\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"Reccomended Use\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/6ffm3mjwmZw?feature=oembed&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</div></figure>\n\n\n\n<p>This dataset was designed primarily for work on isolated sign language recognition (ISLR), and within that space we recommend using this dataset for the task of dictionary retrieval. We define the task of video-based dictionary retrieval as: given a video of a person demonstrating a single sign through a webcam, the system retrieves a ranked list of dictionary entries that match that sign. This task is useful for creating reliable ASL-to-ASL or ASL-to-English dictionaries, which are essential tools for language learners and users.</p>\n\n\n\n<p>We caution against using this dataset for understanding continuous signing by tokenizing sequences of signs that might map to our dataset. Continuous signing &#8212; full sentences and longer signed content &#8212; introduces many grammatical and structural complexities not present in isolated signs (e.g. signs may be modulated, co-articulation effects, how context changes the meaning of signs, etc.). Many of these complexities are also absent from spoken/written languages. At a minimum, this dataset would need to be used in conjunction with other datasets and/or domain knowledge about sign language in order to tackle continuous recognition or translation.</p>\n\n\n\n<p>We ask that this dataset be used with an aim of making the world more equitable and just for deaf people, and with a commitment to do no harm. In that spirit, this dataset should not be used to develop technology that purports to replace sign language interpreters, fluent signing educators, and/or other hard-won accommodations for deaf people. We also ask that users of this dataset make no attempt to identify participants, or to use this dataset for applications that might exploit participant identity or appearance, including (but not limited to) facial recognition, deepfakes, or identification of sensitive attributes like race.</p>\n\n\n\n<p>For whichever application you choose, we recommend using this data with meaningful involvement from Deaf community members at every step. As we describe in our linked paper, research and development of sign language technologies that involves Deaf community members in leadership roles with decision-making authority increases the quality of the work, and can help to ensure technologies are relevant and wanted. Historically, projects developed without meaningful Deaf involvement have not been well received and have damaged relationships between technologists and Deaf communities.</p>\n\n\n\n<p>Please see the links below for an entry point to more information about respectful sign language technology development.</p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://blog.castac.org/2022/04/disability-dongle/\" target=\"_blank\" rel=\"noreferrer noopener\">Disability Dongle &#8211; <span class=\"sr-only\"> (opens in new tab)</span></a><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://blog.castac.org/author/lizjackson/\">Liz Jackson<span class=\"sr-only\"> (opens in new tab)</span></a>,&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://blog.castac.org/author/alexhaagaard/\">Alex Haagaard<span class=\"sr-only\"> (opens in new tab)</span></a>,&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://blog.castac.org/author/ruawilliams/\">Rua Williams<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"http://depts.washington.edu/asluw/SignAloud-openletter.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">SignAloud Open Letter &#8211; Lance Forshay, Kristi Winter, Emily M. Bender<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://aclanthology.org/2021.mtsummit-at4ssl.2/\" target=\"_blank\" rel=\"noreferrer noopener\">Is \u201cgood enough\u201d good enough? Ethical and responsible development of sign language technologies &#8211; <span class=\"sr-only\"> (opens in new tab)</span></a><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://aclanthology.org/people/m/maartje-de-meulder/\">Maartje De Meulder<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://books.google.com/books?hl=en&lr=&id=ohqff8DBt9gC&oi=fnd&pg=PR9&dq=james+charlton+nothing+about+us+without+us&ots=5BSsg-InLx&sig=BlM1jqTuRM1nvuNRhm1j2XSBmT8#v=onepage&q=james%20charlton%20nothing%20about%20us%20without%20us&f=false\" target=\"_blank\" rel=\"noreferrer noopener\">Nothing About Us Without Us: Disability Oppression and Empowerment &#8211; James I. Charlton<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/research/publication/the-fate-landscape-of-sign-language-ai-datasets-an-interdisciplinary-perspective/\">The FATE Landscape of Sign Language AI Datasets: An Interdisciplinary Perspective &#8211; Danielle Bragg, Naomi Caselli, Julie A. Hochgesang, Matt Huenerfauth, Leah Katz-Hernandez, Oscar Koller, Raja Kushalnagar, Christian Vogler, Richard E. Ladner</a></li>\n</ul>\n\n\n\n\n\n<p>Dataset contributors added videos for two simultaneous purposes: 1) to create a dataset to help advance research as described in the other tabs, and 2) to contribute to a community-sourced dictionary showcasing the signing community&#8217;s diversity. By creating this dictionary resource, provide immediate and direct benefits to the signing community, while waiting for longer-term benefits derived from research.</p>\n\n\n\n<p>Please check out the community-sourced dictionary here: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://community.aslgames.org/\" target=\"_blank\" rel=\"noreferrer noopener\">https://community.aslgames.org/<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n\n\n<h2 class=\"wp-block-heading\" id=\"microsoft-research-license-terms\">MICROSOFT RESEARCH LICENSE TERMS</h2>\n\n\n\n<p><strong>IF YOU LIVE IN THE UNITED STATES, PLEASE READ THE \u201cBINDING ARBITRATION AND CLASS ACTION WAIVER\u201d SECTION BELOW. IT AFFECTS HOW DISPUTES ARE RESOLVED.</strong></p>\n\n\n\n<p>These license terms are an agreement between you and Microsoft Corporation (or one of its affiliates). They apply to the source code, object code, or data (collectively \u201cMaterials\u201d) that accompany this license. IF YOU COMPLY WITH THESE LICENSE TERMS, YOU HAVE THE RIGHTS BELOW. BY USING THE MATERIALS, YOU ACCEPT THESE TERMS.</p>\n\n\n\n<p><strong>1) INSTALLATION AND USE RIGHTS to The Materials.</strong></p>\n\n\n\n<p>Subject to the terms of this agreement, you have the below rights, if applicable, to use the Materials solely for non-commercial, non-revenue generating, research purposes:</p>\n\n\n\n<p><strong>a)&nbsp;&nbsp;&nbsp; Source Code.</strong> If source code is included, you may use and modify the source code, but you may not distribute the source code.</p>\n\n\n\n<p><strong>b)&nbsp;&nbsp;&nbsp; </strong><strong>Object Code. </strong>If object code is included, you may use the object code, but you may not distribute the object code.</p>\n\n\n\n<p><strong>c)&nbsp;&nbsp;&nbsp; </strong><strong>Data. </strong>If data is included, you may use and modify the data, but your use and modification must be consistent with the consent under which the data was provided and/or gathered and you may not distribute the data or your modifications to the data.</p>\n\n\n\n<p><strong>2)&nbsp;&nbsp;&nbsp; SCOPE OF LICENSE.</strong> The Materials are licensed, not sold. Microsoft reserves all other rights. Unless applicable law gives you more rights despite this limitation, you will not (and have no right to):</p>\n\n\n\n<p><strong>a)&nbsp;&nbsp;&nbsp; </strong>work around any technical limitations in the Materials that only allow you to use it in certain ways;</p>\n\n\n\n<p><strong>b)&nbsp;&nbsp;&nbsp; </strong>reverse engineer, decompile or disassemble the Materials;</p>\n\n\n\n<p><strong>c)&nbsp;&nbsp;&nbsp; </strong>remove, minimize, block, or modify any notices of Microsoft or its suppliers in the Materials;</p>\n\n\n\n<p><strong>d)&nbsp;&nbsp;&nbsp; </strong>use the Materials in any way that is against the law or to create or propagate malware; or</p>\n\n\n\n<p><strong>e)&nbsp;&nbsp;&nbsp; </strong>share, publish, distribute or lend the Materials, provide the Materials as a stand-alone hosted solution for others to use, or transfer the Materials or this agreement to any third party.</p>\n\n\n\n<p><strong>3)&nbsp;&nbsp;&nbsp; PERSONAL DATA.</strong>&nbsp; If the data (set forth in Section 1(c) above) includes or is found to include any data that enables any ability to identify an individual (\u201cPersonal Data\u201d), you will not use such Personal Data for any purpose other than was authorized and consented to by the data subject/research participant.&nbsp; You will not use Personal Data to contact any person.&nbsp; You will keep Personal Data in strict confidence.&nbsp; You will not share any Personal Data that is collected or in your possession with any third party for any reason and as required under the original consent agreement.&nbsp; Further, you will destroy the Personal Data and any backup or copies, immediately upon the completion of your research.&nbsp;</p>\n\n\n\n<p><strong>4)&nbsp;&nbsp;&nbsp; </strong><strong>LICENSE TO MICROSOFT.&nbsp; </strong>Notwithstanding the limitations in Section 1, you may distribute your modifications back to Microsoft, and if you do provide Microsoft with modifications of the Materials, you hereby grant Microsoft, without any restrictions or limitations, a non-exclusive, perpetual, irrevocable, royalty-free, assignable and sub-licensable license, to reproduce, publicly perform or display, install, use, modify, post, distribute, make and have made, sell and transfer such modifications and derivatives for any purpose.</p>\n\n\n\n<p><strong>5)&nbsp;&nbsp;&nbsp; </strong><strong>Publication.&nbsp; </strong>You may publish (or present papers or articles) on your results from using the Materials provided that no material or substantial portion of the Materials is included in any such publication or presentation.</p>\n\n\n\n<p><strong>6)&nbsp;&nbsp;&nbsp; </strong><strong>FEEDBACK.</strong> Any feedback about the Materials provided by you to us is voluntarily given, and Microsoft shall be free to use the feedback as it sees fit without obligation or restriction of any kind, even if the feedback is designated by you as confidential.&nbsp; Such feedback shall be considered a contribution and licensed to Microsoft under the terms of Section 4 above.</p>\n\n\n\n<p><strong>7)&nbsp;&nbsp;&nbsp; </strong><strong>EXPORT RESTRICTIONS.</strong> You must comply with all domestic and international export laws and regulations that apply to the Materials, which include restrictions on destinations, end users, and end use. For further information on export restrictions, visit (aka.ms/exporting).</p>\n\n\n\n<p><strong>8)&nbsp;&nbsp;&nbsp; </strong><strong>SUPPORT SERVICES.</strong> Microsoft is not obligated under this agreement to provide any support services for the Materials. Any support provided is \u201cas is\u201d, \u201cwith all faults\u201d, and without warranty of any kind.<a></a><a></a></p>\n\n\n\n<p><strong>9)&nbsp;&nbsp;&nbsp; </strong><strong>BINDING ARBITRATION AND CLASS ACTION WAIVER. This Section applies if you live in (or, if a business, your principal place of business is in) the United States.&nbsp; </strong>If you and Microsoft have a dispute, you and Microsoft agree to try for 60 days to resolve it informally. If you and Microsoft can\u2019t, you and Microsoft agree to <strong>binding individual arbitration before the American Arbitration Association</strong> under the Federal Arbitration Act (\u201cFAA\u201d), and <strong>not to sue in court in front of a judge or jury</strong>. Instead, a neutral arbitrator will decide. <strong>Class action lawsuits, class-wide arbitrations, private attorney-general actions,</strong> and any other proceeding where someone acts in a representative capacity <strong>are not allowed</strong>; nor is combining individual proceedings without the consent of all parties. The complete Arbitration Agreement contains more terms and is at aka.ms/arb-agreement-1. You and Microsoft agree to these terms.</p>\n\n\n\n<p><strong>10) </strong><strong>ENTIRE AGREEMENT.</strong> This agreement, and any other terms Microsoft may provide for supplements, updates, or third-party applications, is the entire agreement for the Materials.</p>\n\n\n\n<p><strong>11) </strong><strong>APPLICABLE LAW AND PLACE TO RESOLVE DISPUTES.</strong> If you acquired the Materials in the United States or Canada, the laws of the state or province where you live (or, if a business, where your principal place of business is located) govern the interpretation of this agreement, claims for its breach, and all other claims (including consumer protection, unfair competition, and tort claims), regardless of conflict of laws principles, except that the FAA governs everything related to arbitration. If you acquired the Materials in any other country, its laws apply, except that the FAA governs everything related to arbitration. If U.S. federal jurisdiction exists, you and Microsoft consent to exclusive jurisdiction and venue in the federal court in King County, Washington for all disputes heard in court (excluding arbitration). If not, you and Microsoft consent to exclusive jurisdiction and venue in the Superior Court of King County, Washington for all disputes heard in court (excluding arbitration).</p>\n\n\n\n<p><strong>12) </strong><strong>CONSUMER RIGHTS; REGIONAL VARIATIONS.</strong> This agreement describes certain legal rights. You may have other rights, including consumer rights, under the laws of your state, province, or country. Separate and apart from your relationship with Microsoft, you may also have rights with respect to the party from which you acquired the Materials. This agreement does not change those other rights if the laws of your state, province, or country do not permit it to do so. For example, if you acquired the Materials in one of the below regions, or mandatory country law applies, then the following provisions apply to you:</p>\n\n\n\n<p><strong>a)&nbsp;&nbsp;&nbsp; </strong><strong>Australia.</strong> You have statutory guarantees under the Australian Consumer Law and nothing in this agreement is intended to affect those rights.</p>\n\n\n\n<p><strong>b)&nbsp;&nbsp;&nbsp; </strong><strong>Canada.</strong> If you acquired this software in Canada, you may stop receiving updates by turning off the automatic update feature, disconnecting your device from the Internet (if and when you re-connect to the Internet, however, the Materials will resume checking for and installing updates), or uninstalling the Materials. The product documentation, if any, may also specify how to turn off updates for your specific device or software.</p>\n\n\n\n<p><strong>c)&nbsp;&nbsp;&nbsp;&nbsp; </strong><strong>Germany and Austria.</strong></p>\n\n\n\n<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; i.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong><strong>Warranty.</strong> The properly licensed software will perform substantially as described in any Microsoft materials that accompany the Materials. However, Microsoft gives no contractual guarantee in relation to the licensed software.</p>\n\n\n\n<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ii.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong><strong>Limitation of Liability.</strong> In case of intentional conduct, gross negligence, claims based on the Product Liability Act, as well as, in case of death or personal or physical injury, Microsoft is liable according to the statutory law.</p>\n\n\n\n<p>Subject to the foregoing clause (ii), Microsoft will only be liable for slight negligence if Microsoft is in breach of such material contractual obligations, the fulfillment of which facilitate the due performance of this agreement, the breach of which would endanger the purpose of this agreement and the compliance with which a party may constantly trust in (so-called &#8220;cardinal obligations&#8221;). In other cases of slight negligence, Microsoft will not be liable for slight negligence.</p>\n\n\n\n<ol style=\"list-style-type:1\">\n<li>DISCLAIMER OF WARRANTY. THE MATERIALS ARE LICENSED \u201cAS IS.\u201d YOU BEAR THE RISK OF USING THEM. MICROSOFT GIVES NO EXPRESS WARRANTIES, GUARANTEES, OR CONDITIONS. TO THE EXTENT PERMITTED UNDER APPLICABLE LAWS, MICROSOFT EXCLUDES ALL IMPLIED WARRANTIES, INCLUDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT.</li>\n</ol>\n\n\n\n<ol style=\"list-style-type:1\">\n<li>LIMITATION ON AND EXCLUSION OF DAMAGES. IF YOU HAVE ANY BASIS FOR RECOVERING DAMAGES DESPITE THE PRECEDING DISCLAIMER OF WARRANTY, YOU CAN RECOVER FROM MICROSOFT AND ITS SUPPLIERS ONLY DIRECT DAMAGES UP TO U.S. $5.00. YOU CANNOT RECOVER ANY OTHER DAMAGES, INCLUDING CONSEQUENTIAL, LOST PROFITS, SPECIAL, INDIRECT OR INCIDENTAL DAMAGES.</li>\n</ol>\n\n\n\n<p>This limitation applies to (a) anything related to the Materials, services, content (including code) on third party Internet sites, or third party applications; and (b) claims for breach of contract, warranty, guarantee, or condition; strict liability, negligence, or other tort; or any other claim; in each case to the extent permitted by applicable law.</p>\n\n\n\n<p>It also applies even if Microsoft knew or should have known about the possibility of the damages. The above limitation or exclusion may not apply to you because your state, province, or country may not allow the exclusion or limitation of incidental, consequential, or other damages.</p>\n\n\n\n\n\n<h2 class=\"wp-block-heading is-style-default\" id=\"motivation\">Motivation</h2>\n\n\n\n<p><strong>For what purpose was the dataset created?}{Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.</strong></p>\n\n\n\n<p>The dataset was created to help enable research on isolated sign language recognition (ISLR) &#8211; i.e. recognizing individual signs from video clips &#8211; and sign language modeling more generally.</p>\n\n\n\n<p>Specifically, we frame ISLR as a dictionary retrieval task: given a self-recorded video of a user performing a single sign, we aim to retrieve the correct sign from a sign language dictionary. This dataset was created with this framing in mind, with the intent of grounding ISLR research in a practical application useful to the Deaf community. While we believe this dataset is suited for methods development for ISLR in general, this dataset specifically contains signs in American Sign Language (ASL).</p>\n\n\n\n<p>In designing our collection mechanism, we sought to address limitations of prior ISLR datasets. Previous datasets have been limited in terms of number of videos, vocabulary size (i.e. number of signs contained), real-world recording settings, presence and reliability of labels, Deaf representation, and/or number of contributors. Some past datasets (in particular scraped datasets) have also included videos without explicit consent from the video creators or signers in the videos.</p>\n\n\n\n<p><strong>Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?</strong></p>\n\n\n\n<p>This dataset was created by Microsoft Research in collaboration with Boston University. Each organization&#8217;s involvement in collection is detailed below.</p>\n\n\n\n<p>Microsoft: platform design, platform engineering, primary ethics board (IRB) review of collection procedures (review of record), additional compliance review of and guidance for the platform (e.g. privacy, security, etc.), platform maintenance and debugging, technical support for participants, hosting of collection infrastructure (website, database, videos, backups, etc.), funding for participant compensation, data processing and cleaning, ethics and compliance board review of the dataset release (e.g. privacy, data cleaning, metadata, etc.), hosting of released assets (dataset, code, other supplementary materials)</p>\n\n\n\n<p>Boston University: platform feedback, seed sign recordings, secondary IRB review of collection procedures, participant recruitment, answering or redirecting participant questions, procurement and distribution of participant compensation</p>\n\n\n\n<p><strong>Who funded the creation of the dataset?</strong> If there is an associated grant, please provide the name of the grantor and the grant name and number.</p>\n\n\n\n<p>Microsoft primarily funded the creation of this dataset. Microsoft funded building and maintaining the collection platform, data storage and processing, participant compensation, and all time spent on the Microsoft activities listed above.</p>\n\n\n\n<p>Boston University funded all time spent on the Boston University activities listed above. Support was provided in part by National Science Foundation Grants: BCS-1625954 and BCS-1918556 to Karen Emmorey and Zed Sehyr, BCS-1918252 and BCS-1625793 to Naomi Caselli, and BCS-1625761 and BCS-1918261 to Ariel Cohen-Goldberg. Additional funding was from the National Institutes of Health National Institute on Deafness and Other Communication Disorders of and Office of Behavioral and Social Science Research under Award Number 1R01DC018279.</p>\n\n\n\n<p><strong>Any other comments?</strong></p>\n\n\n\n<p>None</p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"composition\">Composition</h2>\n\n\n\n<p><strong>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?</strong> Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.</p>\n\n\n\n<p>The instances are self-recorded videos of participants performing individual signs in ASL. Examples of still frames from this dataset are shown in our paper publication. Distribution of video lengths is shown in Figure 1.</p>\n\n\n\n<p><strong>How many instances are there in total (of each type, if appropriate)?</strong></p>\n\n\n\n<p>There are 83,399 instances of videos. In total, this data represents videos from 52 participants over a vocabulary size of 2,731 signs in ASL. On average, there are 30.5 videos for each sign and 1,604 videos per participant. The distribution of videos per participant is bimodal because we compensated participants for up to 3,000 videos. For 22 participants, the dataset includes 2992 +/- 16 videos. For the remaining 30 participants, the dataset includes an average of 586 videos. (These video counts are for dataset Version 1.0, our first publicly released version of the dataset, after processing and cleaning.)</p>\n\n\n\n<p><strong>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</strong> If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).</p>\n\n\n\n<p>The dataset contains a sample of single-sign videos, covering many fundamental ASL signs, and demonstrated by a sample of Deaf and hard-of-hearing community members in everyday environments.</p>\n\n\n\n<p>The ASL vocabulary was taken from ASL-LEX [2], which is a linguistically analyzed corpus of ASL vocabulary, covering many fundamental ASL signs. Specifically, our dataset contains 2,731 distinct signs (or glosses).</p>\n\n\n\n<p>We chose to adopt this vocabulary set because of the provision of detailed linguistic analysis of each sign, which complements the video set we provide, and allows for a richer set of uses for the videos. Due to ASL-LEX corpus updates across the time taken for data collection, six glosses in our dataset do not have corresponding linguistic information.</p>\n\n\n\n<p>The videos themselves contain a sample of the ASL community executing these signs to webcams in home environments. This type of crowdsourced collection has the benefit of not restricting geographic proximity (thus potentially expanding diversity), and capturing signers in their natural environments. Still, we recruited largely from our own Deaf community networks using snowball sampling. This type of convenience sampling can result in biases; for example, our sample of videos contains a high proportion of people who self-identified as female, compared to the population of ASL users at large.</p>\n\n\n\n<p><strong>What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features?}{In either case, please provide a description.</strong></p>\n\n\n\n<p>Each instance consists of a video file in .mp4 format. Each instance also has an associated gloss (or English transliteration), which was the target value for the signer. These glosses are consistent with a previous lexical database, ASL-LEX [2], and thus can be mapped onto standardized identifiers and phonological properties for the signs provided in this lexical database. Finally, each instance is also associated with an anonymous user identifier, identifying which of the 52 participants performed the sign.</p>\n\n\n\n<p><strong>Is there a label or target associated with each instance?</strong> If so, please provide a description.</p>\n\n\n\n<p>The label is the English gloss associated with each sign, as described above.</p>\n\n\n\n<p><strong>Is any information missing from individual instances?</strong> If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.</p>\n\n\n\n<p>All instances have complete information. However, we not that some users have blank metadata in their demographic information. This is intentional, as provision of this information was entirely voluntary, and some users did not provide some fields.</p>\n\n\n\n<p><strong>Are relationships between individual instances made explicit (e.g., users\u2019 movie ratings, social network links)?</strong> If so, please describe how these relationships are made explicit.</p>\n\n\n\n<p>Yes. Each instance is tagged with a user ID identifying which user performed the sign. This user ID can be further associated with a separate metadata file containing demographic information on each of the users, such as the self-identified gender of the signer. We do not analyze this demographic information in our manuscript, but provide it because it could be useful for studying fairness (and other research).</p>\n\n\n\n<p><strong>Are there recommended data splits (e.g., training, development/validation, testing)?</strong> If so, please provide a description of these splits, explaining the rationale behind them.</p>\n\n\n\n<p>Yes. Instances are labeled as either <code>train\" (training set),</code>val&#8221; (validation set), or &#8220;test&#8221; (test set), containing 40,154, 10,304, and 32,941 videos respectively. The data splits are stratified by user such that each user is unseen in the other data splits. These splits align with our dictionary retrieval task, because we expect users querying the dictionary to be unseen during training and model selection. Some participants contributed data over the entire vocabulary, while others only contributed data for a subset. To balance our test set metrics across the vocabulary, we assigned 11 participants who contributed 3,000 +/- 5 videos (i.e. the maximum number of videos participants would be compensated for) to the test dataset. The other 11 participants who contributed 3,000 \\textpm \\ 5 videos, in addition to 30 participants who contributed a smaller number of videos, were otherwise split between the train and val set. We tried to balance gender identities across splits, as seen in Figure 2. Other than these factors, participants were randomly assigned to splits.</p>\n\n\n\n<p><strong>Are there any errors, sources of noise, or redundancies in the dataset?}{If so, please provide a description.</strong></p>\n\n\n\n<p>We implemented some filters for blank videos, videos not containing people, and videos without signing as described above. However, these filters are basic, and may not have captured all videos with technical problems. Additionally, since these videos are self-recorded, not all users may perform the same sign for a gloss, since the same English gloss can sometimes refer to multiple signs (e.g. when there are regional variations of a sign, or when the English gloss is a homonym). We limited this issue by collecting data in an ASL-first process, where users watched a video of a sign prompt rather than reading an English gloss prompt, but in some instances, users may still not follow the seed signer (e.g. when their regional variation of a sign is not documented in the dictionary, or when the seed signer is using an outdated sign for signs that rapidly evolve). It is also possible that contributors may have made mistakes in signing, or submitted erroneous videos that our filters did not catch.</p>\n\n\n\n<p><strong>Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?</strong> If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.</p>\n\n\n\n<p>The dataset is self-contained, but the gloss labels can optionally be mapped to ASL-LEX, which provides detailed linguistic analysis of each sign in the vocabulary we used. The linguistic analysis download can be found at <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://asl-lex.org/download.html\" target=\"_blank\" rel=\"noreferrer noopener\">https://asl-lex.org/download.html<span class=\"sr-only\"> (opens in new tab)</span></a>, and information about funding and licenses for ASL-LEX can be found at <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://asl-lex.org/about.html\" target=\"_blank\" rel=\"noreferrer noopener\">https://asl-lex.org/about.html<span class=\"sr-only\"> (opens in new tab)</span></a>.</p>\n\n\n\n<p><strong>Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals non-public communications)?</strong> If so, please provide a description.</p>\n\n\n\n<p>While the videos contain recordings of people signing, all contributors consented to participate in this dataset and agreed to terms of use for our web platform. The consent process provided detailed information about the project&#8217;s purpose, and explained that the dataset would be released to the public for research purposes.</p>\n\n\n\n<p><strong>Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?</strong> If so, please describe why.</p>\n\n\n\n<p>Generally no. The videos reflect signs a viewer would be exposed to in everyday conversational ASL, taken from an established corpus of vocabulary. However, some of the vocabulary may refer to content that some find offensive (e.g. a person&#8217;s private parts). In addition, because this database is a &#8220;snapshot&#8221; of the language at time of curation, some signs may be outdated and refer to stereotypes (e.g. around identity) phased out as the language has evolved and continues to evolve.</p>\n\n\n\n<p>We also believe the chance of erroneous offensive content is extremely low. We recruited from trusted groups, manually vetted the first and last video submitted by each user on each date of submission to verify good faith effort, passed all videos through libraries to detect and blur appearance of third parties, and finally did a manual review of all videos. We conducted our review and cleaning iteratively, under close guidance from Microsoft&#8217;s Ethics and Compliance team. We did not identify any offensive content in any of our reviews. All video blurring and omissions was done out of an abundance of care for our dataset participants (e.g. to remove a third party or personal content). However, it is impossible to guarantee that users did not submit videos that some may find offensive.</p>\n\n\n\n<p><strong>Does the dataset relate to people?</strong> If not, you may skip the remaining questions in this section.</p>\n\n\n\n<p>Yes.</p>\n\n\n\n<p><strong>Does the dataset identify any subpopulations (e.g., by age, gender)?</strong> If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.</p>\n\n\n\n<p>No. We release general aggregated demographics as part of our paper publication, but do not release individual demographics, to help protect participant privacy. These aggregated demographics span gender, age, region, and years of ASL experience. Providing demographic data on the collection platform was fully voluntary (i.e. not required and not tied to compensation) and self-reported.</p>\n\n\n\n<p><strong>Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?</strong> If so, please describe how.</p>\n\n\n\n<p>Yes. The videos contain uncensored faces and are generally filmed in the users&#8217; home environments. We chose not to censor user faces because facial expressions are critical linguistic components of ASL. Users provided consent for dataset release, and were able to delete videos or opt out of the dataset any time prior to release.</p>\n\n\n\n<p><strong>Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?</strong> If so, please provide a description.</p>\n\n\n\n<p>Not directly, but some sensitive attributes about participants might be guessable from the videos (e.g. race, or relation to the Deaf community).</p>\n\n\n\n<p><strong>Any other comments?</strong></p>\n\n\n\n<p>None.</p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"collection-process\">Collection Process</h2>\n\n\n\n<p><strong>How was the data associated with each instance acquired?</strong> Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.</p>\n\n\n\n<p>Videos were self-recorded and contributed by participants through a crowdsourcing web platform. We built on a platform described in [1], with optimizations to support scale. Demographics could optionally be entered into the platform as part of a user profile. Please see the Supplementary Materials in our paper publication for a detailed description of the optimized design components and rationale.</p>\n\n\n\n<p><strong>What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)?</strong> How were these mechanisms or procedures validated?</p>\n\n\n\n<p>Users contributed videos through a web platform that accessed the user&#8217;s webcam to facilitate recording within the website itself. Contributors used their own hardware for recording (e.g., webcams). This setup is consistent with the type of setup that future dictionaries might have to demonstrate and look up a sign.</p>\n\n\n\n<p><strong>If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?</strong></p>\n\n\n\n<p>N/A</p>\n\n\n\n<p><strong>Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?</strong></p>\n\n\n\n<p>A team of researchers, engineers, and a designer were involved in the data collection. The team designed, built, and maintained the platform, and also managed recruitment, participant engagement, and compensation. The team was compensated through salary, stipend, or contract payment.</p>\n\n\n\n<p>Data contributors were also compensated monetarily. The seed signer was paid to record the seed sign videos. The rest of the data was crowdsourced. For every 300 signs recorded, these participants received a $30 Amazon gift card, for up to 3,000 signs.</p>\n\n\n\n<p><strong>Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)?</strong> If not, please describe the timeframe in which the data associated with the instances was created.</p>\n\n\n\n<p>Collection of the seed sign videos ran from April-May 2021. The collection of the community replications ran from July 2021 to April 2022.</p>\n\n\n\n<p><strong>Were any ethical review processes conducted (e.g., by an institutional review board)?</strong> If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.</p>\n\n\n\n<p>Yes. The data collection was reviewed by the two collaborating institutions&#8217; Institutional Review Boards (IRBs) &#8212; Microsoft (primary, IRB of record #418) and Boston University. The platform itself and the dataset release also underwent additional Ethics and Compliance reviews by Microsoft.</p>\n\n\n\n<p><strong>Does the dataset relate to people?</strong> If not, you may skip the remaining questions in this section.</p>\n\n\n\n<p>Yes.</p>\n\n\n\n<p><strong>Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?</strong></p>\n\n\n\n<p>Videos were self-contributed by individuals directly.</p>\n\n\n\n<p><strong>Were the individuals in question notified about the data collection?</strong> If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.</p>\n\n\n\n<p>Yes. When participants first visited our web platform, they engaged in a consent process, which provided detailed information about the procedures, benefits and risks, use of personal information, and other details about the project. In addition, the web platform provided an information page that explained the purpose of the project, a list of team members, and contact information. For the exact consent text, please visit <a href=\"https://www.microsoft.com/en-us/research/project/asl-citizen/consent-form/\">https://www.microsoft.com/en-us/research/project/asl-citizen/consent-form/</a>.</p>\n\n\n\n<p>In addition to the procedures described in the consent form, participants were prompted with instructions as they viewed prompt signs and recorded their own versions. Screenshots that include the task instructions are provided in Fig. 1 of [1].</p>\n\n\n\n<p><strong>Did the individuals in question consent to the collection and use of their data?</strong> If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.</p>\n\n\n\n<p>Yes. (See answer and links above.)</p>\n\n\n\n<p><strong>If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?</strong> If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).</p>\n\n\n\n<p>Yes. Users could re-record videos, delete their videos from the collection, as well as withdraw from the dataset any time before public release.</p>\n\n\n\n<p><strong>Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?</strong> If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.</p>\n\n\n\n<p>Yes, a Data Protection Impact Analysis (DPIA) has been conducted, including taking a detailed inventory of the data types collected and stored and retention policy, and was successfully reviewed by Microsoft.</p>\n\n\n\n<p><strong>Any other comments?</strong></p>\n\n\n\n<p>None.</p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"preprocessing-cleaning-labelling\">Preprocessing/cleaning/labelling</h2>\n\n\n\n<p><strong>Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?</strong> If so, please provide a description. If not, you may skip the remainder of the questions in this section.</p>\n\n\n\n<p>Yes. First, we removed empty videos automatically, by removing those under 150 KB in size or where YOLOv3 [4] did not detect a person (~50 videos). We manually reviewed the first and last videos recorded by each participant on each day and random samples throughout, checking for a list of sensitive content provided by our ethics and compliance board. Three types of personal content were identified for redaction: another person, certificates, and religious symbols. To protect third parties, we used YOLOv3 to detect if multiple people were present. For these videos and others with identified personal content, we blurred the background using MediaPipe holistic user segmentation. We blurred a subset of pixels for one user, since the personal content was reliably limited to a small area. We also removed one user&#8217;s videos, who recorded many videos without a sign in them, and videos of a written error message. Finally, we manually re-reviewed all videos.</p>\n\n\n\n<p>In our reviews, we did not identify any inappropriate content or bad-faith efforts. In total, we blurred the background of 268 videos where a second person was detected automatically, 293 additional videos with sensitive content, and 32 additional videos with a person missed by the automatic detection. We blurred a small fixed range of pixels for 2,933 videos, and omitted 513 videos where the blurring was insufficient or an error message (resulting from the data collection platform) showed.</p>\n\n\n\n<p><strong>Was the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?</strong> If so, please provide a link or other access point to the \u201craw\u201d data.</p>\n\n\n\n<p>No, not publicly.</p>\n\n\n\n<p><strong>Is the software used to preprocess/clean/label the instances available?</strong> If so, please provide a link or other access point.</p>\n\n\n\n<p>No, but these procedures are easily reproducible using public software.</p>\n\n\n\n<p><strong>Any other comments?</strong></p>\n\n\n\n<p>None.</p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"uses\">Uses</h2>\n\n\n\n<p><strong>Has the dataset been used for any tasks already?</strong> If so, please provide a description.</p>\n\n\n\n<p>Yes. We provide supervised classification baselines in the manuscript, and show how these classifiers can be used to solve the dictionary retrieval problem.</p>\n\n\n\n<p><strong>Is there a repository that links to any or all papers or systems that use the dataset?</strong> If so, please provide a link or other access point.</p>\n\n\n\n<p>Yes. The link is available on our project page at <a href=\"https://www.microsoft.com/en-us/research/project/asl-citizen/\" target=\"_blank\" rel=\"noreferrer noopener\">https://www.microsoft.com/en-us/research/project/asl-citizen/</a>.</p>\n\n\n\n<p><strong>What (other) tasks could the dataset be used for?</strong></p>\n\n\n\n<p>Many methods beyond supervised classification can be used to address the dictionary retrieval framing, including unsupervised learning, identification of linguistic features, domain adaptation, etc. To enable these approaches, we provide a larger test dataset. Besides our dictionary retrieval framing, this dataset could be used for a number of purposes both within sign language computing and outside, including pretraining for continuous sign language recognition, or motion tracking.</p>\n\n\n\n<p><strong>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</strong> For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?</p>\n\n\n\n<p>Our dataset collection centers a sociolinguistic minority and disability community (the Deaf community) that is already subject to misconceptions, stereotypes, and marginalization. Sign language is a critical cultural component of this community and must be handled respectfully. Some machine learning efforts on sign language proceed without recognition of these existing inequities and cultural practices, and promote harmful misconceptions (e.g. that sign languages are simple, or just signed versions of English), use offensive language or stereotypes (e.g. outdated terminology like &#8220;hearing impaired&#8221; or &#8220;deaf and dumb&#8221;), or simply exploit the language as a commodity or &#8220;toy problem&#8221; without engaging with the community. Some practices that we outline in our paper can avoid these harms: these include including Deaf collaborators and community input in the work, and ensuring that they are compensated; using a critical problem framing that centers useful and culturally respectful applications (e.g. our dictionary retrieval framing); and ensuring that Deaf scholars are cited and their perspectives, concerns, and priorities are integrated into the design of machine learning algorithms.</p>\n\n\n\n<p><strong>Are there tasks for which the dataset should not be used?</strong> If so, please provide a description.</p>\n\n\n\n<p>We recommend using this data with meaningful involvement from Deaf community members in leadership roles with decision-making authority at every step from conception to execution. As we describe in our linked paper, research and development of sign language technologies that involves Deaf community members increases the quality of the work, and can help to ensure technologies are relevant and wanted. Historically, projects developed without meaningful Deaf involvement have not been well received [3] and have damaged relationships between technologists and deaf communities.</p>\n\n\n\n<p>We ask that this dataset is used with an aim of making the world more equitable and just for deaf people, and with a commitment to &#8220;do no harm&#8221;. In that spirit, this dataset should not be used to develop technology that purports to replace sign language interpreters, fluent signing educators, and/or other hard-won accommodations for deaf people.</p>\n\n\n\n<p>This dataset was designed primarily for work on isolated sign recognition; signing in continuous sentences\u2014like what is needed for translating between ASL and English\u2014is very different. In particular, continuous sign recognition cannot be accomplished by identifying a sequence of signs from a standard dictionary (e.g. by matching to the signs in our dataset), due to grammatical and structural difference in continuous signing from sign modulation, co-articulation effects and contextual changes in the meaning of signs. At a minimum, this dataset would need to be used in conjunction with other datasets and/or domain knowledge about sign language in order to tackle continuous recognition or translation.</p>\n\n\n\n<p><strong>Any other comments?</strong></p>\n\n\n\n<p>None.</p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"distribution\">Distribution</h2>\n\n\n\n<p><strong>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</strong> If so, please provide a description.</p>\n\n\n\n<p>Yes. This dataset is released publicly, to help advance research on isolated sign language recognition.</p>\n\n\n\n<p><strong>How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?</strong> Does the dataset have a digital object identifier (DOI)?</p>\n\n\n\n<p>The dataset will be made publicly available for download through the Microsoft Download Center.</p>\n\n\n\n<p>To download via web interface, please visit: <a href=\"https://www.microsoft.com/en-us/download/details.aspx?id=105253\" target=\"_blank\" rel=\"noreferrer noopener\">https://www.microsoft.com/en-us/download/details.aspx?id=105253</a></p>\n\n\n\n<p>To download via command line, please execute: wget https://download.microsoft.com/download/b/8/8/b88c0bae-e6c1-43e1-8726-98cf5af36ca4/ASL_Citizen.zip</p>\n\n\n\n<p><strong>When will the dataset be distributed?</strong></p>\n\n\n\n<p>The dataset was released on 06/12/2023.</p>\n\n\n\n<p><strong>Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?</strong> If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.</p>\n\n\n\n<p>Yes, the dataset will be published under a license that permits use for research purposes. The license is provided at <a href=\"https://www.microsoft.com/en-us/research/project/asl-citizen/dataset-license/\" target=\"_blank\" rel=\"noreferrer noopener\">https://www.microsoft.com/en-us/research/project/asl-citizen/dataset-license/</a>.</p>\n\n\n\n<p><strong>Have any third parties imposed IP-based or other restrictions on the data associated with the instances?</strong> If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.</p>\n\n\n\n<p>No, there are no third-party restrictions on the data we release. However, the complimentary phonological evaluations of sign vocabulary in our dataset previously published by ASL-LEX are published under a CC BY-NC 4.0 license (see <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://asl-lex.org/download.html\" target=\"_blank\" rel=\"noreferrer noopener\">https://asl-lex.org/download.html<span class=\"sr-only\"> (opens in new tab)</span></a>).</p>\n\n\n\n<p><strong>Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?</strong> If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.</p>\n\n\n\n<p>No.</p>\n\n\n\n<p><strong>Any other comments?</strong></p>\n\n\n\n<p>None.</p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"maintenance\">Maintenance</h2>\n\n\n\n<p><strong>Who will be supporting/hosting/maintaining the dataset?</strong></p>\n\n\n\n<p>The dataset will be hosted on Microsoft Download Center.</p>\n\n\n\n<p><strong>How can the owner/curator/manager of the dataset be contacted (e.g., email address)?</strong></p>\n\n\n\n<p>Please contact <a href=\"mailto:ASL_Citizen@microsoft.com\">ASL_Citizen@microsoft.com</a> with any questions.</p>\n\n\n\n<p><strong>Is there an erratum?</strong> If so, please provide a link or other access point.</p>\n\n\n\n<p>A public-facing website is associated with the dataset (see <a href=\"https://www.microsoft.com/en-us/research/project/asl-citizen/\" target=\"_blank\" rel=\"noreferrer noopener\">https://www.microsoft.com/en-us/research/project/asl-citizen/</a>). We will link to erratum on this website if necessary.</p>\n\n\n\n<p><strong>Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?</strong> If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?</p>\n\n\n\n<p>If updates are neccessary, we will update the dataset. We will release our dataset with a version number, to distinguish it with any future updated versions.</p>\n\n\n\n<p><strong>If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)?</strong> If so, please describe these limits and explain how they will be enforced.</p>\n\n\n\n<p>The dataset will be left up indefinitely, to maximize utility to research. Participants were informed that their contributions might be released in a public dataset.</p>\n\n\n\n<p><strong>Will older versions of the dataset continue to be supported/hosted/maintained?</strong> If so, please describe how. If not, please describe how its obsolescence will be communicated to users.</p>\n\n\n\n<p>All versions of the dataset will be released with a version number on Microsoft Download Center to enable differentiation.</p>\n\n\n\n<p><strong>If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?</strong> If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description.</p>\n\n\n\n<p>We do not have a mechanism for others to contribute to our dataset directly. However, others could create comparable datasets by recording versions of the same signs (from ASL-LEX). Such a dataset could easily be combined with ours by indexing on the signs&#8217; unique identifiers.</p>\n\n\n\n<p><strong>Any other comments?</strong></p>\n\n\n\n<p>None.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"503\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/06/hist-1024x503.png\" alt=\"Histogram of video lengths. X-axis: lengths, Y-axis: video counts. The max is reached around 2.3,2.4, with about 3,500 videos.\" class=\"wp-image-949374\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/06/hist-1024x503.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/hist-300x147.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/hist-768x377.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/hist-1536x754.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/hist-2048x1006.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/hist-240x118.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\">Figure 1: Histogram of video lengths in ASL Citizen dataset.</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"613\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/06/vid_dist-1024x613.png\" alt=\"Histogram of Female (blue) and Male (orange) counts. X-axis: Train, Validation, Test. Y-axis: Video Count. Video count for Female is higher across all three settings. Training has the highest counts overall, then Test, then Validation.\" class=\"wp-image-949380\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/06/vid_dist-1024x613.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/vid_dist-300x180.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/vid_dist-768x460.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/vid_dist-1536x920.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/vid_dist-2048x1226.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/vid_dist-240x144.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\">Figure 2: Distribution of videos across different data splits.</figcaption></figure>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"references\">References</h2>\n\n\n\n<p>[1] Danielle Bragg, Abraham Glasser, Fyodor Minakov, Naomi Caselli, and William Thies. Exploring Collection of Sign Language Videos through Crowdsourcing. <em>Proceedings of the ACM on Human-Computer Interaction</em>&nbsp;6.CSCW2 (2022): 1-24.</p>\n\n\n\n<p>[2] Naomi K Caselli, Zed Sevcikova Sehyr, Ariel M Cohen-Goldberg, and Karen Emmorey. ASL-LEX: A lexical database of American Sign Language. <em>Behavior research methods</em>&nbsp;49 (2017): 784-801</p>\n\n\n\n<p>[3] Michael Erard. Why sign-language gloves don\u2019t help deaf people. <em>The Atlantic</em>&nbsp;9 (2017)</p>\n\n\n\n<p>[4] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. <em>arXiv preprint arXiv:1804.02767</em>&nbsp;(2018).</p>\n\n\n\n\n\n<h2 class=\"wp-block-heading\" id=\"microsoft-research-project-participation-consent-form\">Microsoft Research Project Participation Consent Form</h2>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"introduction\">INTRODUCTION</h3>\n\n\n\n<p>Thank you for deciding to volunteer in a Microsoft Corporation research project.&nbsp; You have no obligation to participate and you may decide to terminate your participation at any time.&nbsp; You also understand that the researcher has the right to withdraw you from participation in the project at any time. Below is a description of the research project, and your consent to participate.&nbsp; Read this information carefully. If you agree to participate, sign in the space provided.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"title-of-research-project\">TITLE OF RESEARCH PROJECT</h3>\n\n\n\n<p>ASL Dataset Community</p>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"principal-investigator\">Principal Investigator</h4>\n\n\n\n<p>Danielle Bragg</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"purpose\">PURPOSE</h3>\n\n\n\n<p>The purpose of this project is to collect sign language videos from volunteer contributors to advance sign language recognition, while fostering American Sign Language (ASL) community online. The website falls under the category of &#8220;citizen science&#8221;, where people contribute for the purpose of advancing science or research. Contributors will be able to do three things: 1) record videos of themselves executing specific signs, 2) validate that other contributors executed signs correctly, and 3) explore the communal dataset.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"procedures\">PROCEDURES</h3>\n\n\n\n<p>During this project, the following will happen:</p>\n\n\n\n<ul>\n<li>You will create a user profile, including a username, email address, and optional picture and demographics.</li>\n\n\n\n<li>You will then be able to do three main things on the website: 1) record videos of yourself signing, 2) validate that other contributors executed signs correctly, and 3) explore the communal dataset.</li>\n\n\n\n<li>For every 300 signs you record, we will send you a $30 Amazon gift card, for up to 3000 signs. The gift card will be sent to the email address associated with your profile.</li>\n</ul>\n\n\n\n<p>Microsoft may document and collect information about your participation by storing your profile information, the videos you submit, your ratings of other contributors\u2019 videos, and any other interactions with the site.</p>\n\n\n\n<p>Approximately 60 participants will be involved in this study.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"personal-information-and-confidentiality\">PERSONAL INFORMATION AND CONFIDENTIALITY</h3>\n\n\n\n<p>Microsoft Research is ultimately responsible for determining the purposes and uses of your personal information.</p>\n\n\n\n<ul>\n<li><strong>Personal information we collect.&nbsp;&nbsp; </strong>During the project we may collect personal information about you such as image, likeness, email, age, gender, and ASL experience.</li>\n</ul>\n\n\n\n<ul>\n<li><strong>How we use personal information.</strong>&nbsp; The personal information and other data collected during this project will be used primarily to perform research for purposes described in the introduction above. &nbsp;&nbsp;Such information and data, or the results of the research may eventually be used to develop and improve our commercial products, services or technologies.</li>\n</ul>\n\n\n\n<ul>\n<li><strong>Right of Publicity</strong>.&nbsp; By submitting video(s) of yourself, you confirm that you are the depicted person in the video(s) you submit and you grant Microsoft an unrestricted, perpetual, worldwide, royalty-free, irrevocable license, with rights to assign and sublicense, to use your image and likeness for the research project above and in any related services, on a worldwide basis.</li>\n\n\n\n<li><strong>How we store and share your personal information.&nbsp; </strong>Your personal data will be stored for a period of up to 5 years from your last login. &nbsp;This project is a collaboration with Boston University, who will have access to collected data. In addition, we may release a dataset that includes videos and other demographics publicly to help advance research.</li>\n</ul>\n\n\n\n<ul>\n<li><strong>How you can access and control your personal information.</strong>&nbsp; If you wish to review or copy any personal information you provided during the study, log in to your account to view, edit or delete data from the live site. If you have any additional questions, please email the research team at: aslgames@microsoft.com.&nbsp; Please note that we will not be able to delete data that has already been shared publicly in a research dataset release. We will respond to questions or concerns within 30 days.</li>\n</ul>\n\n\n\n<p>For additional information on how Microsoft handles your personal information, please see the <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://go.microsoft.com/fwlink/?LinkId=521839\">Microsoft Privacy Statement<span class=\"sr-only\"> (opens in new tab)</span></a>. &nbsp;</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"research-results-feedback\">RESEARCH RESULTS & FEEDBACK</h3>\n\n\n\n<p>Microsoft will own all of the research data and analysis and other results (collectively \u201cResearch Results\u201d) generated from the information you provide and your participation in the research project. You may also provide suggestions, comments or other feedback (\u201cFeedback\u201d) to Microsoft with respect to the research project. Feedback is entirely voluntary, and Microsoft shall be free to use, disclose, reproduce, license, or otherwise distribute, and leverage the Feedback and Research Results.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"microsoft-and-confidentiality\">&nbsp;MICROSOFT AND CONFIDENTIALITY</h3>\n\n\n\n<p>The research project and information you learn by participating in the project is confidential to Microsoft.&nbsp; Sharing this confidential information with people other than those we\u2019ve identified above could negatively affect the scientific integrity of the research study and could even make it more difficult for Microsoft to develop new products based on the information obtained in this study. It is therefore important that you do not talk about the project outside of the study team (unless you are legally required to do so by a court or other government order).&nbsp; This does not apply if the information is general public knowledge or if you have a legal right to share the information.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"benefits-and-risks\">BENEFITS AND RISKS</h3>\n\n\n\n<p><strong>Benefits:&nbsp; &nbsp;&nbsp;</strong>The research team expects to collect videos of diverse signers from this project which we hope will improve the accuracy of sign language recognition systems for diverse signers, for example enabling the creation of drive-through. You will receive any public benefit that may come of these Research Results being shared with the greater scientific community.</p>\n\n\n\n<p><strong>Risks:</strong>&nbsp;&nbsp;&nbsp;&nbsp; During participation, you may experience discomfort at contributing videos of yourself. Because you will be able to view videos recorded by other contributors, it is also possible that you will view inappropriate or offensive content. To alleviate this risk, the website allows participants to flag inappropriate content, which will be reviewed by a moderator.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"future-use-of-your-identifiable-information\">FUTURE USE OF YOUR IDENTIFIABLE INFORMATION</h3>\n\n\n\n<p>Identifiers might be removed from your identifiable private information, and after such removal, the information could be used for future research studies or distributed to another investigator for future research studies without your (or your legally authorized representative\u2019s)&nbsp; additional informed consent,</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"payment-for-participation\">PAYMENT FOR PARTICIPATION</h3>\n\n\n\n<p>For every 300 signs you record, we will send you a $30 Amazon gift card, for up to 3000 signs. The gift card will be sent to the email address associated with your profile.</p>\n\n\n\n<p>Your data may be used to make new products, tests or findings.&nbsp; These may have value and may be developed and owned by Microsoft and/or others.&nbsp; If this happens, there are no plans to pay you.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"participation\">PARTICIPATION</h3>\n\n\n\n<p>Taking part in research is always a choice. If you decide to be in the study, you can change your mind at any time without affecting any rights including payment to which you would otherwise be entitled. If you decide to withdraw, you should contact the person in charge of this study, and also inform that person if you would like your personal information removed as well.</p>\n\n\n\n<p>Microsoft or the person in charge of this study may discontinue the study or your individual participation in the study at any time without your consent for reasons including:</p>\n\n\n\n<ul>\n<li>your failure to follow directions</li>\n\n\n\n<li>it is discovered that you do not meet study requirements</li>\n\n\n\n<li>it is in your best interest medically</li>\n\n\n\n<li>the study is canceled</li>\n\n\n\n<li>administrative reasons</li>\n</ul>\n\n\n\n<p>If you leave the study, the study staff will still be able to use your information that they have already collected, however, you have the right to ask for it to be removed when you leave.</p>\n\n\n\n<p>Significant new findings that develop during the course of this study that might impact your willingness to be in this study will be given to you.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"contact-information\">CONTACT INFORMATION</h3>\n\n\n\n<p>Should you have any questions concerning this project, please contact the research team at aslgames@microsoft.com.&nbsp;</p>\n\n\n\n<p>Should you have any questions about your rights as a research subject, please contact Microsoft Research Ethics Program Feedback at MSRStudyfeedback@microsoft.com.</p>\n\n\n\n<p>CONSENT</p>\n\n\n\n<p>By clicking CONTINUE, you confirm that the study was explained to you, you had a chance to ask questions before beginning the study, and all your questions were answered satisfactorily. At any time, you may ask other questions. By clicking CONTINUE, you voluntarily consent to participate, and you do not give up any legal rights you have as a study participant.</p>\n\n\n\n<p>Please confirm your consent by clicking CONTINUE. If you wish, you may now save a copy of this consent form for future reference. On behalf of Microsoft, we thank you for your contribution and look forward to your research session.</p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>A Community-sourced Dataset for Advancing Isolated Sign Language Recognition Signed languages are the primary languages of about 70 million D/deaf people worldwide (opens in new tab). Despite their importance, existing information and communication technologies are primarily designed for written or spoken language.&nbsp;Though automated solutions&nbsp;might help address such&nbsp;accessibility gaps, the state of sign language modeling is [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 965577,
        "date": "2023-05-22T22:38:00",
        "slug": "gaming-interaction",
        "title": "Emergent Interaction Agent",
        "link": "https://www.microsoft.com/en-us/research/project/gaming-interaction/",
        "content": "\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\"><section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"mindagent-emerging-gaming-interaction\">MindAgent\uff1aEmerging Gaming Interaction</h1>\n\n\n\n<p>We collaborate with X-Box and Mesh team, explored a new gaming infrastructure and designed the dynamic real-time system for human-player and NPCs with GPT-X in the multi-agent platform.</p>\n\n\n\n<p>GitHub: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://mindagent.github.io/\">MindAgent<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<p>ArXiv: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2309.09971\">https://arxiv.org/abs/2309.09971<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<p>Demo: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://microsoft-my.sharepoint.com/:v:/p/qihua/Ed3G5NLrlRhNsCr9sneAQycB_CANxFMNlajJV4TbhKZuzw?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJTdHJlYW1XZWJBcHAiLCJyZWZlcnJhbFZpZXciOiJTaGFyZURpYWxvZyIsInJlZmVycmFsQXBwUGxhdGZvcm0iOiJXZWIiLCJyZWZlcnJhbE1vZGUiOiJ2aWV3In19&e=XqVR4p\">MindAgent.mp4<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<p> </p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n</div>\n\n\n\n\n\n<p>Gaming Interaction Infrastructure:</p>\n\n\n\n<p class=\"has-text-align-center\"><img loading=\"lazy\" decoding=\"async\" width=\"2890\" height=\"1944\" class=\"wp-image-973128\" style=\"width: 800px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MindAgent_mindagent.png\" alt=\"MindAgent\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MindAgent_mindagent.png 2890w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MindAgent_mindagent-300x202.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MindAgent_mindagent-1024x689.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MindAgent_mindagent-768x517.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MindAgent_mindagent-1536x1033.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MindAgent_mindagent-2048x1378.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/MindAgent_mindagent-240x161.png 240w\" sizes=\"(max-width: 2890px) 100vw, 2890px\" /></p>\n\n\n\n\n\n<p>We are very excited to share the good news. Our project \u201c<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://mindagent.github.io/\" target=\"_blank\" rel=\"noreferrer noopener\">MindAgent: Emergent Gaming Interaction<span class=\"sr-only\"> (opens in new tab)</span></a>\u201d is public recently. We seek to develop a unified interaction infrastructure and architecture that can jointly: understand large language corpora, visual (image and video) inputs, as well as provide meaningful action-based outputs.&nbsp; Our model on a broad range of gaming video tasks and show agent action stream efficacy across a range of tasks including interactive agent, visual and natural language understanding. In this work, we propose a novel infrastructure &#8211; <strong>MindAgent</strong> &#8211; to evaluate planning and coordination emergent capabilities for gaming interaction. In particular, our infrastructure leverages existing gaming framework, to i) require understanding of the coordinator for a multi-agent system, ii) collaborate with human players via un-finetuned proper instructions, and iii) establish an in-context learning on few-shot prompt with feedback. Furthermore, we introduce <strong>CuisineWorld</strong>, a new gaming scenario and related benchmark that dispatch a multi-agent collaboration efficiency and supervise multiple agents playing the game simultaneously. We conduct comprehensive evaluations with new auto-metric <strong>CoS</strong> for calculating the collaboration efficiency. Finally, our infrastructure can be deployed into real-world gaming scenarios in a customized VR version of CuisineWorld and adapted in existing broader Minecraft gaming domain. By creating a powerful and general-purpose foundation model with visual, language, and action capabilities, we can have great impact across many industries, both within Microsoft and external.</p>\n\n\n\n<p><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.youtube.com/watch?v=LBD3dmlinG4\">minecraft vr demo &#8211; YouTube<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>We collaborate with X-Box and Mesh team, explored a new gaming infrastructure and designed the dynamic real-time system for human-player and NPCs with GPT-X in the multi-agent platform. GitHub: MindAgent (opens in new tab) ArXiv: https://arxiv.org/abs/2309.09971 (opens in new tab) Demo: MindAgent.mp4 (opens in new tab) Gaming Interaction Infrastructure: We are very excited to share [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 778522,
        "date": "2023-05-16T14:26:13",
        "slug": "ai-for-health",
        "title": "AI for Health",
        "link": "https://www.microsoft.com/en-us/research/project/ai-for-health/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"3840\" height=\"1440\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2022/12/coverAIforhealth.png\" class=\"attachment-full size-full\" alt=\"decorative page cover\" style=\"object-position: 23% 50%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2022/12/coverAIforhealth.png 3840w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/coverAIforhealth-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/coverAIforhealth-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/coverAIforhealth-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/coverAIforhealth-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/coverAIforhealth-2048x768.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/coverAIforhealth-1920x720.png 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/coverAIforhealth-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2022/12/coverAIforhealth-240x90.png 240w\" sizes=\"(max-width: 3840px) 100vw, 3840px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 align-self-center\">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"ai-for-health-research\">AI for Health</h1>\n\n\n\n<p>Research and collaborations contributing to the Microsoft AI for Health program</p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/group/ai-for-good-research-lab/\">< AI For Good Lab</a></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>AI for Health is a philanthropic program launched by Microsoft, which aims to support nonprofits, researchers, and organizations working on global health challenges. The program provides access to artificial intelligence (AI) technology and expertise in three main areas:</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-131 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p><strong>Population health</strong></p>\n\n\n\n<p>By integrating data from various health sectors and utilizing AI and visualization techniques, the program aims to offer decision-makers valuable insights into the factors driving diseases.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p><strong>Imaging analytics</strong></p>\n\n\n\n<p>AI is applied to image-based data to improve clinical decision-making processes, extend the reach of imaging tools, and enhance their precision and accuracy.</p>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p><strong>Genomics & proteomics</strong></p>\n\n\n\n<p>AI is utilized to analyze genomic and proteomic data. It can help predict disease risks and identify specific areas in proteins that require further investigation for potential disease intervention.</p>\n</div>\n</div>\n\n\n\n<p>Since its launch in January 2020, the AI for Health Program has partnered with more than 200 grantees, supporting projects that accelerate medical research, enhance research capabilities, increase global health insights, and address health inequities.</p>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h2 class=\"wp-block-heading\" id=\"empower-communities-to-take-anticipatory-action-with-early-warnings\">Protecting public health</h2>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-132 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-3-Health-equity-1024x576.jpg\" alt=\"AI4Good - Expand Opportunity | health equity map of the United States\" class=\"wp-image-1017789\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-3-Health-equity-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-3-Health-equity-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-3-Health-equity-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-3-Health-equity-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-3-Health-equity-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-3-Health-equity-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-3-Health-equity-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-3-Health-equity-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-3-Health-equity-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-3-Health-equity.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Understanding health equity</h4>\n\n\n\n<p>The AI for Health dashboard provides an opportunity for researchers and other interested parties to easily explore relationships between county-level measures of health status, health services utilization and quality, and social determinants of health.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-52 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://aka.ms/healthequity\" target=\"_blank\" rel=\"noreferrer noopener\">Visualization</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/publication/rural-urban-disparities-in-health-outcomes-clinical-care-health-behaviors-and-social-determinants-of-health-and-an-action-oriented-dynamic-tool-for-visualizing-them/\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/publication/an-observational-sequential-analysis-of-the-relationship-between-local-economic-distress-and-inequities-in-health-outcomes-clinical-care-health-behaviors-and-social-determinants-of-health/\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/05/AI4Health_New-York-City_skyline-1024x576.jpg\" alt=\"AI4Good | AI for Health | New York City skyline\" class=\"wp-image-1021512\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2023/05/AI4Health_New-York-City_skyline-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/05/AI4Health_New-York-City_skyline-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/05/AI4Health_New-York-City_skyline-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/05/AI4Health_New-York-City_skyline-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/05/AI4Health_New-York-City_skyline-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/05/AI4Health_New-York-City_skyline-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/05/AI4Health_New-York-City_skyline-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/05/AI4Health_New-York-City_skyline-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/05/AI4Health_New-York-City_skyline-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2023/05/AI4Health_New-York-City_skyline.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">AI4HealthyCities</h4>\n\n\n\n<p>AI4HealthyCities is an initiative by the Novartis Foundation in collaboration with Microsoft AI for Health and local partners, bringing together existent but disconnected sets of data within a city and using advanced analytics and AI to uncover cardiovascular risk factors in its population.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-53 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.novartisfoundation.org/transforming-population-health/ai4healthycities\" target=\"_blank\" rel=\"noreferrer noopener\">Visit the site</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-5-Chatbot-1024x576.jpg\" alt=\"AI4Good - Expand Opportunity | photo of someone holding a smartphone and viewing the QuitBot app\" class=\"wp-image-1017795\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-5-Chatbot-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-5-Chatbot-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-5-Chatbot-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-5-Chatbot-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-5-Chatbot-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-5-Chatbot-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-5-Chatbot-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-5-Chatbot-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-5-Chatbot-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-5-Chatbot.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Chatbot app aims to combat smoking addiction</h4>\n\n\n\n<p>Over 1.3 billion individuals are regular smokers, causing 7.7 million annual deaths, with 1.3 million non-smokers affected by second-hand smoke exposure. To combat this epidemic, the AI for Good Lab worked together with Fred Hutch to develop a chatbot app for smoking cessation.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-54 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/video/the-prompt-with-trevor-noah-episode-4-how-can-large-language-models-help-people-combat-addiction/\" target=\"_blank\" rel=\"noreferrer noopener\">Video</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.linkedin.com/pulse/quitbot-using-ai-help-fight-addiction-juan-m-lavista-ferres-rdlpc/\" target=\"_blank\" rel=\"noreferrer noopener\">Article</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://quitbot.net\" target=\"_blank\" rel=\"noreferrer noopener\">Visit the site</a></div>\n</div>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<blockquote class=\"wp-block-quote is-style-spectrum--blue-green is-layout-flow wp-block-quote-is-layout-flow\">\n<p>&#8220;It was very hard to get Quitbot to understand what people meant when they asked a question, we\u2019ve come a long way&nbsp;and learned a lot about how to use natural language processing to be able to do it.\u201d</p>\n<cite>\u2013 Dr. Jonathan Bricker, Professor, Cancer Prevention Program, Public Health Sciences Division, Fred Hutch Cancer Center</cite></blockquote>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper\">\n\t\t\t<h2 class=\"wp-block-heading\" id=\"empower-communities-to-take-anticipatory-action-with-early-warnings\">Improving cancer diagnosis with computer vision</h2>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-133 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-6-Pancreatic-cancer-1024x576.jpg\" alt=\"AI4Good - Expand Opportunity | photo of a person reviewing four scans of a pancreas and surrounding areas\" class=\"wp-image-1017798\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-6-Pancreatic-cancer-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-6-Pancreatic-cancer-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-6-Pancreatic-cancer-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-6-Pancreatic-cancer-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-6-Pancreatic-cancer-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-6-Pancreatic-cancer-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-6-Pancreatic-cancer-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-6-Pancreatic-cancer-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-6-Pancreatic-cancer-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-6-Pancreatic-cancer.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Critical early detection of pancreatic cancer with AI</h4>\n\n\n\n<p>85% of people with pancreatic cancer are diagnosed too late to receive life-saving treatment. Early diagnosis is crucial, yet in ~ 40% of CT scans, tumors are not detected. Working together with Fred Hutch we are training AI to identify tumors often missed by the human eye, potentially saving up to 30,000 lives annually.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-55 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/video/the-prompt-with-trevor-noah-episode-5-how-ai-can-help-clinicians-improve-pancreatic-cancer-detection/\" target=\"_blank\" rel=\"noreferrer noopener\">Video</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-7-Breast-cancer-1024x576.jpg\" alt=\"AI4Good - Expand Opportunity | close up photo of six views of a breast MRI scan\" class=\"wp-image-1017801\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-7-Breast-cancer-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-7-Breast-cancer-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-7-Breast-cancer-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-7-Breast-cancer-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-7-Breast-cancer-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-7-Breast-cancer-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-7-Breast-cancer-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-7-Breast-cancer-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-7-Breast-cancer-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-7-Breast-cancer.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">AI can help radiologists better detect breast cancer</h4>\n\n\n\n<p>Breast cancer is the second leading cause of cancer related death in women, early detection is critical for improving treatment outcomes. Learn how AI is helping professionals quickly learn from thousands of patient images to improve the way we detect, diagnose, and rule out false positives.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-56 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/video/the-prompt-with-trevor-noah-episode-3-how-can-ai-help-radiologists-better-detect-breast-cancer/\" target=\"_blank\" rel=\"noreferrer noopener\">Video</a></div>\n\n\n\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.linkedin.com/pulse/transforming-breast-cancer-detection-ai-juan-m-lavista-ferres-c2y2c%3FtrackingId=szV8LrQmr45P8ZW7nsQqiQ%253D%253D/?trackingId=szV8LrQmr45P8ZW7nsQqiQ%3D%3D\" target=\"_blank\" rel=\"noreferrer noopener\">Article</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-8-Prostate-cancer-1024x576.jpg\" alt=\"AI4Good - Expand Opportunity | photo of a medical tech standing next to a patient going into an MRI machine\" class=\"wp-image-1017804\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-8-Prostate-cancer-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-8-Prostate-cancer-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-8-Prostate-cancer-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-8-Prostate-cancer-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-8-Prostate-cancer-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-8-Prostate-cancer-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-8-Prostate-cancer-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-8-Prostate-cancer-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-8-Prostate-cancer-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/I-8-Prostate-cancer.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<div style=\"height:10px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n\n<h4 class=\"wp-block-heading\" id=\"population-mapping-protects-vulnerable-communities\">Revolutionizing precision of prostate cancer diagnosis</h4>\n\n\n\n<p>Prostate cancer, the second most diagnosed cancer in men, claims over 350,000 lives yearly. Automated lesion segmentation in radiological PET CT scans promises personalized treatment and enhanced monitoring. While AI won&#8217;t replace radiologists, it enhances precision and efficiency.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-container-core-buttons-is-layout-57 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-cta\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/publication/automatic-segmentation-of-prostate-cancer-metastases-in-psma-pet-ct-images-using-deep-neural-networks-with-weighted-batch-wise-dice-loss/\" target=\"_blank\" rel=\"noreferrer noopener\">Publication</a></div>\n</div>\n</div>\n</div>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div style=\"padding-bottom:0; padding-top:0\" class=\"wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section\">\n\t\n\t<div class=\"container\">\n\t\t<div class=\"wp-block-msr-immersive-section__wrapper col-lg-11 col-xl-9 px-0 m-auto\">\n\t\t\t<blockquote class=\"wp-block-quote is-style-spectrum--blue-green is-layout-flow wp-block-quote-is-layout-flow\">\n<p>\u201cThe truth is companies outside of medicine can really have the biggest impact. If medicine wants to move forward, they need to work closely with the best computer scientists because we understand the problem and they know how to find the solutions.\u201d</p>\n<cite>\u2013 Dr. Elliot K. Fishman, Professor of Radiology and Radiological Science</cite></blockquote>\t\t</div>\n\t</div>\n\n\t</div>\n\n\n\n<div class=\"wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  has-media-on-the-right is-stacked-on-mobile\" data-bi-an=\"media-text\"><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<h3 class=\"wp-block-heading\" id=\"the-microsoft-ai-for-health-program-solving-the-world-s-biggest-health-issues-one-life-at-a-time\">The Microsoft AI for Health program: Solving the world\u2019s biggest health issues, one life at a time</h3>\n\n\n\n<p>William B. Weeks, Director of AI for Health in the AI for Good Research Lab, shares insights on the Microsoft AI for Health program.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/group/ai-for-good-research-lab/articles/microsofts-ai-for-health-program-solving-the-worlds-biggest-health-issues-one-life-at-a-time\" target=\"_blank\" rel=\"noreferrer noopener\" data-bi-cn=\"The Microsoft AI for Health program: Solving the world\u2019s biggest health issues, one life at a time\">Read the blog</a></div>\n</div>\n</div><figure class=\"wp-block-media-text__media\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/AI-for-Health_feature_1400x788-1024x576.jpg\" alt=\"AI for Health - four people in lab coats conferring at a large monitor\" class=\"wp-image-940083 size-full\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/05/AI-for-Health_feature_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/AI-for-Health_feature_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/AI-for-Health_feature_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/AI-for-Health_feature_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/AI-for-Health_feature_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/AI-for-Health_feature_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/AI-for-Health_feature_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/AI-for-Health_feature_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/AI-for-Health_feature_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/AI-for-Health_feature_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/AI-for-Health_feature_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>AI for Health is a philanthropic program launched by Microsoft, which aims to support nonprofits, researchers, and organizations working on global health challenges. The program provides access to artificial intelligence (AI) technology and expertise in three main areas: population health, imaging analytics, genomics & proteomics.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 938424,
        "date": "2023-05-02T11:08:50",
        "slug": "copilot-for-pull-requests",
        "title": "Copilot for Pull Requests",
        "link": "https://www.microsoft.com/en-us/research/project/copilot-for-pull-requests/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 id=\"copilot-for-pull-requests\" class=\"wp-block-heading\">Copilot for Pull Requests</h1>\n\n\n\n<p>Pull requests are a central part of the GitHub user experience. Copilot for PRs brings the power of Copilot to the PR experience, to help you write better PR descriptions, and to help your team review and merge PRs faster.</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>This is a project at <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://githubnext.com/\">GitHub Next<span class=\"sr-only\"> (opens in new tab)</span></a>, see <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://githubnext.com/projects/copilot-for-pull-requests/\">GitHub Next | Copilot for Pull Requests<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Pull requests are a central part of the GitHub user experience. Copilot for PRs brings the power of Copilot to the PR experience, to help you write better PR descriptions, and to help your team review and merge PRs faster. This is a project at GitHub Next (opens in new tab), see GitHub Next | [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 937905,
        "date": "2023-05-01T15:33:11",
        "slug": "transcendence",
        "title": "Transcendence",
        "link": "https://www.microsoft.com/en-us/research/project/transcendence/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-plum card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 class=\"wp-block-heading\" id=\"transcendence\">Transcendence </h1>\n\n\n\n<p>Reinventing how we work together</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>The Transcendence Project at Microsoft Research is reimagining interaction, productivity, and collaboration, harnessing the power of AI to transcend space, time, and modality, and redefine how we work together in the future.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Reinventing how we work together The Transcendence Project at Microsoft Research is reimagining interaction, productivity, and collaboration, harnessing the power of AI to transcend space, time, and modality, and redefine how we work together in the future. Opens in a new tab</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 931254,
        "date": "2023-04-26T11:06:46",
        "slug": "mixed-reality",
        "title": "Infinite Mixed Reality with Emergent Abilities",
        "link": "https://www.microsoft.com/en-us/research/project/mixed-reality/",
        "content": "\n<p></p>\n\n\n\n<div class=\"wp-block-group is-layout-constrained wp-block-group-is-layout-constrained\">\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-134 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:100%\"><section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading is-style-l\" id=\"augmented-interaction-with-emergent-abilities\">Augmented Interaction with Emergent Abilities</h2>\n\n\n\n<p>&#8212;-Gaming/Mix-Reality/Robots</p>\n\n\n\n<p>Knowledge-memory augmented interaction for cross-modality and reality-agnostic integration with Emergence Mechanism.</p>\n\n\n\n<p>Selected as the project in&nbsp;<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" style=\"font-size: calc(14.9px + 0.22vw)\" href=\"https://hackbox.microsoft.com/hackathons/FixHackLearn-Feb2023/project/21201\">HackBox 2023<span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n<p>Shipped to Office Teams</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n</div>\n</div>\n</div>\n\n\n\n<h3 class=\"wp-block-heading has-text-align-center\" id=\"ark-augmented-reality-with-knowledge-emergent-infrastructure\"><a href=\"https://www.microsoft.com/en-us/research/publication/ark/\">ArK: Augmented Reality with Knowledge Emergent Infrastructure</a></h3>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"615\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/12/qiuyuan_crossreality-1024x615.png\" alt=\"ArK: Generative Agent in Mixed-Reality\" class=\"wp-image-993375\" style=\"width:896px;height:auto\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/12/qiuyuan_crossreality-1024x615.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/qiuyuan_crossreality-300x180.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/qiuyuan_crossreality-768x461.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/qiuyuan_crossreality-1536x922.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/qiuyuan_crossreality-2048x1229.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/qiuyuan_crossreality-240x144.png 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>&#8212;-Gaming/Mix-Reality/Robots Knowledge-memory augmented interaction for cross-modality and reality-agnostic integration with Emergence Mechanism. Selected as the project in&nbsp;HackBox 2023 (opens in new tab) Shipped to Office Teams Opens in a new tab</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 926853,
        "date": "2023-04-03T10:34:50",
        "slug": "multimodal-physiological-and-cognitive-sensing",
        "title": "Multimodal physiological and cognitive sensing",
        "link": "https://www.microsoft.com/en-us/research/project/multimodal-physiological-and-cognitive-sensing/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-header_1920x720.png\" class=\"attachment-full size-full\" alt=\"multimodal brain computer interface header - illustration of a brain as a lightbulb\" style=\"object-position: 80% 46%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-header_1920x720.png 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-header_1920x720-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-header_1920x720-1024x384.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-header_1920x720-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-header_1920x720-1536x576.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-header_1920x720-1600x600.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-header_1920x720-240x90.png 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 id=\"multimodal-physiological-and-cognitive-sensing-for-improved-digital-ergonomics\">Multimodal physiological and cognitive sensing for improved digital ergonomics</h2>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>The way we work has fundamentally changed over the past several years, enabling employees to have more flexible working environments. At the same time, the rising number of meetings and hours spent working has led to digital exhaustion. In <a href=\"https://www.microsoft.com/en-us/worklab/work-trend-index/hybrid-work\" target=\"_blank\" rel=\"noreferrer noopener\">Microsoft\u2019s 2021 Work Trend Index Report</a>, a global external study revealed that 54% of respondents reported feeling overworked and 39% said they were outright exhausted. Employees will frequently endure the daily meeting marathon, with many finding themselves needing to multitask as other work-associated tasks arise.</p>\n\n\n\n<p>Our goal is to make work more sustainable for everyone by leveraging technology to increase focus, productivity, and wellbeing. We are investigating AI enabled applications&nbsp;that utilize multiple modalities for passive biosensing, working in conjunction with the Microsoft ecosystem to provide a personalized health and user experience. We are developing generalized machine learning models to detect emotional and physical state in real-time, based on multimodal sensing data. With this information, we can better adapt our digital workspaces to reduce fatigue and encourage self-care and awareness.</p>\n\n\n\n<p>As Microsoft continues to design innovations that empower people to do more, we are working to help ensure we are developing products that improve the wellbeing of everyone. We are engaged with product teams to understand the impact that new features have on fatigue and look for ways to improve the way we work in this increasingly digital environment.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"384\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-state_header_1920x720-1024x384.jpg\" alt=\"multimodal brain computer interface illustration\" class=\"wp-image-927102\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-state_header_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-state_header_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-state_header_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-state_header_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-state_header_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-state_header_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/multimodal-BCI-state_header_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>We are investigating AI enabled applications\u00a0that utilize multiple modalities for passive biosensing, working in conjunction with the Microsoft ecosystem to provide a personalized health and user experience.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 925446,
        "date": "2023-03-31T10:55:02",
        "slug": "3d-telemedicine",
        "title": "3D Telemedicine",
        "link": "https://www.microsoft.com/en-us/research/project/3d-telemedicine/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed_header_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"3D telemedicine - clinician interacting with patient on-screen in real-time\" style=\"object-position: 65% 53%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed_header_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed_header_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed_header_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed_header_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed_header_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed_header_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed_header_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 id=\"3d-telemedicine\" class=\"wp-block-heading\">3D Telemedicine</h2>\n\n\n\n<p>Bringing specialized healthcare to rural and underserved communities with live 3D communication</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Our mission is to increase access to healthcare for rural and underserved communities by providing live 3D communication between patients and doctors. We employ Microsoft\u2019s <a href=\"https://www.microsoft.com/en-us/research/project/holoportation-3/\" target=\"_blank\" rel=\"noreferrer noopener\">Holoportation<sup>TM</sup></a> technology and low-cost Azure Kinect sensors to capture and stream live 3D content of patients to remote clinicians for clinical consultation in real-time.&nbsp;&nbsp;</p>\n\n\n\n<p>Existing telemedicine solutions that rely on 2D video communication fail to provide doctors and surgeons with the same quantity and quality of information as in-person consultation. Our research narrows this gap, demonstrating that 3D provides increased patient satisfaction, realism, and quality when compared to standard 2D telemedicine <a href=\"#footnote\">[1]</a>.</p>\n\n\n\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"Microsoft\u2019s Holoportation\u2122 Communications Technology: Facilitating 3D Telemedicine\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/ml4Dv5CWC7s?feature=oembed&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</div></figure>\n\n\n\n<div class=\"wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-59 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/blog/3d-telemedicine-brings-better-care-to-underserved-and-rural-communities-even-across-continents/\">Read the blog</a></div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-135 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://ars.els-cdn.com/content/image/1-s2.0-S1748681522005551-mmc5.mp4\" target=\"_blank\" rel=\"noreferrer noopener\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed-Clinician-side_1280x720-1024x576.jpg\" alt=\"3D telemedicine - clinician interacting with patient on-screen in real-time\" class=\"wp-image-925563\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed-Clinician-side_1280x720-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed-Clinician-side_1280x720-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed-Clinician-side_1280x720-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed-Clinician-side_1280x720-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed-Clinician-side_1280x720-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed-Clinician-side_1280x720-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed-Clinician-side_1280x720-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed-Clinician-side_1280x720-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed-Clinician-side_1280x720-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3D-Telemed-Clinician-side_1280x720.jpg 1280w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\">Graphic content disclaimer. Video and image courtesy of Lo et al. <a href=\"#footnote\">[1]</a></figcaption></figure>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://ars.els-cdn.com/content/image/1-s2.0-S1748681522005551-mmc5.mp4\" target=\"_blank\" rel=\"noreferrer noopener\">Watch the video</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side_1400x788-1024x576.jpg\" alt=\"3D telemedicine - patient interacting with clinician on-screen in real-time\" class=\"wp-image-925566\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption class=\"wp-element-caption\">Image courtesy of Lo et al. <a href=\"#footnote\">[1]</a></figcaption></figure>\n</div>\n</div>\n\n\n\n<p>Our system generates a real-time 3D model of the patient, providing a true-to-life 360-degree view to the clinician that can be used for both patient evaluation and to improve informed discussions with patients. The system is applicable to a wide range of medical scenarios and is currently being used in reconstructive plastic surgery.</p>\n\n\n\n<p>This research is an international collaboration between Microsoft, the Canniesburn Regional Plastic Surgery Unit in Scotland, the West of Scotland NHS Innovations Hub, and the National Reconstructive Plastic Surgery and Burns Centre at Korle Bu Teaching Hospital in Ghana. In 2020, the system was implemented as a fast-tracked COVID-19 research project in Scotland, to develop new and improved methods of remote consultation during the pandemic. Reconstructive plastic surgeons are using the 3D telemedicine system for pre- and post-op patient assessment\u2014for example in patients with burns, cancer, or trauma reconstruction. The future goal is to bring the system to remote parts of Scotland to facilitate communication between specialists and patients.</p>\n\n\n\n<div class=\"wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile is-vertically-aligned-top\" style=\"grid-template-columns:20% auto\" data-bi-an=\"media-text\"><figure class=\"wp-block-media-text__media\"><img loading=\"lazy\" decoding=\"async\" width=\"245\" height=\"300\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side-rig_01-245x300.jpg\" alt=\"3D telemedicine - patient side rig with multiple ring lights, monitor, and chair\" class=\"wp-image-925569 size-medium\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side-rig_01-245x300.jpg 245w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side-rig_01-147x180.jpg 147w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-patient-side-rig_01.jpg 723w\" sizes=\"(max-width: 245px) 100vw, 245px\" /></figure><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<h2 id=\"technology\" class=\"wp-block-heading\">Technology</h2>\n\n\n\n<p>The system consists of 10 capture devices radially positioned around the patient, each one containing an Azure Kinect sensor connected to a NVIDIA Jetson Nano computer. The Azure Kinect includes both a standard color camera and a depth camera, which allow us to capture the color and the corresponding distance or depth to each pixel captured. These data are collected by the NVIDIA Jetson Nano computers and sent to a GPU-powered workstation in which Holoportation<sup>TM</sup> algorithms fuse the depth maps to produce a streaming 3D model of the patient. The model is colorized on a secondary rendering desktop and sent to a remote viewer application through which the clinician interacts in real-time.</p>\n</div></div>\n\n\n\n<p>On the patient&#8217;s side, the system includes a monitor and audio system for a Microsoft Teams video call with the clinician. The interaction with the 3D model can be screenshared with the patient for enhanced communication. This includes 3D drawing tools on the remote viewer app that can be used by the surgeon on the patient\u2019s model to explain details of the surgery, providing a \u2018personalized\u2019 medicine approach to the patient\u2019s care.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-136 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-01_1400x788-1024x576.jpg\" alt=\"3D telemedicine - patient interacting with clinician on-screen in real-time\" class=\"wp-image-925572\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-01_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-01_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-01_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-01_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-01_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-01_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-01_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-01_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-01_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-01_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-01_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-02_1400x788-1024x576.jpg\" alt=\"3D telemedicine - patient interacting with clinician on-screen in real-time\" class=\"wp-image-925575\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-02_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-02_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-02_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-02_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-02_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-02_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-02_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-02_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-02_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-02_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/3DTM-Viewer-02_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-137 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://ars.els-cdn.com/content/image/1-s2.0-S1748681522005551-mmc6.mp4\" target=\"_blank\" rel=\"noreferrer noopener\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/video_3DTM_viewer-functionalities_1280x720-1024x576.jpg\" alt=\"3D Telemedicine viewer functionalities - close-up of a patient's chest area highlighted\" class=\"wp-image-932322\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/video_3DTM_viewer-functionalities_1280x720-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/video_3DTM_viewer-functionalities_1280x720-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/video_3DTM_viewer-functionalities_1280x720-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/video_3DTM_viewer-functionalities_1280x720-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/video_3DTM_viewer-functionalities_1280x720-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/video_3DTM_viewer-functionalities_1280x720-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/video_3DTM_viewer-functionalities_1280x720-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/video_3DTM_viewer-functionalities_1280x720-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/video_3DTM_viewer-functionalities_1280x720-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/video_3DTM_viewer-functionalities_1280x720.jpg 1280w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\">Graphic content disclaimer. Video and image courtesy of Lo et al. <a href=\"#footnote\">[1]</a></figcaption></figure>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://ars.els-cdn.com/content/image/1-s2.0-S1748681522005551-mmc6.mp4\" target=\"_blank\" rel=\"noreferrer noopener\">Watch the video</a></div>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\"></div>\n</div>\n\n\n\n<h2 id=\"participatory-development-and-validation\" class=\"wp-block-heading\">Participatory development and validation</h2>\n\n\n\n<p>Patients were placed at the heart of the development process, and were involved in early discussions on functionality, quality, and usability. The 3D telemedicine system subsequently underwent clinical testing against standard 2D telemedicine <a href=\"#footnote\">[1]</a>, demonstrating improved patient metrics that included validated metrics of satisfaction, realism or \u201cpresence,\u201d and quality. Safety and clinical concordance of 3D telemedicine with a face-to-face consultation were equivalent or exceeded estimates for 2D telemedicine. A randomized controlled trial is currently underway to provide definitive evidence of patient benefits.</p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" />\n\n\n\n<p id=\"footnote\">[1] <a href=\"https://www.microsoft.com/en-us/research/publication/participatory-development-of-a-3d-telemedicine-system-during-covid-the-future-of-remote-consultations/\">Participatory Development of a 3D Telemedicine system during Covid: the future of remote consultations</a></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Bringing specialized healthcare to rural and underserved communities with live 3D communication Our mission is to increase access to healthcare for rural and underserved communities by providing live 3D communication between patients and doctors. We employ Microsoft\u2019s HoloportationTM technology and low-cost Azure Kinect sensors to capture and stream live 3D content of patients to remote [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 931371,
        "date": "2023-03-28T13:43:57",
        "slug": "lida-automatic-generation-of-grammar-agnostic-visualizations",
        "title": "LIDA: Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models",
        "link": "https://www.microsoft.com/en-us/research/project/lida-automatic-generation-of-grammar-agnostic-visualizations/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-grey card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"lida\">LIDA</h2>\n\n\n\n<p>Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Systems that support users in the automatic creation of visualizations must address several subtasks &#8211; understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we\u00a0pose visualization generation as a multi-stage generation problem\u00a0and argue that well-orchestrated pipelines based on large language models (LLMs) and image generation models (IGMs) are suitable to addressing these tasks. We present LIDA, a novel tool for generating grammar-agnostic visualizations and infographics. LIDA comprises of 4 modules &#8211; A SUMMARIZER that converts data into a rich but compact natural language summary, a GOAL EXPLORER that enumerates visualization goals given the data, a VISGENERATOR that generates, refines, executes and filters visualization code and an INFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA provides a python api, and a hybrid user interface (direct manipulation and\u00a0multilingual\u00a0natural language) for interactive chart, infographics and data story generation.</p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://microsoft.github.io/lida/\" target=\"_blank\" rel=\"noreferrer noopener\">Learn more</a></div>\n</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models Systems that support users in the automatic creation of visualizations must address several subtasks &#8211; understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we\u00a0pose visualization generation as a multi-stage generation problem\u00a0and argue that well-orchestrated pipelines based [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 923382,
        "date": "2023-03-24T09:06:03",
        "slug": "venice",
        "title": "Project Venice",
        "link": "https://www.microsoft.com/en-us/research/project/venice/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Venice_header_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"overhead view of Venice canals and channels\" style=\"object-position: 51% 63%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Venice_header_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Venice_header_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Venice_header_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Venice_header_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Venice_header_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Venice_header_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Venice_header_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"project-venice\">Project Venice</h2>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<h2 class=\"wp-block-heading\" id=\"project-venice-principled-design-for-side-channel-protection\">Project Venice: principled design for side-channel protection</h2>\n\n\n\n<p>Cloud tenants share hardware resources such as CPU cores, caches, memory, and network. A malicious tenant can observe usage patterns in those shared resources to infer information about other tenants. This has been exploited in various side-channel attacks, including Spectre-style attacks that use shared microarchitectural states to exfiltrate information illegitimately obtained during transient execution. Current countermeasures mitigate known vulnerabilities but fail to provide comprehensive guarantees.</p>\n\n\n\n<p>Side channels are of particular concern for confidential computing, which ensures the code and data of security-critical applications are systematically encrypted (thereby preventing direct access by other tenants or even the cloud provider) but does not preclude such information leakage.&nbsp;</p>\n\n\n\n<p>The goal of Project Venice is to provide strong end-to-end protection against software side-channel attacks, with confidential cloud computing as its main use case.&nbsp;To achieve this goal, we investigate novel mechanisms for the secure sharing and partitioning of compute resources, together with techniques for specifying and rigorously testing their resilience to side-channel attacks.</p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>The goal of Project Venice is to provide strong end-to-end protection against software side-channel attacks, with confidential cloud computing as its main use case.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 923796,
        "date": "2023-03-06T12:20:41",
        "slug": "living-display",
        "title": "Living Display",
        "link": "https://www.microsoft.com/en-us/research/project/living-display/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Living-Display_header_1920x720.jpg\" class=\"attachment-full size-full\" alt=\"woman facing a video screen using a Living Display for a video coference\" style=\"object-position: 77% 44%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Living-Display_header_1920x720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Living-Display_header_1920x720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Living-Display_header_1920x720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Living-Display_header_1920x720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Living-Display_header_1920x720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Living-Display_header_1920x720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Living-Display_header_1920x720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 id=\"living-display\">Living Display</h2>\n\n\n\n<p>Visually natural teleconferencing</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-138 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_static_1280x720-1024x576.jpg\" alt=\"two men facing each other in a video conference using Living Display\" class=\"wp-image-924477\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_static_1280x720-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_static_1280x720-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_static_1280x720-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_static_1280x720-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_static_1280x720-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_static_1280x720-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_static_1280x720-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_static_1280x720-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_static_1280x720-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_static_1280x720.jpg 1280w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_moving_1280x720-1024x576.jpg\" alt=\"two men facing each other in a video conference using Living Display\" class=\"wp-image-924480\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_moving_1280x720-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_moving_1280x720-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_moving_1280x720-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_moving_1280x720-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_moving_1280x720-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_moving_1280x720-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_moving_1280x720-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_moving_1280x720-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_moving_1280x720-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_Spencer-Ben_moving_1280x720.jpg 1280w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_facing-video-screen_16-9-1024x576.jpg\" alt=\"a man and a woman facing each other in a video conference using Living Display\" class=\"wp-image-924483\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_facing-video-screen_16-9-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_facing-video-screen_16-9-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_facing-video-screen_16-9-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_facing-video-screen_16-9-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_facing-video-screen_16-9-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_facing-video-screen_16-9-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_facing-video-screen_16-9-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_facing-video-screen_16-9-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_facing-video-screen_16-9-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR_facing-video-screen_16-9.jpg 1280w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n</div>\n</div>\n\n\n\n<p>Videoconferencing has become an integral part of our lives and was a critical instrument for personal and professional interaction during the COVID-19 pandemic. Businesses, education, and interpersonal relationships have been transformed in recent years by the adoption of video communication, which continues to grow as our society discovers the benefits of hybrid work. However, we as humans evolved over millennia to perceive verbal and non-verbal social cues that are critical during in-person communication. Current videoconferencing technologies are unable to fully replicate them. The purpose of this research is to provide an immersive, visually natural videoconferencing experience that is much closer to an in-person, face-to-face meeting than existing conferencing products.&nbsp;Our approach provides direct eye contact, true motion dynamics, and does so with no requirement for a head-mounted display.</p>\n\n\n\n<div class=\"wp-block-media-text has-video  has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile\" data-bi-an=\"media-text\"><figure class=\"wp-block-media-text__media video-wrapper\"><iframe class=\"media-text__video\" src=\"https://www.youtube-nocookie.com/embed/zJc_BQZ2yYk?enablejsapi=1&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></figure><div class=\"wp-block-media-text__content\" data-bi-an=\"media-text\">\n<p>The Living Display system allows you to look directly at your counterpart on the monitor and for your counterpart to experience you looking directly at them, rather than facing cameras above or below your display. In a typical video conference, when you change your position relative to the monitor, what you see stays the same.&nbsp;Our system goes beyond eye contact correction and provides correct motion dynamics of the person in 3D. As you move, what is shown on the monitor changes much as it would if you were looking through an actual physical window into another room. This video illustrates our vision for the Living Display system and how it can help bring a better sense of presence for videoconferencing.</p>\n</div></div>\n\n\n\n<h2 id=\"technology\">Technology</h2>\n\n\n\n<figure class=\"wp-block-video alignright\"><video controls src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/TLDR-Demo-Ben-and-Spencer_cropped.mp4\"></video></figure>\n\n\n\n<p>To capture fluid motion dynamics, two Azure Kinect sensors positioned above the display, and one below, provide RGB plus depth information (RGB-D) to create a 3D model of you and your surroundings using Microsoft\u2019s <a href=\"https://www.microsoft.com/en-us/research/project/holoportation-3/\">Holoportation</a><sup>TM</sup> technology. The model is either rendered at source and streamed as video, or the raw RGB-D data is transmitted across the network and rendered into a 3D model just before presentation to remote participants. You view remote participants the same way, providing a real-time, natural conferencing experience without the need for head-mounted displays.</p>\n\n\n\n<p>The depth sensors around the local display allow tracking of your current position and adjustment of the local view based on your gaze. Hence, the correct parallax effect of looking through a window is provided. Two-way data communication ensures that the participants on either side experience those motion dynamics in the video call.</p>\n\n\n\n<p><a id=\"_msocom_1\"></a></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>The purpose of this research is to provide an immersive, visually natural videoconferencing experience that is much closer to an in-person, face-to-face meeting than existing conferencing products.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 924843,
        "date": "2023-03-03T18:33:49",
        "slug": "micro-climate-predictions",
        "title": "Micro-climate Predictions",
        "link": "https://www.microsoft.com/en-us/research/project/micro-climate-predictions/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-racing-green card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"2400\" height=\"1350\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2.jpg\" class=\"attachment-full size-full\" alt=\"Micro-climate Prediction on Farms\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2.jpg 2400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2-1536x864.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2-2048x1152.jpg 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture2-1920x1080.jpg 1920w\" sizes=\"(max-width: 2400px) 100vw, 2400px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/research-for-industry/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"Research for Industry\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tResearch for Industry\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"micro-climate-predictions\">Micro-climate predictions</h2>\n\n\n\n<p>Addressing micro-climate prediction workflows for industries and communities</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Knowledge of micro-climate and micro-climate predictions are of importance in agriculture, forestry, renewable energy systems, architecture, urban design, ecology conservation, maritime and many other domains. Access to high-quality micro-climate predictions is difficult due to high degree of variability across regions and stochastic local effects. This project addresses these issues by developing technology that enable systems workflows relying on micro-climate predictions.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Addressing micro-climate prediction workflows for industries and communities Knowledge of micro-climate and micro-climate predictions are of importance in agriculture, forestry, renewable energy systems, architecture, urban design, ecology conservation, maritime and many other domains. Access to high-quality micro-climate predictions is difficult due to high degree of variability across regions and stochastic local effects. This project addresses [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 924828,
        "date": "2023-03-03T17:33:00",
        "slug": "foodvibes",
        "title": "Project FoodVibes",
        "link": "https://www.microsoft.com/en-us/research/project/foodvibes/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-racing-green card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1280\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture1.png\" class=\"attachment-full size-full\" alt=\"FoodVibes\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture1.png 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture1-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/Picture1-960x540.png 960w\" sizes=\"(max-width: 1280px) 100vw, 1280px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/research-for-industry/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"Research for Industry\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tResearch for Industry\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"foodvibes\">FoodVibes</h2>\n\n\n\n<p>Reducing food waste and optimizing production</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Through the FoodVibes, we are addressing the global food challenge by developing and deploying affordable technology that helps reduce waste and optimize production. Starting from the farms, we focus on production, harvest and post-harvest food supply chain. We address problems around tracking and traceability, food supply chain optimization, data sharing, simulators for predicting food availability, among others.</p>\n\n\n\n<p>Learn more about our sister project,\u00a0<a href=\"https://www.microsoft.com/en-us/research/project/project-farmvibes/\">FarmVibes<span class=\"sr-only\"> (opens in new tab)</span></a>,\u00a0and read our <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://ieeexplore.ieee.org/abstract/document/9773091\" target=\"_blank\" rel=\"noreferrer noopener\">FoodVibes<span class=\"sr-only\"> (opens in new tab)</span></a> paper.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Reducing food waste and optimizing production Through the FoodVibes, we are addressing the global food challenge by developing and deploying affordable technology that helps reduce waste and optimize production. Starting from the farms, we focus on production, harvest and post-harvest food supply chain. We address problems around tracking and traceability, food supply chain optimization, data [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 924807,
        "date": "2023-03-03T17:20:01",
        "slug": "community-powered-micro-grids",
        "title": "Community Powered Micro-grids",
        "link": "https://www.microsoft.com/en-us/research/project/community-powered-micro-grids/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-racing-green card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"680\" height=\"340\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/29079.jpg\" class=\"attachment-full size-full\" alt=\"Community powered Micro-Grids\" style=\"object-position: 72% 59%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/03/29079.jpg 680w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/29079-300x150.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/03/29079-240x120.jpg 240w\" sizes=\"(max-width: 680px) 100vw, 680px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/group/research-for-industry/\" class=\"icon-link icon-link--reverse mb-2\" data-bi-cN=\"Research for Industry\">\n\t\t\t\t\t\t\t\t\t<span class=\"c-glyph glyph-chevron-left\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\tResearch for Industry\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"community-powered-micro-grids\">Community powered micro-grids</h2>\n\n\n\n<p>Driving equitable power management</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>With solar & wind technology decentralizing energy production, a new opportunity for driving equity is arising. Communities that are marginalized on electricity grids have the opportunity to power and manage their energy requirements. This project develops technologies which enable development and adoption of micro-grids driving equity and community resiliency.</p>\n\n\n\n<p><strong>Virtual battery: Dynamic time-shifting for micro-grids</strong></p>\n\n\n\n<p>Time-shifting is a method applied during the off-peak times that will lower the utility expenses. Following the peak hours pricing, the battery is charged when electricity is the cheapest and discharged when it is the most expensive.</p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Driving equitable power management With solar & wind technology decentralizing energy production, a new opportunity for driving equity is arising. Communities that are marginalized on electricity grids have the opportunity to power and manage their energy requirements. This project develops technologies which enable development and adoption of micro-grids driving equity and community resiliency. Virtual battery: [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 908799,
        "date": "2023-02-26T18:18:39",
        "slug": "rodin-diffusion",
        "title": "Rodin Diffusion: A Generative Model for Sculpting 3D Digital Avatars",
        "link": "https://www.microsoft.com/en-us/research/project/rodin-diffusion/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-catalina-blue card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/rodin-diffusion-kv.jpg\" class=\"attachment-full size-full\" alt=\"kv\" style=\"object-position: 81% 33%\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/rodin-diffusion-kv.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/rodin-diffusion-kv-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/rodin-diffusion-kv-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/rodin-diffusion-kv-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/rodin-diffusion-kv-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/rodin-diffusion-kv-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/rodin-diffusion-kv-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 id=\"rodin-diffusion\">Rodin Diffusion</h1>\n\n\n\n<p>A Generative Model for Sculpting 3D Digital Avatars</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>We propose a 3D generative model that uses diffusion models to automatically generate 3D digital avatars represented as neural radiance fields. A significant challenge in generating such avatars is that the memory and processing costs in 3D are prohibitive for producing the rich details required for high-quality avatars. To tackle this problem we propose the roll-out diffusion network (Rodin), which represents a neural radiance field as multiple 2D feature maps and rolls out these maps into a single 2D feature plane within which we perform 3D-aware diffusion. The Rodin model brings the much-needed computational efficiency while preserving the integrity of diffusion in 3D by using 3D-aware convolution that attends to projected features in the 2D feature plane according to their original relationship in 3D. We also use latent conditioning to orchestrate the feature generation for global coherence, leading to high-fidelity avatars and enabling their semantic editing based on text prompts. Finally, we use hierarchical synthesis to further enhance details. The 3D avatars generated by our model compare favorably with those produced by existing generative techniques. We can generate highly detailed avatars with realistic hairstyles and facial hair like beards. We also demonstrate 3D avatar generation from image or text as well as text-guided editability.</p>\n\n\n\n<p>Please <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://3d-avatar-diffusion.microsoft.com/\">visit our homepage to&nbsp;learn more ><span class=\"sr-only\"> (opens in new tab)</span></a></p>\n\n\n\n\n\n<p></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>A Generative Model for Sculpting 3D Digital Avatars We propose a 3D generative model that uses diffusion models to automatically generate 3D digital avatars represented as neural radiance fields. A significant challenge in generating such avatars is that the memory and processing costs in 3D are prohibitive for producing the rich details required for high-quality [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 922440,
        "date": "2023-02-24T04:30:21",
        "slug": "programmable-ran-platform",
        "title": "Project Janus: Programmable RAN and Platform",
        "link": "https://www.microsoft.com/en-us/research/project/programmable-ran-platform/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-auburn card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1080\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/stock_background.jpg\" class=\"attachment-full size-full\" alt=\"Background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/stock_background.jpg 1080w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/stock_background-300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/stock_background-1024x683.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/stock_background-768x512.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/stock_background-240x160.jpg 240w\" sizes=\"(max-width: 1080px) 100vw, 1080px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"project-janus\"><img loading=\"lazy\" decoding=\"async\" width=\"1510\" height=\"1511\" class=\"wp-image-1011411\" style=\"width: 80px;height: 80px\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Janus2.png\" alt=\"Project Janus\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Janus2.png 1510w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Janus2-300x300.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Janus2-1024x1024.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Janus2-150x150.png 150w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Janus2-768x769.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Janus2-180x180.png 180w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Janus2-360x360.png 360w\" sizes=\"(max-width: 1510px) 100vw, 1510px\" />  Project Janus</h2>\n\n\n\n<p>Programmable RAN and Platform</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p><em>Check our latest MWC 2024 announcement and the demo videos:</em></p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://techcommunity.microsoft.com/t5/azure-for-operators-blog/microsoft-and-industry-leaders-enable-ran-and-platform/ba-p/4066159?WT.mc_id=DT-MVP-5001664\">MWC 2024 announcement<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/research/video/project-janus-the-anomaly-detection-demo-mwc-2024/\">The anomaly detection demo (University of Edinburgh, Capgemini, Intel)</a></li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/research/video/project-janus-xapp-using-dynamic-service-models-mwc-2024/\">xApp using Dynamic Service Models (Amdocs, Juniper)</a></li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/research/video/project-janus-support-for-cohere-multi-g-networking-mwc-2024/\">Support for Multi-G networking (Cohere)</a></li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/research/video/project-janus-l1-processing-with-mavenir-ran-mwc-2024/\">L1 processing with RAN (Mavenir)</a></li>\n</ul>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" />\n\n\n\n<p>Virtualization of telco networks is beginning to happen in Radio Access Networks (RAN), especially regarding Open Radio Access Networks (O-RAN). However, there are some obstacles to this trend. The current O-RAN architecture limits programmability to predefined telemetry and control that an xApp can access through existing, standardized service models which may limit the pace of innovation.</p>\n\n\n\n<p>To address this limitation and unleash O-RAN\u2019s full potential, Microsoft has developed Project Janus, programmable RAN platform technology that introduces flexible, dynamically loadable service models. The proposed architecture is primarily based on the existing O-RAN architecture. The main difference is the dynamic service model, whose functionality can be implemented by the application designer and deployed at run-time without affecting RAN operations. All this flexibility does not come at the cost of reliability, safety, or security.</p>\n\n\n\n<p>New applications of analytics and automation are possible with the flexibility of dynamic service models and low latency, including RAN energy efficiency and anomaly detection. New applications are able to access almost all information available at different layers of RAN and exercise control at many different levels of RAN. As a result, Open RAN has the potential to significantly accelerate the pace of RAN transformation, making it possible to achieve the full benefits of 5G sooner.</p>\n\n\n\n<p>Read the recent <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://techcommunity.microsoft.com/t5/azure-for-operators-blog/microsoft-and-industry-leaders-enable-ran-and-platform/ba-p/4066159?WT.mc_id=DT-MVP-5001664\">blog<span class=\"sr-only\"> (opens in new tab)</span></a> and a (slightly old) <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://azure.microsoft.com/mediahandler/files/resourcefiles/microsoft-programmable-ran-platform-with-dynamic-service-models/[Microsoft]%20Advancing%20RAN%20analytics.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">whitepaper<span class=\"sr-only\"> (opens in new tab)</span></a>. </p>\n\n\n\n<p>This project is partially funded by the UK government <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.gov.uk/guidance/future-ran-diversifying-the-5g-supply-chain-competition-winners#towards-ai-powered-and-secure-carrier-grade-open-ran-platform\">FRANC<span class=\"sr-only\"> (opens in new tab)</span></a> and <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.gov.uk/government/publications/open-networks-ecosystem-competition-successful-projects/open-networks-ecosystem-competition-successful-projects#perceptran-towards-maturing-o-ran-based-data-driven-ran-monitoring-and-control\">ONE<span class=\"sr-only\"> (opens in new tab)</span></a> competitions. </p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Programmable RAN and Platform Check our latest MWC 2024 announcement and the demo videos: Virtualization of telco networks is beginning to happen in Radio Access Networks (RAN), especially regarding Open Radio Access Networks (O-RAN). However, there are some obstacles to this trend. The current O-RAN architecture limits programmability to predefined telemetry and control that an [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 920784,
        "date": "2023-02-18T15:01:15",
        "slug": "energy-efficiency-for-dnn-inference-by-performance-maintainedtransformation",
        "title": "Energy Efficiency for DNN Inference by Performance MaintainedTransformation",
        "link": "https://www.microsoft.com/en-us/research/project/energy-efficiency-for-dnn-inference-by-performance-maintainedtransformation/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 id=\"\"></h2>\n\n\n\n<p>We design and implement a performance simulator for DNN models. It converts the<br> the input training script to a directed-acyclic-graph and emulates the execution of<br> operations. The simulator works for both data parallelism and pipeline parallelism.<br> We provide an in-depth analysis on the implementation of NCCL allreduce. By characterizing its behavior, we introduce a network latency predictor to provide<br> a reliable estimation on communication time for our simulator.<br>We consider the interference between computation and communication operations<br> in distributed training. Based on the empirical evidences, we propose a mathematical<br>formulation used for quantifying the impact to training step time during simulation.<br> We collect the running time traces of operations and investigate the intra-GPU and inter-GPU execution time variance. We adopt a statistical approach to formulate the execution time variance and incorporate it into our simulator.</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>We design and implement a performance simulator for DNN models. It converts the the input training script to a directed-acyclic-graph and emulates the execution of operations. The simulator works for both data parallelism and pipeline parallelism. We provide an in-depth analysis on the implementation of NCCL allreduce. By characterizing its behavior, we introduce a network [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 920775,
        "date": "2023-02-18T14:57:06",
        "slug": "merak-an-analytical-performance-simulator-for-large-scale-distributed-training",
        "title": "Merak: An Analytical Performance Simulator for Large-scale Distributed Training",
        "link": "https://www.microsoft.com/en-us/research/project/merak-an-analytical-performance-simulator-for-large-scale-distributed-training/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background bg-gray-200 has-background- card-background--full-bleed\">\n\t\t\t\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 id=\"\"></h2>\n\n\n\n<p>The growing computational demand for training deep neural networks (DNNs) makes it a standard practice to<br>adopt distributed training. Though existing training systems use multiple devices to achieve high degrees of<br>data parallelism, linear speedup of the performance of large-scale distributed training cannot be promised. A<br>major challenge faced by practitioners is that they cannot learn the precise efficiency of the task unless they<br>deploy the model and profile its performance in a cluster. However, deployment and profiling are tedious and<br>cost inefficient. We address this problem by introducing Merak, a DAG-based simulator, which vividly replays<br>the training process and accurately predicts the step time. We draw attention to the communication operations<br>in distributed training and report two critical problems in existing simulation work. (1) We propose a running<br>time formulation for all-reduce kernels, which features the cost of data propagation and reduce operation. (2)<br>We design and train an ML-based prediction model to capture the interference between computation kernels and<br>all-reduce kernels. We adopt the profile-and-predict approach to derive the step time of a large-scale distributed<br>task from the knowledge of a small-scale task. We implement Merak for PyTorch with NCCL communication<br>library and evaluate the performance on Nvidia Ampere A100 clusters. Extensive experiments on various DNN<br>models show that the average accuracy of Merak\u2019s prediction is up to 98.25%</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>The growing computational demand for training deep neural networks (DNNs) makes it a standard practice toadopt distributed training. Though existing training systems use multiple devices to achieve high degrees ofdata parallelism, linear speedup of the performance of large-scale distributed training cannot be promised. Amajor challenge faced by practitioners is that they cannot learn the precise [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 915084,
        "date": "2023-02-15T12:02:14",
        "slug": "lean",
        "title": "Lean",
        "link": "https://www.microsoft.com/en-us/research/project/lean/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/math-2-banner.png\" class=\"attachment-full size-full\" alt=\"mathematical equations\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/math-2-banner.png 1920w, https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/math-2-banner-300x113.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/math-2-banner-768x288.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/math-2-banner-1024x384.png 1024w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h1 id=\"lean\">Lean</h1>\n\n\n\n<p>Programming language and theorem prover</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<h2 id=\"what-is-lean\">What is Lean?</h2>\n\n\n\n<figure class=\"wp-block-image alignright size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"550\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2022/07/LEAN-Theorem-Prover-1024x550.png\" alt=\"LEAN Theorem Prover graphic\" class=\"wp-image-865017\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2022/07/LEAN-Theorem-Prover-1024x550.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2022/07/LEAN-Theorem-Prover-300x161.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2022/07/LEAN-Theorem-Prover-768x413.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2022/07/LEAN-Theorem-Prover-710x380.png 710w, https://www.microsoft.com/en-us/research/uploads/prod/2022/07/LEAN-Theorem-Prover-240x129.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2022/07/LEAN-Theorem-Prover.png 1128w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<p>Lean is a functional programming language and interactive theorem prover. Our project strives to revolutionize mathematics by empowering anyone with an interest to grow in the field using Lean as their assistant. Lean was developed by Microsoft Research in 2013 as an initial effort to help mathematicians and engineers solve complex math problems. Lean is an open-source development environment for formal mathematics, also known as machine-checkable mathematics, used by and contributed to by an active community of mathematicians around the world.</p>\n\n\n\n<p>The digital revolution has been driven by mathematical innovation. The complexity of mathematical problems is increasing massively. Yet today\u2019s math is hard to referee, seldom read, or cited. We now have proofs that cannot be manually refereed. Collaboration in math is difficult and still limited to small groups. Historically, technical achievements through verified proofs have been gatekept by peer validation, keeping the upper most levels of the field exclusive and causing a trust bottleneck in novel explorations. Deconstructing the thought process used to achieve a result into widely checkable commentary is fertile for introductory methods. Lean is eliminating the bottleneck by digitizing mathematics and enabling computers to verify mathematical theorems. We are building the platform for the next wave of mathematicians to embrace formal mathematics.</p>\n\n\n\n<p>Microsoft Research collaborates with the academic and Lean community in pursuit of our mission to democratize mathematics, accelerate the frontiers of mathematical research, and improve equal access to math education. Our community of students, professors, and mathematicians&nbsp;actively contribute to Lean\u2019s mathematical library (<a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/leanprover-community/mathlib4\" target=\"_blank\" rel=\"noreferrer noopener\">Mathlib<span class=\"sr-only\"> (opens in new tab)</span></a>). The community has digitized over half of the undergraduate mathematics curriculum into Lean as well as concepts at the frontiers of mathematics (such as Perfectoid Spaces) and is over one million lines of code. The community\u2019s goal is to reach 10 million lines of code in the next five years to completely digitize the undergraduate mathematics curriculum and further the progress of formalized proofs, such as Fermat\u2019s Last Theorem. Lean has already demonstrated its potential to revolutionize and radically accelerate mathematics, for example, helping Fields Medalist Peter Scholze confirm a new theorem in the <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/leanprover-community/lean-liquid\" target=\"_blank\" rel=\"noreferrer noopener\">Liquid Tensor Experiment<span class=\"sr-only\"> (opens in new tab)</span></a>. At Microsoft Research, we are working on the algorithms, data structures, and proof automation for the Lean platform to support a digital library of this scale effectively and greatly reduce formal mathematics overhead. We will claim the formal mathematics revolution is complete by reaching a de Bruijn factor of 1, when the ratio of proof length in Lean is equal to the proof length in mathematical prose. We believe Lean is spearheading the formal mathematics revolution and will empower the next generation of mathematicians to prove major open conjectures previously deemed impossible.</p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-139 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\"></div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\"></div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-bottom is-layout-flow wp-block-column-is-layout-flow\"></div>\n</div>\n\n\n\n<div class=\"wp-block-columns are-vertically-aligned-top is-layout-flex wp-container-core-columns-is-layout-140 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"Hoskinson Center for Formal Mathematics\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/3snIzhjqsk0?feature=oembed&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</div><figcaption class=\"wp-element-caption\">Hoskinson Center for Formal Mathematics</figcaption></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"Kevin Buzzard: The rise of formalism in mathematics\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/SEID4XYFN7o?feature=oembed&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</div><figcaption class=\"wp-element-caption\">Kevin Buzzard: The rise of formalism in mathematics</figcaption></figure>\n</div>\n\n\n\n<div class=\"wp-block-column is-vertically-aligned-top is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe loading=\"lazy\" title=\"2020's Biggest Breakthroughs in Math and Computer Science\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/HL7DEkXV_60?feature=oembed&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</div><figcaption class=\"wp-element-caption\">2020&#8217;s Biggest Breakthroughs in Math and Computer Science</figcaption></figure>\n</div>\n</div>\n\n\n\n<div class=\"wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-63 wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-fill\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://www.microsoft.com/en-us/research/video/lightning-talks-empowering-mathematicians-with-technology/\" target=\"_blank\" rel=\"noreferrer noopener\">Watch Research Summit talks</a></div>\n</div>\n\n\n\n<p>Finally, we are in collaboration with our academic partners to <a href=\"https://www.microsoft.com/en-us/research/academic-program/microsoft-research-lean-award-program/\">develop high-quality university courses in Lean and supporting literature<span class=\"sr-only\"> (opens in new tab)</span></a>, such as our upcoming textbook <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://leanprover.github.io/functional_programming_in_lean/title.html\" target=\"_blank\" rel=\"noreferrer noopener\"><em>Functional Programming in Lean,</em><span class=\"sr-only\"> (opens in new tab)</span></a> that will fuel the adoption of Lean as the ultimate mathematics democratizer. Our long-term goal is to make Lean available in all classrooms, providing access to formal mathematics to kids, teens, and anyone who might not think of themselves as a mathematician. Our hope is to fully democratize mathematics and to help developing students and future scientists.</p>\n\n\n\n<p>Formal mathematics is our primary focus, but formal methods have proven to be propitious for other applications. Lean has the potential to radically change any scope of logic that can be formalized, such as <a href=\"https://www.microsoft.com/en-us/research/video/research-talk-correct-computational-law-and-civil-procedure-with-the-lean-proof-assistant/\">computational law and civil procedures</a>.<a id=\"_msocom_1\"></a></p>\n\n\n\n<div class=\"wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex\">\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://leanprover.github.io/\" target=\"_blank\" rel=\"noreferrer noopener\">Learn more about Lean</a></div>\n\n\n\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://github.com/leanprover\" target=\"_blank\" rel=\"noreferrer noopener\">Get the code</a></div>\n\n\n\n<div class=\"wp-block-button is-style-outline\"><a data-bi-type=\"button\" class=\"wp-block-button__link wp-element-button\" href=\"https://leanprover.zulipchat.com/\" target=\"_blank\" rel=\"noreferrer noopener\">Join the conversation</a></div>\n</div>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Lean is a functional programming language and interactive theorem prover. Our project strives to revolutionize mathematics by empowering anyone with an interest to grow in the field using Lean as their assistant. Lean was developed by Microsoft Research in 2013 as an initial effort to help mathematicians and engineers solve complex math problems. Lean is an open-source development environment for formal mathematics, also known as machine-checkable mathematics, used by and contributed to by an active community of mathematicians around the world.</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 917364,
        "date": "2023-02-06T17:19:42",
        "slug": "tools-for-managing-and-ideating-responsible-ai-mitigations",
        "title": "Tools for Managing and Ideating Responsible AI Mitigations",
        "link": "https://www.microsoft.com/en-us/research/project/tools-for-managing-and-ideating-responsible-ai-mitigations/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background-plum card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"2000\" height=\"728\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Ideating-Responsible-AI-Mitigations-Background.png\" class=\"attachment-full size-full\" alt=\"General Header Image Responsible AI\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Ideating-Responsible-AI-Mitigations-Background.png 2000w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Ideating-Responsible-AI-Mitigations-Background-300x109.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Ideating-Responsible-AI-Mitigations-Background-1024x373.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Ideating-Responsible-AI-Mitigations-Background-768x280.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Ideating-Responsible-AI-Mitigations-Background-1536x559.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Ideating-Responsible-AI-Mitigations-Background-240x87.png 240w\" sizes=\"(max-width: 2000px) 100vw, 2000px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"ideating-responsible-ai-mitigations\">Ideating Responsible AI Mitigations</h2>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>News: Our slides from the FAccT Tutorial on Responsible AI Toolbox are available <a href=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/06/responsible_ai_toolbox_facct_tutorial_2023.pdf\">here</a>.</p>\n\n\n\n<p>ML algorithms and systems are often prone to severe bias and highly consequential failure modes that are not well understood. This project advances the methods, tools, and infrastructure for debugging and mitigating these failure modes so practitioners may act on them before deploying ML systems in the real world. The project is part of <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/responsible-ai-toolbox\">Responsible AI Toolbox<span class=\"sr-only\"> (opens in new tab)</span></a>, a larger collaborative effort between Microsoft Research, AETHER, and Azure Machine Learning for integrating and building development tools for responsible AI.</p>\n\n\n\n<p>The goal of this project is two-fold:&nbsp;</p>\n\n\n\n<ol>\n<li>Building tools that enable ML engineers to identify, diagnose, and mitigate problems quickly and systematically.&nbsp;</li>\n\n\n\n<li>Conducting research that supports the above processes by better understanding and improving algorithmic robustness and failure explainability for different model architectures and data types.</li>\n</ol>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Responsible-AI-Toolbox-1024x576.jpg\" alt=\"Flowchart showing how responsible AI tools are used together for targeted debugging of machine learning models: the Responsible AI Dashboard for the identification of failures; followed by the Responsible AI Dashboard and Mitigations Library for the diagnosis of failures; then the Responsible AI Mitigations Library for mitigating failures; and lastly the Responsible AI Tracker for tracking, comparing, and validating mitigation techniques from which an arrow points back to the identification phase of the cycle to indicate the repetition of the process as models and data continue to evolve during the ML lifecycle.\" class=\"wp-image-917367\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Responsible-AI-Toolbox-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Responsible-AI-Toolbox-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Responsible-AI-Toolbox-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Responsible-AI-Toolbox-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Responsible-AI-Toolbox-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Responsible-AI-Toolbox-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Responsible-AI-Toolbox-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Responsible-AI-Toolbox-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Responsible-AI-Toolbox-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Responsible-AI-Toolbox-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Responsible-AI-Toolbox.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<p><strong>Recent releases</strong>:</p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/responsible-ai-toolbox-tracker\"><strong>Responsible AI Tracker</strong> <span class=\"sr-only\"> (opens in new tab)</span></a>A Jupyter Lab extension for managing, tracking, and comparing different RAI mitigation experiments. The goal is to accelerate improvement iterations for ML practitioners by enabling them to experiment and compare mitigation results quickly.&nbsp;</li>\n</ul>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/responsible-ai-toolbox-mitigations\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>Responsible AI Mitigations library</strong><span class=\"sr-only\"> (opens in new tab)</span></a> is the ML backend support for targeted mitigation steps that can be used in the Responsible AI Tracker or in any other RAI tool. The designed functionalities guide model improvement by targeting mitigations to errors that affect particular data cohorts, with the goal of reducing performance discrepancies across cohorts.&nbsp;</li>\n</ul>\n\n\n\n<p><strong>Past releases</strong>: In collaboration with Azure Machine Learning, AETHER, and the Mixed Reality Group, we have built the following Responsible AI tools:&nbsp;</p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"http://erroranalysis.ai/\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>Error Analysis</strong><span class=\"sr-only\"> (opens in new tab)</span></a> for identifying and diagnosing errors of ML models and systems.&nbsp;</li>\n</ul>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://responsibleaitoolbox.ai/\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>Responsible AI Dashboard</strong></a> as a one-stop shop dashboard for integrating together several Responsible AI tools on error analysis, interpretability, causality, fairness, and decision making. The dashboard builds upon other RAI offerings at Microsoft such as <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"http://erroranalysis.ai/\" target=\"_blank\" rel=\"noreferrer noopener\">Error Analysis</a>, <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"http://fairlearn.org/\" target=\"_blank\" rel=\"noreferrer noopener\">Fairlearn</a>, <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"http://interpret.ml/\" target=\"_blank\" rel=\"noreferrer noopener\">InterpretML</a>, and <a href=\"https://www.microsoft.com/en-us/research/project/econml/\" target=\"_blank\" rel=\"noreferrer noopener\">EconML</a>.&nbsp;</li>\n</ul>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/backwardcompatibilityML\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>BackwardCompatibilityML</strong><span class=\"sr-only\"> (opens in new tab)</span></a> for training ML models that do not regress and do not introduce new errors. The tool also provides visualizations for model comparison.&nbsp;</li>\n</ul>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>News: Our slides from the FAccT Tutorial on Responsible AI Toolbox are available here. ML algorithms and systems are often prone to severe bias and highly consequential failure modes that are not well understood. This project advances the methods, tools, and infrastructure for debugging and mitigating these failure modes so practitioners may act on them [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 911553,
        "date": "2023-01-15T19:00:15",
        "slug": "project-diviner",
        "title": "Project Diviner",
        "link": "https://www.microsoft.com/en-us/research/project/project-diviner/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1920\" height=\"720\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/01/oracle-bone-script-1920-720.jpg\" class=\"attachment-full size-full\" alt=\"Oracle Bone Script\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/01/oracle-bone-script-1920-720.jpg 1920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/oracle-bone-script-1920-720-300x113.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/oracle-bone-script-1920-720-1024x384.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/oracle-bone-script-1920-720-768x288.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/oracle-bone-script-1920-720-1536x576.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/oracle-bone-script-1920-720-1600x600.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/oracle-bone-script-1920-720-240x90.jpg 240w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 id=\"project-diviner\"><strong>Project Diviner</strong></h2>\n\n\n\n<p></p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Dated back in the Shang dynasty, the oracle bone script is the oldest form of Chinese language engraved on animal bones. A large body of the characters and writings still remains mysterious and awaits to be deciphered. Understanding the language and the actual meaning of the inscriptions will uncover the history thousands of years ago and reveal the mystery for the Chinese civilization.&nbsp;Project Diviner aims to bring computational models and AI tools to assist the historian to solve this grand challenge.</p>\n\n\n\n<p><strong>Oracle Bone Script Restoration and Curation&nbsp;</strong></p>\n\n\n\n<p>The first goal of project is to restore the ancient script to its most original and complete form. This will lay the foundation for all further studies. Due to historical reasons, the oracle bones are heavily fragmented with lots of duplicated rubbings. We are going to develop state-of-the-art tools based on image processing and visual understanding to recover the information engraved on the bones.</p>\n\n\n\n<p>We have started the exploration of the first goal by thoroughly comparing 181,134 pieces of inscription rubbings and assisted oracle bones experts in finding a large number of new duplicate copies across more than 100 databases of oracle bone scripts. The research findings has been published on the website of the Pre Qin History Research Office of the Chinese Academy of Social Sciences: <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" rel=\"noreferrer noopener\" href=\"https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.xianqin.org%2Fblog%2Farchives%2F17264.html&data=05%7C01%7Cv-cheqi%40microsoft.com%7C77c2b83754884550ae7208daf7cf3dd2%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638094766728425222%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=pZiB1PNy%2FGN%2BeFwihE8N4jrhHlU8YHlCcv%2BATbaMuLA%3D&reserved=0\" target=\"_blank\">https://www.xianqin.org/blog/archives/17264.html<span class=\"sr-only\"> (opens in new tab)</span></a>.</p>\n\n\n\n<p><strong>Oracle Bone Script Translation</strong></p>\n\n\n\n<p>The second goal of the project is to translate oracle bone script into modern Chinese in a sentence level manner. This will be extremely challenging as the total amount of the available corpus is limited, and the known ones are even less. Researchers will need to take account of all possible cues beyond the text itself for translation. Translating the oracle bone script will uncover the history thousands of years ago.</p>\n\n\n\n<p><strong>Oracle Bone Character Decipher</strong></p>\n\n\n\n<p>The third goal of the project is to decipher individual characters and connect the dots through the evolution of the ancient characters to modern Chinese. This will also help to understand the grammar, and identify the pronunciation of the oracle bone script. This marks the grand challenge of the project.</p>\n\n\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://news.microsoft.com/apac/features/microsoft-university-researchers-use-ai-to-aid-in-study-of-ancient-script-on-chinas-oracle-bones/\">Microsoft, university researchers use AI to aid in study of ancient script on China\u2019s \u201coracle bones\u201d<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.msra.cn/zh-cn/news/features/oracle-bone-script\">\u4eba\u5de5\u667a\u80fd\u5f00\u542f\u7532\u9aa8\u6587\u6574\u7406\u7814\u7a76\u65b0\u8303\u5f0f<span class=\"sr-only\"> (opens in new tab)</span></a></li>\n</ul>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Dated back in the Shang dynasty, the oracle bone script is the oldest form of Chinese language engraved on animal bones. A large body of the characters and writings still remains mysterious and awaits to be deciphered. Understanding the language and the actual meaning of the inscriptions will uncover the history thousands of years ago [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 912408,
        "date": "2023-01-12T17:14:14",
        "slug": "graph-ai-for-organizational-analytics",
        "title": "Graph AI for organizational analytics",
        "link": "https://www.microsoft.com/en-us/research/project/graph-ai-for-organizational-analytics/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"1595\" height=\"922\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/01/microsoftnetwork.png\" class=\"attachment-full size-full\" alt=\"a network map of Microsoft\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/01/microsoftnetwork.png 1595w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/microsoftnetwork-300x173.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/microsoftnetwork-1024x592.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/microsoftnetwork-768x444.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/microsoftnetwork-1536x888.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/microsoftnetwork-240x139.png 240w\" sizes=\"(max-width: 1595px) 100vw, 1595px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 id=\"graph-ai-for-organizational-analytics\">Graph AI for organizational analytics</h2>\n\n\n\n<p>New advances in graph machine learning paired with telemetry unlock a disruptive new ability to measure and reason about how organizations function.&nbsp;</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>The modern workplace has changed the way collaborative work is performed.&nbsp; With the shift to remote and hybrid work, more communications between people moved from conference rooms to digital platforms.&nbsp; Using these aggregated signals from these digital platforms, we can create and deploy new technologies to drive better understanding as to how teams work together irrespective of the formal org chart.&nbsp; Furthermore, by applying state-of-the-art advances in Graph AI, we can now empower every individual in these new workplace modalities to better support them in their collaborations by providing better recommendations for a wide variety of organizational tasks.&nbsp;</p>\n\n\n\n<p>Using these methods, we can see beyond the existing org chart \u2013 the limitations of org structure can be overcome by the reality of working relationships.&nbsp; This also means we can move to models that dynamically adapt to the real-time interactions and relationships within the organization. This would allow companies to better understand the strengths of their teams, identify bottlenecks and inefficiencies, and make more informed decisions about resource allocation and talent retention. Additionally, graph AI can be used to analyze and predict patterns of collaboration and communication across the organization, providing valuable insights into how to optimize workflows and foster a more empowered and engaged workforce. Ultimately, the combination of the shift towards remote and hybrid work and the advancements in graph AI creates a unique opportunity for organizations to gain a deeper understanding of their operations and unlock new levels of creative output.</p>\n\n\n\n<p>Using these techniques, we can imagine tools to solve a variety of tasks: from measuring the impact of mergers and acquisitions, to better quantifying the effects of reorgs, and to detecting the dynamic formation and dissolution of ad-hoc / project / or virtual teams (which often span org structure).&nbsp; This last capability is especially critical as understating virtual teams spanning the org chart is a <strong>critical</strong> task for a modern organization.&nbsp; Virtual teams often span formal organizational boundaries and may only exist for short periods of time as they are often formed for highly specific objectives on short time horizons (to which a formal org chart cannot adapt quickly enough).&nbsp; Identification of virtual teams can lead to empowerment as those teams can be recognized and resourced.&nbsp; This can provide a new set of tools that can be used by leadership as a signal to help drive alignment across the organization.&nbsp; We are researching the new technologies to enable such capabilities, and much more, through the use of graph AI.&nbsp; Already, some of our research has shipped within <a href=\"https://www.microsoft.com/en-us/microsoft-viva\">Microsoft Viva</a> and we are continuing to explore new frontiers for new empowering technologies across the full suite of Microsoft products.</p>\n\n\n\n<p>In addition to being able to run core analytics, we also want to ensure that users can meaningfully visualize and interact with the complex structures of an organization as a whole.&nbsp; As such, we have also developed methods to automatically create <a href=\"https://www.microsoft.com/en-us/worklab/patterns-hidden-inside-the-org-chart\">Organizational Network Maps</a>.&nbsp; These tools, for the first time, provide a way to visually navigate an organization\u2019s geography and terrain as it changes over time.&nbsp; Additionally, these can be used to drive recommendation systems that make it easier to find colleagues and teams for new collaborations.&nbsp; We can use these views to paint a vibrant picture of workplace behavior that doesn\u2019t align with formal organizational structures.&nbsp; Formal organizational structures alone have many shortcomings.&nbsp; They can\u2019t adapt at the speed of virtual teams, they are often determined with inconsistent methodology (each leader has their own optimal structure), and they are intensely hierarchical.&nbsp; Creating new technologies that analyze collective behavior to automatically understand an org\u2019s collaborative structure can help lead to improved systems that could ultimately help improve employee retention, facilitate space planning, and lead to better organizational outcomes.&nbsp;</p>\n\n\n\n<p>Finally, many of the graph AI techniques that we have been researching are directly applicable to other domains, such as <a href=\"https://www.microsoft.com/en-us/research/project/human-rights-technology/\">counter human trafficking and anti-corruption</a>.</p>\n\n\n\n<p>More information can be found using the links below or by contacting <a href=\"https://www.microsoft.com/en-us/research/people/jolarso/\">Jonathan Larson</a>.</p>\n\n\n\n<p><strong>Learn More:</strong></p>\n\n\n\n<ul><li>Network map and organizational behavior research<ul><li><a href=\"https://www.microsoft.com/en-us/worklab/patterns-hidden-inside-the-org-chart\">WorkLab</a></li></ul><ul><li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://workplaceinsights.microsoft.com/workplace-analytics/uncovering-resilience-measuring-organizational-networks-during-crisis/\">Workplace Insights</a></li></ul><ul><li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://workplaceinsights.microsoft.com/networks/toward-resilience-adapting-to-crisis-through-the-lens-of-organizational-networks/\">Workplace Insights Deep Dive</a></li></ul><ul><li><a href=\"https://www.microsoft.com/en-us/worklab/work-trend-index/hybrid-work\">Work Trends Index</a></li></ul><ul><li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://hbr.org/2021/03/what-a-year-of-wfh-has-done-to-our-relationships-at-work\">Harvard Business Review</a></li></ul><ul><li><a href=\"https://www.microsoft.com/en-us/research/blog/advancing-organizational-science-using-network-machine-learning-to-measure-innovation-in-the-workplace/\">Microsoft Research Blog Post</a></li></ul></li><li>Supporting graph technology and background research papers<ul><li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.youtube.com/watch?v=cfueAalOgiQ\">Tech Minute</a></li></ul><ul><li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/pdf/2104.00641.pdf\">Paper on Dynamic Silos</a></li></ul><ul><li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/pdf/2005.00402.pdf\">Paper on Workgroup Mapping</a></li></ul><ul><li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/graspologic\">Graspologic</a></li></ul></li><li>Related Products / Research Efforts<ul><li><a href=\"https://www.microsoft.com/en-us/microsoft-viva\">Microsoft Viva</a></li></ul><ul><li><a href=\"https://www.microsoft.com/en-us/research/project/human-rights-technology/\">Human Rights Technology</a></li></ul></li></ul>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>New advances in graph machine learning paired with telemetry unlock a disruptive new ability to measure and reason about how organizations function.&nbsp; The modern workplace has changed the way collaborative work is performed.&nbsp; With the shift to remote and hybrid work, more communications between people moved from conference rooms to digital platforms.&nbsp; Using these aggregated [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    },
    {
        "id": 911022,
        "date": "2023-01-11T05:25:34",
        "slug": "human-rights-technology",
        "title": "Human Rights Technology",
        "link": "https://www.microsoft.com/en-us/research/project/human-rights-technology/",
        "content": "<section class=\"mb-3 moray-highlight\">\n\t<div class=\"card-img-overlay mx-lg-0\">\n\t\t<div class=\"card-background  has-background- card-background--full-bleed\">\n\t\t\t<img loading=\"lazy\" decoding=\"async\" width=\"2560\" height=\"1707\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/01/22781064853_291a2660fc_o-scaled.jpg\" class=\"attachment-full size-full\" alt=\"Chain-Link Fence Background\" style=\"\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/01/22781064853_291a2660fc_o-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/22781064853_291a2660fc_o-300x200.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/22781064853_291a2660fc_o-1024x683.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/22781064853_291a2660fc_o-768x512.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/22781064853_291a2660fc_o-1536x1024.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/22781064853_291a2660fc_o-2048x1365.jpg 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/01/22781064853_291a2660fc_o-240x160.jpg 240w\" sizes=\"(max-width: 2560px) 100vw, 2560px\" />\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"card-foreground d-flex mt-md-n5 my-lg-5 px-g px-lg-0\">\n\t\t\t<!-- Container -->\n\t\t\t<div class=\"container d-flex mt-md-n5 my-lg-5 \">\n\t\t\t\t<!-- Card wrapper -->\n\t\t\t\t<div class=\"w-100 w-lg-col-5\">\n\t\t\t\t\t<!-- Card -->\n\t\t\t\t\t<div class=\"card material-md-card py-5 px-md-5\">\n\t\t\t\t\t\t<div class=\"card-body \">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n<h2 class=\"wp-block-heading\" id=\"human-rights-technology\">Human Rights Technology</h2>\n\n\n\n<p>Helping communities take evidence-based action in the fight against human rights violations</p>\n\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</section>\n\n\n\n\n\n<p>Human rights are universal&nbsp;\u2013 every person on the planet is entitled to the same rights and freedoms that enable a life of dignity. In any given society, however, and especially across global society, the enjoyment and realization of human rights is not uniform. Human rights deficits are pervasive, disproportionately affecting some of the most vulnerable and disadvantaged populations around the world. In many cases, they are also perpetuated by the systematic exploitation of people (e.g., human trafficking) or power (e.g., corruption) for private gain. How can we work towards a future in which such organized violations of human rights are no longer possible?</p>\n\n\n\n<p>In <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://aka.ms/msr-sp\" target=\"_blank\" rel=\"noreferrer noopener\">Microsoft Research Special Projects<span class=\"sr-only\"> (opens in new tab)</span></a>, we tackle societal problems through <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://hbr.org/2013/10/special-forces-innovation-how-darpa-attacks-problems\" target=\"_blank\" rel=\"noreferrer noopener\">use-inspired basic research<span class=\"sr-only\"> (opens in new tab)</span></a> \u2013 collaborating with domain experts and frontline organizations to envision technology solutions that would <em>fundamentally transform real-world practice</em> for the better, yet which require <em>fundamental research advances</em> to achieve. In the context of Human Rights Technology, this means combining our expertise in HCI, data science, and engineering to create human-centered data technologies that are ready for immediate use in specific human rights contexts, as well as future use in other contexts affected by similar data challenges.</p>\n\n\n\n<p>Our work focuses on human trafficking and corruption as urgent problem areas where new technology has the potential to make a significant impact now and into the future, given the sizeable communities and substantial relevant data that already exist. Despite this potential, however, the use of data to motivate collective action across either community remains challenging in practice. For example, victim case records contain valuable evidence on the nature of human trafficking, but privacy and safety concerns mean that datasets typically remain unshared. Similarly, open government data contains valuable evidence on how actors and entities participate in corruption, but deliberate suppression of incriminating relationships means that risks typically remain undetected.</p>\n\n\n\n<p>In both cases, and for organized forms of exploitation in general, our collaborations with domain experts suggest the need for a new class of <em>transparency-enhancing technologies</em> designed from the ground up with human rights use cases, users, and stakeholders in mind. Our technology portfolio currently comprises three examples of such technology, designed to tackle complementary aspects of the same target problem: </p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/synthetic-data-showcase\" target=\"_blank\" rel=\"noreferrer noopener\">Synthetic Data Showcase<span class=\"sr-only\"> (opens in new tab)</span></a> for safe and private data sharing, creating the <em>data transparency</em> needed for collective understanding of rights violations;</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/transparency-engine\" target=\"_blank\" rel=\"noreferrer noopener\">Transparency Engine<span class=\"sr-only\"> (opens in new tab)</span></a> for implicit relationship mapping, creating the <em>risk transparency </em>needed for targeted action against networks of rights violators;</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/showwhy\" target=\"_blank\" rel=\"noreferrer noopener\">ShowWhy<span class=\"sr-only\"> (opens in new tab)</span></a> for causal decision making, creating the <em>impact transparency</em> needed for evidence-based policy in the fight against human rights violations.</li>\n</ul>\n\n\n\n<p>We have also co-founded major industry and company initiatives that provide real-world context for the design of such technologies, as well as partnerships with the government and civil society organizations that enable use, feedback, and impact at scale:</p>\n\n\n\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://techagainsttrafficking.org/\" target=\"_blank\" rel=\"noreferrer noopener\">Tech Against Trafficking<span class=\"sr-only\"> (opens in new tab)</span></a> (TAT), an industry coalition of technology companies collaborating with global experts to help eradicate human trafficking using technology;</li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/microsoftacts\" target=\"_blank\" rel=\"noreferrer noopener\">Microsoft ACTS</a> (Advanced Cloud Transparency Services), a <a href=\"https://www.microsoft.com/en-us/legal\" target=\"_blank\" rel=\"noreferrer noopener\">CELA</a> program mobilizing the power of data and technology to help governments accelerate transparency.</li>\n</ul>\n\n\n\n<p>More information can be found using the links below or by contacting <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://aka.ms/darrenedge\" target=\"_blank\" rel=\"noreferrer noopener\">Darren Edge<span class=\"sr-only\"> (opens in new tab)</span></a>. </p>\n\n\n\n<p>Return to this project page and check for future updates via <a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://aka.ms/humanrightstechnology\" target=\"_blank\" rel=\"noreferrer noopener\">https://aka.ms/humanrightstechnology<span class=\"sr-only\"> (opens in new tab)</span></a>.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"learn-more\">Learn more</h3>\n\n\n\n<ul>\n<li><strong>Synthetic Data Showcase</strong>\n<ul>\n<li>Software releases\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/synthetic-data-showcase\" target=\"_blank\" rel=\"noreferrer noopener\">Open source code<span class=\"sr-only\"> (opens in new tab)</span></a> (GitHub)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://microsoft.github.io/synthetic-data-showcase/\" target=\"_blank\" rel=\"noreferrer noopener\">Public utility web app<span class=\"sr-only\"> (opens in new tab)</span></a> (GitHub IO)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://docs.smartnoise.org/synth/synthesizers/pac_synth.html\" target=\"_blank\" rel=\"noreferrer noopener\">PAC-Synth<span class=\"sr-only\"> (opens in new tab)</span></a> (SmartNoise DP synthesizer)</li>\n</ul>\n</li>\n\n\n\n<li>Data releases\n<ul>\n<li>IOM differentially-private synthetic data on victims of human trafficking (2024)\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.ctdatacollaborative.org/global-synthetic-dataset\" target=\"_blank\" rel=\"noreferrer noopener\">The CTDC global synthetic dataset<span class=\"sr-only\"> (opens in new tab)</span></a> (current CTDC data release)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://migrantprotection.iom.int/en/spotlight/articles/publication/iom-releases-global-synthetic-dataset\" target=\"_blank\" rel=\"noreferrer noopener\">IOM releases the Global Synthetic Dataset<span class=\"sr-only\"> (opens in new tab)</span></a> (IOM publication)</li>\n</ul>\n</li>\n\n\n\n<li>IOM differentially-private synthetic data on victim-perpetrator relationships (2022)\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.iom.int/news/iom-microsoft-release-first-public-dataset-victims-and-perpetrators-trafficking\" target=\"_blank\" rel=\"noreferrer noopener\">IOM-Microsoft release the first public dataset on victims and perpetrators of trafficking<span class=\"sr-only\"> (opens in new tab)</span></a> (IOM press release)</li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/research/blog/iom-and-microsoft-release-first-ever-differentially-private-synthetic-dataset-to-counter-human-trafficking/\" target=\"_blank\" rel=\"noreferrer noopener\">IOM and Microsoft release first-ever differentially private synthetic dataset to counter human trafficking</a> (MSR blog)</li>\n</ul>\n</li>\n\n\n\n<li>IOM k-anonymous synthetic data on victims of human trafficking (2021)\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.iom.int/news/iom-microsoft-collaboration-enables-release-largest-public-dataset-bolster-fight-against-human-trafficking\" target=\"_blank\" rel=\"noreferrer noopener\">IOM-Microsoft collaboration enables release of largest public dataset to bolster fight against human trafficking<span class=\"sr-only\"> (opens in new tab)</span></a> (IOM press release)</li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/research/blog/real-world-evidence-and-the-path-from-data-to-impact/\" target=\"_blank\" rel=\"noreferrer noopener\">Real-world evidence and the path from data to impact</a> (MSR blog)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://techcrunch.com/2021/09/23/synthetic-dataset-of-human-trafficking-victims-could-allow-big-data-work-without-privacy-compromises/\" target=\"_blank\" rel=\"noreferrer noopener\">Synthetic data set of human trafficking victims could allow big data work without privacy compromises<span class=\"sr-only\"> (opens in new tab)</span></a> (TechCrunch)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.techrepublic.com/article/new-microsoft-tools-help-identify-and-understand-trends-without-compromising-privacy/\" target=\"_blank\" rel=\"noreferrer noopener\">New Microsoft analytics tools help identify and understand trends without compromising privacy<span class=\"sr-only\"> (opens in new tab)</span></a> (TechRepublic)</li>\n</ul>\n</li>\n</ul>\n</li>\n\n\n\n<li>References in international reports\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.state.gov/wp-content/uploads/2023/06/2023-TIP-Report.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">US Department of State Trafficking in Persons (TIP) Report<span class=\"sr-only\"> (opens in new tab)</span></a> (2023)</li>\n</ul>\n</li>\n</ul>\n</li>\n\n\n\n<li><strong>Transparency Engine</strong>\n<ul>\n<li>Software releases\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/transparency-engine\" target=\"_blank\" rel=\"noreferrer noopener\">Open source code<span class=\"sr-only\"> (opens in new tab)</span></a> (GitHub)</li>\n</ul>\n</li>\n\n\n\n<li>Articles\n<ul>\n<li><a href=\"https://www.microsoft.com/en-us/research/group/societal-resilience/articles/revealing-the-hidden-structure-of-corruption/\" target=\"_blank\" rel=\"noreferrer noopener\">Revealing the hidden structure of corruption</a> (MSR Societal Resilience)</li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/microsoftacts/microsoft-acts-and-microsoft-research\" target=\"_blank\" rel=\"noreferrer noopener\">Microsoft ACTS and Microsoft Research</a> (ACTS Feature)</li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/microsoftacts/shining-a-light-on-beneficial-ownership?rtc=1\" target=\"_blank\" rel=\"noreferrer noopener\">Shining a light on beneficial ownership</a> (ACTS Feature)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://blogs.worldbank.org/governance/can-artificial-intelligence-stop-corruption-its-tracks\" target=\"_blank\" rel=\"noreferrer noopener\">Can artificial intelligence stop corruption in its tracks?<span class=\"sr-only\"> (opens in new tab)</span></a> (World Bank blog)</li>\n</ul>\n</li>\n\n\n\n<li>Approach (University of Bristol collaboration)\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://arxiv.org/abs/2202.03945\" target=\"_blank\" rel=\"noreferrer noopener\">Spectral embedding and the latent geometry of multipartite networks<span class=\"sr-only\"> (opens in new tab)</span></a> (arXiv)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://proceedings.neurips.cc/paper/2021/hash/5446f217e9504bc593ad9dcf2ec88dda-Abstract.html\" target=\"_blank\" rel=\"noreferrer noopener\">Spectral embedding for dynamic networks with stability guarantees<span class=\"sr-only\"> (opens in new tab)</span></a> (NeurIPS 2021)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://proceedings.neurips.cc/paper/2021/hash/007ff380ee5ac49ffc34442f5c2a2b86-Abstract.html\" target=\"_blank\" rel=\"noreferrer noopener\">Matrix factorisation and the interpretation of geodesic distance<span class=\"sr-only\"> (opens in new tab)</span></a> (NeurIPS 2021)</li>\n</ul>\n</li>\n\n\n\n<li>Related projects\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://aka.ms/msrorganalytics\" target=\"_blank\" rel=\"noreferrer noopener\">Graph AI for organizational analytics<span class=\"sr-only\"> (opens in new tab)</span></a> (MSR Special Projects)</li>\n</ul>\n</li>\n</ul>\n</li>\n\n\n\n<li><strong>ShowWhy</strong>\n<ul>\n<li>Software releases\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://github.com/microsoft/showwhy\" target=\"_blank\" rel=\"noreferrer noopener\">Open source code <span class=\"sr-only\"> (opens in new tab)</span></a>(GitHub)</li>\n</ul>\n</li>\n\n\n\n<li>Articles\n<ul>\n<li><a href=\"https://www.microsoft.com/en-us/research/uploads/prod/2022/11/CausalAISuiteForDecisionMaking.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">A Causal AI suite for decision-making</a> (NeurIPS 2022 workshop)</li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/research/blog/real-world-evidence-and-the-path-from-data-to-impact/\" target=\"_blank\" rel=\"noreferrer noopener\">Real-world evidence and the path from data to impact</a> (MSR blog)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.techrepublic.com/article/new-microsoft-tools-help-identify-and-understand-trends-without-compromising-privacy/\" target=\"_blank\" rel=\"noreferrer noopener\">New Microsoft analytics tools help identify and understand trends without compromising privacy<span class=\"sr-only\"> (opens in new tab)</span></a> (TechRepublic)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.techrepublic.com/article/microsoft-teaches-computers-cause-effect/\" target=\"_blank\" rel=\"noreferrer noopener\">Microsoft is teaching computers to understand cause and effect<span class=\"sr-only\"> (opens in new tab)</span></a> (TechRepublic)</li>\n</ul>\n</li>\n\n\n\n<li>Videos\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.youtube.com/watch?v=Im1V4h4mT-0\" target=\"_blank\" rel=\"noreferrer noopener\">Introduction to ShowWhy, user interfaces for causal decision making<span class=\"sr-only\"> (opens in new tab)</span></a> (YouTube)</li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/research/video/update-on-microsoft-causal-open-source-libraries-community-workshop-on-microsofts-causal-tools/\" target=\"_blank\" rel=\"noreferrer noopener\">Update on Microsoft causal open-source libraries</a> (Community workshop)</li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/research/video/tutorial-translating-real-world-data-into-evidence/\" target=\"_blank\" rel=\"noreferrer noopener\">Tutorial: Translating real-world data into evidence</a> (MSR Research Summit)</li>\n</ul>\n</li>\n</ul>\n</li>\n\n\n\n<li><strong>Tech Against Trafficking</strong>\n<ul>\n<li>Sites\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://techagainsttrafficking.org/\" target=\"_blank\" rel=\"noreferrer noopener\">Tech Against Trafficking<span class=\"sr-only\"> (opens in new tab)</span></a> (Homepage)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.bsr.org/en/collaboration/groups/tech-against-trafficking\" target=\"_blank\" rel=\"noreferrer noopener\">Tech Against Trafficking<span class=\"sr-only\"> (opens in new tab)</span></a> (BSR initiative page)</li>\n</ul>\n</li>\n\n\n\n<li>Articles\n<ul>\n<li><a href=\"https://www.microsoft.com/en-us/research/group/societal-resilience/articles/case-study-tech-against-trafficking/\" target=\"_blank\" rel=\"noreferrer noopener\">Case study: Tech Against Trafficking</a> (MSR Societal Resilience)</li>\n</ul>\n</li>\n\n\n\n<li>Videos\n<ul>\n<li><a href=\"https://www.microsoft.com/en-us/research/video/technology-demo-using-technology-to-combat-human-trafficking/\" target=\"_blank\" rel=\"noreferrer noopener\">Technology demo: Using technology to combat human trafficking</a> (MSR Research Summit 2021)</li>\n</ul>\n</li>\n\n\n\n<li>Accelerator programs\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.bsr.org/en/blog/tech-driven-insight-to-address-labor-exploitation-tat-launches-third-accelerator\" target=\"_blank\" rel=\"noreferrer noopener\">Tech-driven insight to address labor exploitation: TAT launches third accelerator<span class=\"sr-only\"> (opens in new tab)</span></a> (2023 launch)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.bsr.org/en/blog/tech-against-trafficking-leverage-innovation-tackle-modern-slavery\" target=\"_blank\" rel=\"noreferrer noopener\">Tech Against Trafficking Summit: How to leverage innovation to tackle modern slavery<span class=\"sr-only\"> (opens in new tab)</span></a> (2022 summit)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://techagainsttrafficking.org/tech-against-trafficking-concludes-the-second-accelerator/\" target=\"_blank\" rel=\"noreferrer noopener\">Tech Against Trafficking concludes the second accelerator<span class=\"sr-only\"> (opens in new tab)</span></a> (2021-2022 showcase)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://techagainsttrafficking.org/accelerating-the-use-of-technology-to-combat-human-trafficking/\" target=\"_blank\" rel=\"noreferrer noopener\">Tech Against Trafficking launches second accelerator program<span class=\"sr-only\"> (opens in new tab)</span></a> (2021-2022 launch)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://techagainsttrafficking.org/accelerating-toward-data-insights-tech-against-trafficking-successfully-concludes-its-pilot-accelerator/\" target=\"_blank\" rel=\"noreferrer noopener\">Accelerating toward data insights: Tech Against Trafficking successfully concludes its pilot accelerator<span class=\"sr-only\"> (opens in new tab)</span></a> (2019-2020 showcase)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://techagainsttrafficking.org/scaling-impact-tech-against-trafficking-launches-accelerator-program/\" target=\"_blank\" rel=\"noreferrer noopener\">Scaling impact: Tech Against Trafficking launches accelerator program<span class=\"sr-only\"> (opens in new tab)</span></a> (2019-2020 launch)</li>\n</ul>\n</li>\n\n\n\n<li>Landscape mapping\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.osce.org/cthb/455458\" target=\"_blank\" rel=\"noreferrer noopener\">OSCE today launched ground-breaking study analyzing more than 300 anti-trafficking tech tools<span class=\"sr-only\"> (opens in new tab)</span></a> (OSCE press release)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.osce.org/secretariat/455206\" target=\"_blank\" rel=\"noreferrer noopener\">Leveraging innovation to fight trafficking in human beings: a comprehensive analysis of technology tools<span class=\"sr-only\"> (opens in new tab)</span></a> (OSCE report)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://techagainsttrafficking.org/interactive-map/\" target=\"_blank\" rel=\"noreferrer noopener\">Interactive map of anti-trafficking technology tools<span class=\"sr-only\"> (opens in new tab)</span></a> (TAT tool)</li>\n</ul>\n</li>\n\n\n\n<li>Testimonies in US Congressional Hearings\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://republicans-science.house.gov/sites/republicans.science.house.gov/files/2022-02-08%20Testimony%20Darnton.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">Data challenges impacting human trafficking research and development of anti-trafficking technological tools<span class=\"sr-only\"> (opens in new tab)</span></a> (2022)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.congress.gov/116/meeting/house/110942/witnesses/HHRG-116-SY21-Wstate-DarntonH-20200728.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">The role of technology in countering trafficking in persons<span class=\"sr-only\"> (opens in new tab)</span></a> (2020)</li>\n</ul>\n</li>\n\n\n\n<li>References in international reports\n<ul>\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://www.state.gov/wp-content/uploads/2023/06/2023-TIP-Report.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">US Department of State Trafficking in Persons (TIP) Report<span class=\"sr-only\"> (opens in new tab)</span></a> (2023)</li>\n\n\n\n<li><a class=\"msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall\" href=\"https://undocs.org/Home/Mobile?FinalSymbol=A%2F78%2F161&Language=E&DeviceType=Desktop&LangRequested=False\">Report of the Special Rapporteur on contemporary forms of slavery (UN General Assembly)<span class=\"sr-only\"> (opens in new tab)</span></a> (2023)</li>\n</ul>\n</li>\n</ul>\n</li>\n\n\n\n<li><strong>Microsoft ACTS</strong>\n\n\n\n\n<ul>\n<li>Sites\n<ul>\n<li><a href=\"https://www.microsoft.com/en-us/microsoftacts\" target=\"_blank\" rel=\"noreferrer noopener\">Microsoft ACTS</a> (Homepage)</li>\n</ul>\n</li>\n\n\n\n<li>Articles\n<ul>\n<li><a href=\"https://www.microsoft.com/en-us/microsoftacts/microsoft-acts-and-microsoft-research\" target=\"_blank\" rel=\"noreferrer noopener\">Microsoft ACTS and Microsoft Research</a> (ACTS Feature)</li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/microsoftacts/shining-a-light-on-beneficial-ownership?rtc=1\" target=\"_blank\" rel=\"noreferrer noopener\">Shining a light on beneficial ownership</a> (ACTS Feature)</li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/microsoftacts/tackling-corruption-with-transparency-and-technology-part-2\" target=\"_blank\" rel=\"noreferrer noopener\">Tackling corruption with transparency and technology. Part 2: Building data tools for increased transparency</a> (ACTS Feature)</li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/microsoftacts/tackling-corruption-with-transparency-and-technology-part-1\" target=\"_blank\" rel=\"noreferrer noopener\">Tackling corruption with transparency and technology. Part 1: Making a difference with data</a> (ACTS Feature)</li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/microsoftacts/open-ownership-partner-spotlight\" target=\"_blank\" rel=\"noreferrer noopener\">Microsoft ACTS and Open Ownership focus on global data standard to boost beneficial ownership transparency</a> (ACTS Spotlight)</li>\n\n\n\n<li><a href=\"https://www.microsoft.com/en-us/microsoftacts/open-contracting-partnership-spotlight?rtc=1\" target=\"_blank\" rel=\"noreferrer noopener\">Microsoft ACTS and Open Contracting Partnership join to increase transparency with advanced data solutions</a> (ACTS Spotlight)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n\n\n\n<p></p>\n\n\n<span id=\"label-external-link\" class=\"sr-only\" aria-hidden=\"true\">Opens in a new tab</span>",
        "excerpt": "<p>Helping communities take evidence-based action in the fight against human rights violations Human rights are universal&nbsp;\u2013 every person on the planet is entitled to the same rights and freedoms that enable a life of dignity. In any given society, however, and especially across global society, the enjoyment and realization of human rights is not uniform. [&hellip;]</p>\n",
        "published_date": "",
        "authors": []
    }
]